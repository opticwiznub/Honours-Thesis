@article{sanchez-lengeling2021a,
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  title = {A Gentle Introduction to Graph Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/gnn-intro},
  doi = {10.23915/distill.00033}
}

@article{daigavane2021understanding,
  author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  title = {Understanding Convolutions on Graphs},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/understanding-gnns},
  doi = {10.23915/distill.00032}
}

@book{deng-deep-learning,
  author={Deng, Li and Yu, Dong},
  title={Deep Learning: Methods and Applications},
  publisher={Now Foundations and Trends},
  year={2014},
  volume={},
  number={},
  pages={},
  doi={}
}

@misc{kipf2017semisupervised,
      title={Semi-Supervised Classification with Graph Convolutional Networks}, 
      author={Thomas N. Kipf and Max Welling},
      year={2017},
      eprint={1609.02907},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kipf2016variational,
      title={Variational Graph Auto-Encoders}, 
      author={Thomas N. Kipf and Max Welling},
      year={2016},
      eprint={1611.07308},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{yu2019dag,
  title={DAG-GNN: DAG Structure Learning with Graph Neural Networks},
  author={Yue Yu, Jie Chen, Tian Gao, and Mo Yu},
  booktitle={Proceedings of the 36th International Conference on Machine Learning},
  year={2019}
}

@misc{peng2023sparsification,
      title={Towards Sparsification of Graph Neural Networks}, 
      author={Hongwu Peng and Deniz Gurevin and Shaoyi Huang and Tong Geng and Weiwen Jiang and Omer Khan and Caiwen Ding},
      year={2023},
      eprint={2209.04766},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2021unified,
      title={A Unified Lottery Ticket Hypothesis for Graph Neural Networks}, 
      author={Tianlong Chen and Yongduo Sui and Xuxi Chen and Aston Zhang and Zhangyang Wang},
      year={2021},
      eprint={2102.06790},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rumelhart1986learning,
  added-at = {2023-03-05T10:30:55.000+0100},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  biburl = {https://www.bibsonomy.org/bibtex/2197dd0ba9ef6a582f0219953dc315f39/jascal_panetzky},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {197dd0ba9ef6a582f0219953dc315f39},
  journal = {nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2023-03-05T10:34:04.000+0100},
  title = {Learning representations by back-propagating errors},
  volume = 323,
  year = 1986
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    year={2016}
}

 @book{pml1Book,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: An introduction",
 publisher = "MIT Press",
 year = 2022,
 url = "probml.ai"
}

@misc{veličković2018graph,
      title={Graph Attention Networks}, 
      author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
      year={2018},
      eprint={1710.10903},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{frankle2020linear,
      title={Linear Mode Connectivity and the Lottery Ticket Hypothesis}, 
      author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
      year={2020},
      eprint={1912.05671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{edwards2011history,
  title={History of climate modeling},
  author={Edwards, Paul N},
  journal={Wiley Interdisciplinary Reviews: Climate Change},
  volume={2},
  number={1},
  pages={128--139},
  year={2011},
  publisher={Wiley Online Library}
}

@incollection{lee2021future,
  title={Future global climate: scenario-based projections and near-term information},
  author={Lee, June-Yi and Marotzke, Jochem and Bala, Govindasamy and Cao, Long and Corti, Susanna and Dunne, John P and Engelbrecht, Francois and Fischer, Erich and Fyfe, John C and Jones, Christopher and others},
  booktitle={Climate change 2021: The physical science basis. Contribution of working group I to the sixth assessment report of the intergovernmental panel on climate change},
  pages={553--672},
  year={2021},
  publisher={Cambridge University Press}
}

@article { ClimateModelDependenceandtheEnsembleDependenceTransformationofCMIPProjections,
      author = "G. Abramowitz and C. H. Bishop",
      title = "Climate Model Dependence and the Ensemble Dependence Transformation of CMIP Projections",
      journal = "Journal of Climate",
      year = "2015",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "28",
      number = "6",
      doi = "https://doi.org/10.1175/JCLI-D-14-00364.1",
      pages=      "2332 - 2348",
      url = "https://journals.ametsoc.org/view/journals/clim/28/6/jcli-d-14-00364.1.xml"
}

@inproceedings{
xu2022chemistry,
title={Chemistry Insights for Large Pretrained {GNN}s},
author={Katherine Xu and Janice Lan},
booktitle={NeurIPS 2022 AI for Science: Progress and Promises},
year={2022},
url={https://openreview.net/forum?id=hNWJbH2lVW}
}

@inproceedings{rath2020detecting,
  title={Detecting fake news spreaders in social networks using inductive representation learning},
  author={Rath, Bhavtosh and Salecha, Aadesh and Srivastava, Jaideep},
  booktitle={2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
  pages={182--189},
  year={2020},
  organization={IEEE}
}

@article{zhang2020revisiting,
  title={Revisiting graph neural networks for link prediction},
  author={Zhang, Muhan and Li, Pan and Xia, Yinglong and Wang, Kai and Jin, Long},
  year={2020}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{Cao_Gu_2020, 
  title={Generalization Error Bounds of Gradient Descent for Learning Over-Parameterized Deep ReLU Networks}, 
  volume={34}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/5736}, 
  DOI={10.1609/aaai.v34i04.5736}, 
  abstractNote={&lt;p&gt;Empirical studies show that gradient-based methods can learn deep neural networks (DNNs) with very good generalization performance in the over-parameterization regime, where DNNs can easily fit a random labeling of the training data. Very recently, a line of work explains in theory that with over-parameterization and proper random initialization, gradient-based methods can find the global minima of the training loss for DNNs. However, existing generalization error bounds are unable to explain the good generalization performance of over-parameterized DNNs. The major limitation of most existing generalization bounds is that they are based on uniform convergence and are independent of the training algorithm. In this work, we derive an algorithm-dependent generalization error bound for deep ReLU networks, and show that under certain assumptions on the data distribution, gradient descent (GD) with proper random initialization is able to train a sufficiently over-parameterized DNN to achieve arbitrarily small generalization error. Our work sheds light on explaining the good generalization performance of over-parameterized deep neural networks.&lt;/p&gt;}, 
  number={04}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Cao, Yuan and Gu, Quanquan}, 
  year={2020}, 
  month={Apr.}, 
  pages={3349-3356} 
}

@misc{Harrisson_2021, 
  title={CMIP6: The next generation of climate models explained}, 
  url={https://www.carbonbrief.org/cmip6-the-next-generation-of-climate-models-explained/}, 
  journal={Carbon Brief}, 
  author={Harrisson, Thomas},
  year={2021}, 
  month={Oct}
} 

@phdthesis{Kulinich_2022, 
  place={Sydney}, 
  title={A Markov chain method for weighting climate model ensembles and uncertainty estimation on spatially explicit data}, 
  school={UNSW}, 
  author={Kulinich, Max}, 
  year={2022}
} 

@article{Hubel_Wiesel_1962, 
  title={Receptive fields, binocular interaction and functional architecture in the Cat’s visual cortex}, 
  volume={160}, 
  DOI={10.1113/jphysiol.1962.sp006837}, 
  number={1},
  journal={The Journal of Physiology},
  author={Hubel, D. H. and Wiesel, T. N.}, 
  year={1962}, 
  pages={106–154}
}

@misc{villalobos2022machine,
  title={Machine Learning Model Sizes and the Parameter Gap}, 
  author={Pablo Villalobos and Jaime Sevilla and Tamay Besiroglu and Lennart Heim and Anson Ho and Marius Hobbhahn},
  year={2022},
  eprint={2207.02852},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{Overpeck2011,
  author = {Overpeck, Jonathan and Meehl, Gerald and Bony, Sandrine and Easterling, David},
  year = {2011},
  month = {02},
  pages = {700-2},
  title = {Climate Data Challenges in the 21st Century},
  volume = {331},
  journal = {Science (New York, N.Y.)},
  doi = {10.1126/science.1197869}
}

@inproceedings{NIPS2014_109d2dd3,
 author = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Number of Linear Regions of Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/109d2dd3608f669ca17920c511c2a41e-Paper.pdf},
 volume = {27},
 year = {2014}
}


@InProceedings{pmlr-v70-raghu17a,
  title = 	 {On the Expressive Power of Deep Neural Networks},
  author =       {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl-Dickstein},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2847--2854},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/raghu17a.html},
  abstract = 	 {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.}
}

@article{HORNIK1991251,
title = {Approximation capabilities of multilayer feedforward networks},
journal = {Neural Networks},
volume = {4},
number = {2},
pages = {251-257},
year = {1991},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(91)90009-T},
url = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
author = {Kurt Hornik},
keywords = {Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation},
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.}
}

@article{RIAHI2017153,
title = {The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview},
journal = {Global Environmental Change},
volume = {42},
pages = {153-168},
year = {2017},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2016.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0959378016300681},
author = {Keywan Riahi and Detlef P. {van Vuuren} and Elmar Kriegler and Jae Edmonds and Brian C. O’Neill and Shinichiro Fujimori and Nico Bauer and Katherine Calvin and Rob Dellink and Oliver Fricko and Wolfgang Lutz and Alexander Popp and Jesus Crespo Cuaresma and Samir KC and Marian Leimbach and Leiwen Jiang and Tom Kram and Shilpa Rao and Johannes Emmerling and Kristie Ebi and Tomoko Hasegawa and Petr Havlik and Florian Humpenöder and Lara Aleluia {Da Silva} and Steve Smith and Elke Stehfest and Valentina Bosetti and Jiyong Eom and David Gernaat and Toshihiko Masui and Joeri Rogelj and Jessica Strefler and Laurent Drouet and Volker Krey and Gunnar Luderer and Mathijs Harmsen and Kiyoshi Takahashi and Lavinia Baumstark and Jonathan C. Doelman and Mikiko Kainuma and Zbigniew Klimont and Giacomo Marangoni and Hermann Lotze-Campen and Michael Obersteiner and Andrzej Tabeau and Massimo Tavoni},
keywords = {Shared Socioeconomic Pathways, SSP, Climate change, RCP, Community scenarios, Mitigation, Adaptation},
abstract = {This paper presents the overview of the Shared Socioeconomic Pathways (SSPs) and their energy, land use, and emissions implications. The SSPs are part of a new scenario framework, established by the climate change research community in order to facilitate the integrated analysis of future climate impacts, vulnerabilities, adaptation, and mitigation. The pathways were developed over the last years as a joint community effort and describe plausible major global developments that together would lead in the future to different challenges for mitigation and adaptation to climate change. The SSPs are based on five narratives describing alternative socio-economic developments, including sustainable development, regional rivalry, inequality, fossil-fueled development, and middle-of-the-road development. The long-term demographic and economic projections of the SSPs depict a wide uncertainty range consistent with the scenario literature. A multi-model approach was used for the elaboration of the energy, land-use and the emissions trajectories of SSP-based scenarios. The baseline scenarios lead to global energy consumption of 400–1200 EJ in 2100, and feature vastly different land-use dynamics, ranging from a possible reduction in cropland area up to a massive expansion by more than 700 million hectares by 2100. The associated annual CO2 emissions of the baseline scenarios range from about 25 GtCO2 to more than 120 GtCO2 per year by 2100. With respect to mitigation, we find that associated costs strongly depend on three factors: (1) the policy assumptions, (2) the socio-economic narrative, and (3) the stringency of the target. The carbon price for reaching the target of 2.6W/m2 that is consistent with a temperature change limit of 2°C, differs in our analysis thus by about a factor of three across the SSP marker scenarios. Moreover, many models could not reach this target from the SSPs with high mitigation challenges. While the SSPs were designed to represent different mitigation and adaptation challenges, the resulting narratives and quantifications span a wide range of different futures broadly representative of the current literature. This allows their subsequent use and development in new assessments and research projects. Critical next steps for the community scenario process will, among others, involve regional and sectoral extensions, further elaboration of the adaptation and impacts dimension, as well as employing the SSP scenarios with the new generation of earth system models as part of the 6th climate model intercomparison project (CMIP6).}
}

@misc{Harrisson_2021_SSP, 
title={Explainer: How ‘Shared Socioeconomic Pathways’ explore future climate change}, 
url={https://www.carbonbrief.org/explainer-how-shared-socioeconomic-pathways-explore-future-climate-change/}, 
journal={Carbon Brief}, 
author={Harrisson, Thomas}, 
year={2018}, 
month={Apr}
}

@incollection{KOTU201919,
title = {Chapter 2 - Data Science Process},
editor = {Vijay Kotu and Bala Deshpande},
booktitle = {Data Science (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {19-37},
year = {2019},
isbn = {978-0-12-814761-0},
doi = {https://doi.org/10.1016/B978-0-12-814761-0.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128147610000022},
author = {Vijay Kotu and Bala Deshpande},
keywords = {CRISP, KDD, data mining process, prior knowledge, modeling, data preparation, evaluation, application},
abstract = {Successfully uncovering patterns using data science is an iterative process. This chapter provides a framework to solve a data science problem. The five-step process outlined in this chapter provides guidelines on gathering subject matter expertise; exploring the data with statistics and visualization; building a model using data science algorithms; testing the model and deploying it in a production environment; and finally reflecting on new knowledge gained in the cycle. Over the years of evolution of data mining and data science practices, different frameworks have been put forward by various academic and commercial bodies, like the Cross Industry Standard Process for Data Mining, knowledge discovery in databases, etc. These data science frameworks exhibit common characteristics, and hence, generic framework closely resembling the CRISP process will be used.}
}


@InProceedings{pmlr-v97-yu19a,
  title = 	 {{DAG}-{GNN}: {DAG} Structure Learning with Graph Neural Networks},
  author =       {Yu, Yue and Chen, Jie and Gao, Tian and Yu, Mo},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7154--7163},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/yu19a/yu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/yu19a.html},
  abstract = 	 {Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \url{https://github.com/fishmoon1234/DAG-GNN}.}
}

@article{Palmer2008,
author = {Palmer, Tim and Williams, Phoebe},
year = {2008},
month = {04},
pages = {2421-7},
title = {Stochastic Physics and Climate Models},
volume = {366},
journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
doi = {10.1098/rsta.2008.0059}
}

@inproceedings{Knutti2010,
author = {Knutti, Reto and Abramowitz, Gab and Collins, M. and Eyring, Veronika and Gleckler, Peter and Hewitson, Bruce and Mearns, Linda},
year = {2010},
month = {01},
pages = {1-15},
title = {Good Practice Guidance Paper on Assessing and Combining Multi Model Climate Projections}
}

@article{Rosenblatt_1958, 
  title={The Perceptron: A probabilistic model for information storage and organization in the brain.}, 
  volume={65}, 
  DOI={10.1037/h0042519}, 
  number={6}, 
  journal={Psychological Review}, 
  author={Rosenblatt, F.}, 
  year={1958}, 
  pages={386–408}
}

@book{inmon2007tapping,
  title={Tapping into Unstructured Data: Integrating Unstructured Data and Textual Analytics into Business Intelligence},
  author={Inmon, W.H. and Nesavich, A.},
  isbn={9780132712910},
  url={https://books.google.com.au/books?id=lg-7rimfw_kC},
  year={2007},
  publisher={Pearson Education}
}

@phdthesis{nouri2021improving,
  title={Improving the Performance of Clinical Prediction Tasks by Using Structured and Unstructured Data Combined with a Patient Network},
  author={Nouri Golmaei, Sara},
  year={2021}
}

@article{nordstrom1992using,
  title={Using and designing massively parallel computers for artificial neural networks},
  author={Nordstr{\"o}m, Tomas and Svensson, Bertil},
  journal={Journal of parallel and distributed computing},
  volume={14},
  number={3},
  pages={260--285},
  year={1992},
  publisher={Elsevier}
}

@article{ma2019transformed,
  title={Transformed L1 regularization for learning sparse deep neural networks},
  author={Ma, Rongrong and Miao, Jianyu and Niu, Lingfeng and Zhang, Peng},
  journal={Neural Networks},
  volume={119},
  pages={286--298},
  year={2019},
  publisher={Elsevier}
}

@article{scardapane2017group,
  title={Group sparse regularization for deep neural networks},
  author={Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
  journal={Neurocomputing},
  volume={241},
  pages={81--89},
  year={2017},
  publisher={Elsevier}
}

@article{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={International Conference on Machine Learning},
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@article{louizos2017bayesian,
  title={Bayesian compression for deep learning},
  author={Louizos, Christos and Ullrich, Karen and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kharin2002climate,
  title={Climate predictions with multimodel ensembles},
  author={Kharin, Viatcheslav V and Zwiers, Francis W},
  journal={Journal of Climate},
  volume={15},
  number={7},
  pages={793--799},
  year={2002},
  publisher={American Meteorological Society}
}

@article{feng2011comparison,
  title={Comparison of four ensemble methods combining regional climate simulations over Asia},
  author={Feng, Jinming and Lee, Dong-Kyou and Fu, Congbin and Tang, Jianping and Sato, Yasuo and Kato, Hisashi and Mcgregor, John L and Mabuchi, Kazuo},
  journal={Meteorology and Atmospheric Physics},
  volume={111},
  pages={41--53},
  year={2011},
  publisher={Springer}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{nalisnick2019dropout,
  title={Dropout as a structured shrinkage prior},
  author={Nalisnick, Eric and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Smyth, Padhraic},
  booktitle={International Conference on Machine Learning},
  pages={4712--4722},
  year={2019},
  organization={PMLR}
}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@article{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{alzubaidi2021review,
  title={Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions},
  author={Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamar{\'\i}a, Jos{\'e} and Fadhel, Mohammed A and Al-Amidie, Muthana and Farhan, Laith},
  journal={Journal of big Data},
  volume={8},
  pages={1--74},
  year={2021},
  publisher={Springer}
}

@article{adeel2020contextual,
  title={Contextual deep learning-based audio-visual switching for speech enhancement in real-world environments},
  author={Adeel, Ahsan and Gogate, Mandar and Hussain, Amir},
  journal={Information Fusion},
  volume={59},
  pages={163--170},
  year={2020},
  publisher={Elsevier}
}

@article{tian2020evolutionary,
  title={Evolutionary programming based deep learning feature selection and network construction for visual data classification},
  author={Tian, Haiman and Chen, Shu-Ching and Shyu, Mei-Ling},
  journal={Information systems frontiers},
  volume={22},
  pages={1053--1066},
  year={2020},
  publisher={Springer}
}

@article{young2018recent,
  title={Recent trends in deep learning based natural language processing},
  author={Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  journal={ieee Computational intelligenCe magazine},
  volume={13},
  number={3},
  pages={55--75},
  year={2018},
  publisher={IEEE}
}

@article{kim2015ppcor,
  title={ppcor: an R package for a fast calculation to semi-partial correlation coefficients},
  author={Kim, Seongho},
  journal={Communications for statistical applications and methods},
  volume={22},
  number={6},
  pages={665},
  year={2015},
  publisher={NIH Public Access}
}

@article{frank1993statistical,
  title={A statistical view of some chemometrics regression tools},
  author={Frank, LLdiko E and Friedman, Jerome H},
  journal={Technometrics},
  volume={35},
  number={2},
  pages={109--135},
  year={1993},
  publisher={Taylor \& Francis}
}