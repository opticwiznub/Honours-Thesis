%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  A small sample UNSW Honours Thesis file.
%  Any questions to Ian Doust i.doust@unsw.edu.au
%
% Edited CSG 11.9.2015, use some of Gery's ideas for front matter; add a conclusion chapter.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  The first part pulls in a UNSW Thesis class file.  This one is
%  slightly nonstandard and has been set up to do a couple of
%  things automatically
%
 
\documentclass[honours,12pt]{unswthesis}
\linespread{1}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym,amsmath}
\usepackage{graphicx}
\usepackage{afterpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  The following are some simple LaTeX macros to give some
%  commonly used letters in funny fonts. You may need more or less of
%  these
%
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\B}{\mathfrak{B}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\M}{\mathfrak{M}}
\newcommand{\X}{\mathfrak{X}}
\newcommand{\Y}{\mathfrak{Y}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\ZZ}{\mathcal{Z}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% The following are much more esoteric commands that I have left in
% so that this file still processes. Use or delete as you see fit
%
\newcommand{\bv}[1]{\mbox{BV($#1$)}}
\newcommand{\comb}[2]{\left(\!\!\!\begin{array}{c}#1\\#2\end{array}\!\!\!\right)
}
\newcommand{\Lat}{{\rm Lat}}
\newcommand{\var}{\mathop{\rm var}}
\newcommand{\Pt}{{\mathcal P}}
\def\tr(#1){{\rm trace}(#1)}
\def\Exp(#1){{\mathbb E}(#1)}
\def\Exps(#1){{\mathbb E}\sparen(#1)}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\hatt}[1]{\widehat #1}
\newcommand{\modeq}[3]{#1 \equiv #2 \,(\text{mod}\, #3)}
\newcommand{\rmod}{\,\mathrm{mod}\,}
\newcommand{\p}{\hphantom{+}}
\newcommand{\vect}[1]{\mbox{\boldmath $ #1 $}}
\newcommand{\reff}[2]{\ref{#1}.\ref{#2}}
\newcommand{\psum}[2]{\sum_{#1}^{#2}\!\!\!'\,\,}
\newcommand{\bin}[2]{\left( \begin{array}{@{}c@{}}
				#1 \\ #2
			\end{array}\right)	}
%
%  Macros - some of these are in plain TeX (gasp!)
%
\newcommand{\be}{($\beta$)}
\newcommand{\eqp}{\mathrel{{=}_p}}
\newcommand{\ltp}{\mathrel{{\prec}_p}}
\newcommand{\lep}{\mathrel{{\preceq}_p}}
\def\brack#1{\left \{ #1 \right \}}
\def\bul{$\bullet$\ }
\def\cl{{\rm cl}}
\let\del=\partial
\def\enditem{\par\smallskip\noindent}
\def\implies{\Rightarrow}
\def\inpr#1,#2{\t \hbox{\langle #1 , #2 \rangle} \t}
\def\ip<#1,#2>{\langle #1,#2 \rangle}
\def\lp{\ell^p}
\def\maxb#1{\max \brack{#1}}
\def\minb#1{\min \brack{#1}}
\def\mod#1{\left \vert #1 \right \vert}
\def\norm#1{\left \Vert #1 \right \Vert}
\def\paren(#1){\left( #1 \right)}
\def\qed{\hfill \hbox{$\Box$} \smallskip}
\def\sbrack#1{\Bigl \{ #1 \Bigr \} }
\def\ssbrack#1{ \{ #1 \} }
\def\smod#1{\Bigl \vert #1 \Bigr \vert}
\def\smmod#1{\bigl \vert #1 \bigr \vert}
\def\ssmod#1{\vert #1 \vert}
\def\sspmod#1{\vert\, #1 \, \vert}
\def\snorm#1{\Bigl \Vert #1 \Bigr \Vert}
\def\ssnorm#1{\Vert #1 \Vert}
\def\sparen(#1){\Bigl ( #1 \Bigr )}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% These environments allow you to get nice numbered headings
%  for your Theorems, Definitions etc.  
%
%  Environments
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}[theorem]{Question}
\newtheorem{notation}[theorem]{Notation}
\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  If you've got some funny special words that LaTeX might not
% hyphenate properly, you can give it a helping hand:
%
\hyphenation{Mar-cin-kie-wicz Rade-macher}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% OK...Now we get to some actual input.  The first part sets up
% the title etc that will appear on the front page
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Title?}

\authornameonly{Justin Clarke}

\author{\Authornameonly\\{\bigskip}Supervisor: Associate Professor Yanan Fan}

\copyrightfalse
\figurespagefalse
\tablespagefalse

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  And now the document begins
%  The \beforepreface and \afterpreface commands puts the
%  contents page etc in
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\beforepreface

\afterpage{\blankpage}

% plagiarism

\prefacesection{Plagiarism statement}

\vskip 10pc \noindent I declare that this thesis is my
own work, except where acknowledged, and has not been submitted for
academic credit elsewhere. 

\vskip 2pc  \noindent I acknowledge that the assessor of this
thesis may, for the purpose of assessing it:
\begin{itemize}
\item Reproduce it and provide a copy to another member of the University; and/or,
\item Communicate a copy of it to a plagiarism checking service (which may then retain a copy of it on its database for the purpose of future plagiarism checking).
\end{itemize}

\vskip 2pc \noindent I certify that I have read and understood the University Rules in
respect of Student Academic Misconduct, and am aware of any potential plagiarism penalties which may 
apply.\vspace{24pt}

\vskip 2pc \noindent By signing 
this declaration I am
agreeing to the statements and conditions above.
\vskip 2pc \noindent
Signed: Justin Clarke \hfill Date: 21/07/2023 \newline
\vskip 1pc

\afterpage{\blankpage}

% Acknowledgements are optional


\prefacesection{Acknowledgements}

% {\bigskip}By far the greatest thanks must go to my supervisor for
% the guidance, care and support they provided. 

% {\bigskip\noindent}Thanks 
% must also go to Emily, Michelle, John and Alex who helped by
% proof-reading the document in the final stages of preparation.

% {\bigskip\noindent}Although
% I have not lived with them for a number of years, my family also deserve
% many thanks for their encouragement.

% {\bigskip\noindent} Thanks go to Robert Taggart for allowing his thesis
% style to be shamelessly copied.

% {\bigskip\bigskip\bigskip\noindent} Fred Flintstone, 2 November 2015.

\afterpage{\blankpage}

% Abstract

\prefacesection{Abstract}

Graph sparification techniques for graph neural networks have traditionally been used to accelerate training and inference on real-world graphs which have billions of paramaters.
There are also many different climate models which use complex mathematical models to model the interactions between energy and matter over the world. 
Many of these models share components and parameters and in this paper, I attempt to quantify these relationships through graph sparsification. (Talk more about climate?)
\afterpage{\blankpage}


\afterpreface

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Now we can start on the first chapter
% Within chapters we have sections, subsections and so forth
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\afterpage{\blankpage}

\chapter{Introduction}\label{s-intro}

{\section{Motivation}}\label{motivation}

{\noindent} The simplest climate modeles have existed since the 1950's with the very first computers modelling small two-dimensional climates. 
Modern models have become increasingly more complex in part due to the increasing computational power available today and the large amount of data available
worldwide to train these models on. Many of these models have become unexplainable due to the sheer complexity and number of their parts yet many share components
and frameworks. One of the most important questions that climate science is attempting to answer today is what impact have humans had on the future of the climate.
The prediction of climate change is important as it can guide us on the potential harms we may be causing to environment and life around us. As such, many models
and 'scenario runs' have been developed which predict various outcomes in temperature, precipitation, air pressure and solar radiation given a certain level of 
societal development. On the lower end, SSP126 assumes an increasingly sustainable world where consumption is oriented towards minimising material resource and energy usage
while SSP585 assumes a worst case scenario where fossil fuel usage and an energy-intensive lifestyle intensifies.
(Talk more about the math behind these models? Stochastic Differential models or talk about a few of the main models in use today?)

{\noindent} In recent years, Graph neural networks have become the premier method of processing data with non-cartesian structure. Much of this data exists in the world
in applications such as chemical analysis, social networks and link prediction (Insert references for each from reading). The main feature of GNNs is the message passing framework,
where information from features on each node is passed to neighbouring nodes then aggregated and embedded. This is then propagated through a neural network structure to
perform a range of tasks on the entire graph, individual nodes and edges.        

{\section{Problem Formulation}}\label{problem-formulation}
We define a graph as ${\mathcal{G}} = ({\mathcal{V}}, \textbf{A})$, where $\mathcal{V}$ represents a set of verticies which contains a list of nodes
$\{ v_1, \dots, v_n \}$ and $\textbf{A} \in \mathbb{R}^{n \times n}$ the adjacentcy matrix which contains information on the graph topology. If an edge exists
between two node $v_i$ and $v_j$, then $\textbf{A}_{ij} = 1$ else, $\textbf{A}_{ij} = 0$. Each node has a {p}-dimensional feature vector ${x_i} \in \mathbb{R}^{p}$
which describes some information about the node in the graph. By combining all ${n}$ feature vectors from all nodes, we have a feature matrix
$\textbf{X} \in \mathbb{R}^{n\times p}$. The graph also has a regression target ${Y} \in \mathbb{R}$ which refers to the historical temperature that each model from the
graph is attempting to predict.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background and Related Techniques}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In this chapter we will give a brief overview on how the climate works and review 
current standards in climate modelling along with the basics
behind neural networks and the extensions towards graph neural networks.


%%%%%%%%%%%
\section{Climate Models}\label{climate}
%%%%%%%%%%%



%%%%%%%%%%%%
\section{Neural Networks}\label{nn}
%%%%%%%%%%%%
Traditional machine learning techiques generally require meaningful data cleaning and feature creation which was costly to develop and often had many errors.
The advent of deep learning allowed algorithms to 


%%%%%%%%%%%%
\section{Graphs}\label{g}
%%%%%%%%%%%%



%%%%%%%%%%%%
\section{Graph Neural Networks}\label{gnn}
%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Framework}\label{framework}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%
\section{Dataset}\label{data}
%%%%%%%%%%%%%%%%




\chapter{Conclusion}\label{ccl}

In mathematics, certain kinds of mistaken proof are often exhibited, and sometimes collected, as illustrations of a concept of mathematical fallacy. There is a distinction between a simple mistake and a mathematical fallacy in a proof: a mistake in a proof leads to an invalid proof just in the same way, but in the best-known examples of mathematical fallacies, there is some concealment in the presentation of the proof. For example, the reason validity fails may be a division by zero that is hidden by algebraic notation. There is a striking quality of the mathematical fallacy: as typically presented, it leads not only to an absurd result, but does so in a crafty or clever way. Therefore these fallacies, for pedagogic reasons, usually take the form of spurious proofs of obvious contradictions. Although the proofs are flawed, the errors, usually by design, are comparatively subtle, or designed to show that certain steps are conditional, and should not be applied in the cases that are the exceptions to the rules. \\

\noindent The traditional way of presenting a mathematical fallacy is to give an invalid step of deduction mixed in with valid steps, so that the meaning of fallacy is here slightly different from the logical fallacy. The latter applies normally to a form of argument that is not a genuine rule of logic, where the problematic mathematical step is typically a correct rule applied with a tacit wrong assumption. Beyond pedagogy, the resolution of a fallacy can lead to deeper insights into a subject (such as the introduction of Pasch's axiom of Euclidean geometry and the five color theorem of graph theory). Pseudaria, an ancient lost book of false proofs, is attributed to Euclid. \\

\noindent Mathematical fallacies exist in many branches of mathematics. In elementary algebra, typical examples may involve a step where division by zero is performed, where a root is incorrectly extracted or, more generally, where different values of a multiple valued function are equated. Well-known fallacies also exist in elementary Euclidean geometry and calculus.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\addcontentsline{toc}{chapter}{References}
\bibliographystyle{unswthesis}

\begin{thebibliography}{999}

\bibitem
{BBG} Benzinger, H., Berkson, E. and Gillespie, T.A.,
Spectral families of projections, semigroups, and differential operators,
\textit{Tran. Amer. Math. Soc.} \textbf{275} (1983), 431--475.

\bibitem
{BG Fourier} Berkson, E. and Gillespie, T.A.,
Fourier series criteria for operator decomposability,
\textit{Integral Equations Operator Theory} \textbf{9} (1986), 767--789.

\bibitem
{BG Spectral} Berkson, E., and Gillespie, T.A.,
Spectral decompositions and harmonic analysis on UMD spaces,
\textit{Studia Mathematica} \textbf{112}(1) (1994), 12--49.

\bibitem
{BGM} Berkson, E. Gillespie, T.A. and Muhly, P.S.,
Abstract spectral decompositions guaranteed by the Hilbert transform,
\textit{Proc. London Math. Soc. (3)} \textbf{53} (1986), 489--517.

\bibitem
{Bourgain telaviv} 
Bourgain, J., {\em Martingale transforms and geometry of Banach spaces},
Israel seminar on geometrical aspects of functional analysis
(1983/84), XIV, 16 pp., Tel Aviv Univ., Tel Aviv, 1984.

\bibitem
{Bourgain} Bourgain, J.,
Some remarks on Banach spaces in which martingale difference sequences are
unconditional,
\textit{Ark. Mat.} \textbf{21} (1983), 163--168.

\bibitem
{Bourg} Bourgain, J.,
Vector-valued singular integrals and the $H^1$-BMO duality,
\textit{Probability Theory and Harmonic Analysis} (ed. Chao, J. A.) (1986), 
1--19.

\bibitem
{Burk3} Burkholder, D.L.,
A geometrical characterisation of Banach spaces in which martingale difference
sequences are unconditional,
\textit{Ann. Probability} \textbf{9} (1981), 997--1011.

\bibitem
{Burk2} Burkholder, D.L.,
A geometric condition that implies the existence of certain singular integrals
of Banach-space valued functions,
\textit{Proceedings of Conference on Harmonic Analysis in Honor of Antoni
Zygmund} (Chicago, Illinois, 1981), Wadsworth Publishers: Belmont,
1983, pp. 270--286.

\bibitem
{Burk4} Burkholder, D.L.,
Martingales and Fourier analysis in Banach spaces,
\textit{Lecture Notes in Mathematics} \textbf{1206} (1986), 61--108.

\bibitem
{Burk1} Burkholder, D.L.,
Martingale transforms,
\textit{Ann. Math. Statist.} \textbf{37} (1966), 1494--1504.

\bibitem
{Coifman} Coifman R.R., and Weiss, G.,
\textit{Transference methods in analysis},
Regional Conference Series in Mathematics 31,
Amer. Math. Soc., Providence, R.I., 1977.

\bibitem
{Con} Conway, J.,
\textit{A Course in Functional Analysis},
Springer-Verlag: New York, 1985.

\bibitem
{Dowson} Dowson, H.,
\textit{Spectral theory of linear operators},
London Math. Soc. Monographs, No. 12. Academic Press: New York, 1978.

\bibitem
{Doust} 
Doust, I., Norms of $0$-$1$ matrices in $\CC_p$,  pp 50-55, Proc.
Centre Math. Appl. Austral. Nat. Univ., 39, Austral. Nat. Univ.,
Canberra, 2000.

\bibitem
{DG} Doust, I.  and Gillespie, T. A.,
Schur multipliers on $\CC_p$ spaces,
in preparation.

\bibitem
{Dun} Dunford, N., and Schwartz, J.T.
\textit{Linear operators I: General theory},
Pure and Applied Mathematics 7. Interscience: New York, 1958.

\bibitem
{Gaudry} Edwards, R.E., and Gaudry, G.I.,
\textit{Littlewood--Paley and Multiplier Theory},
Springer-Verlag: Berlin, 1977.

\bibitem
{Katznelson}
Katznelson, Y.,
\textit{An introduction to harmonic analysis},
Second corrected edition, Dover: New York, 1976.

\bibitem
{Gohberg}
Gohberg, I.C., and Kre\u{\i}n, M. G.,
\textit{Introduction to the theory of linear nonselfadjoint operators},
Translations of Mathematical Monographs, Vol. 18, Amer. Math. Soc.,
Providence, R.I., 1969.

\bibitem
{Gowers} Gowers, W. T., and Maurey, B.,
The unconditional basic sequence problem,
\textit{J. Amer. Math. Soc.} \textbf{6} (1993), 851--874.

\bibitem
{Lind}
Lindenstrauss, J., and Tzafriri, L.,
\textit{Classical Banach Spaces I and II},
Springer: Berlin, 1996.

\bibitem
{Pedersen}
Pedersen, G. K.,
\textit{Analysis Now},
Springer: New York, 1989.

\bibitem
{Stein1}
Stein, E. M.,
The development of square functions in the work of A. Zygmund,
\textit{Bulletin (New Series) of the Amer. Math. Soc},
\textbf{6} (1982), 5--30.

\bibitem
{Stein}
Stein, E. M.,
\textit{Topics in Harmonic Analysis Related to the Littlewood--Paley Theory},
Princeton UP: Princeton, 1970.

\end{thebibliography}




\end{document}





