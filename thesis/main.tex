%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  A small sample UNSW Honours Thesis file.
%  Any questions to Ian Doust i.doust@unsw.edu.au
%
% Edited CSG 11.9.2015, use some of Gery's ideas for front matter; add a conclusion chapter.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  The first part pulls in a UNSW Thesis class file.  This one is
%  slightly nonstandard and has been set up to do a couple of
%  things automatically
%
 
\documentclass[honours,12pt]{unswthesis}
\linespread{1}
\usepackage{cite}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{latexsym,amsmath}
\usepackage{graphicx}
\usepackage{afterpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  The following are some simple LaTeX macros to give some
%  commonly used letters in funny fonts. You may need more or less of
%  these
%
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\B}{\mathfrak{B}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\M}{\mathfrak{M}}
\newcommand{\X}{\mathfrak{X}}
\newcommand{\Y}{\mathfrak{Y}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\ZZ}{\mathcal{Z}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% The following are much more esoteric commands that I have left in
% so that this file still processes. Use or delete as you see fit
%
\newcommand{\bv}[1]{\mbox{BV($#1$)}}
\newcommand{\comb}[2]{\left(\!\!\!\begin{array}{c}#1\\#2\end{array}\!\!\!\right)
}
\newcommand{\Lat}{{\rm Lat}}
\newcommand{\var}{\mathop{\rm var}}
\newcommand{\Pt}{{\mathcal P}}
\def\tr(#1){{\rm trace}(#1)}
\def\Exp(#1){{\mathbb E}(#1)}
\def\Exps(#1){{\mathbb E}\sparen(#1)}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\hatt}[1]{\widehat #1}
\newcommand{\modeq}[3]{#1 \equiv #2 \,(\text{mod}\, #3)}
\newcommand{\rmod}{\,\mathrm{mod}\,}
\newcommand{\p}{\hphantom{+}}
\newcommand{\vect}[1]{\mbox{\boldmath $ #1 $}}
\newcommand{\reff}[2]{\ref{#1}.\ref{#2}}
\newcommand{\psum}[2]{\sum_{#1}^{#2}\!\!\!'\,\,}
\newcommand{\bin}[2]{\left( \begin{array}{@{}c@{}}
				#1 \\ #2
			\end{array}\right)	}
%
%  Macros - some of these are in plain TeX (gasp!)
%
\newcommand{\be}{($\beta$)}
\newcommand{\eqp}{\mathrel{{=}_p}}
\newcommand{\ltp}{\mathrel{{\prec}_p}}
\newcommand{\lep}{\mathrel{{\preceq}_p}}
\def\brack#1{\left \{ #1 \right \}}
\def\bul{$\bullet$\ }
\def\cl{{\rm cl}}
\let\del=\partial
\def\enditem{\par\smallskip\noindent}
\def\implies{\Rightarrow}
\def\inpr#1,#2{\t \hbox{\langle #1 , #2 \rangle} \t}
\def\ip<#1,#2>{\langle #1,#2 \rangle}
\def\lp{\ell^p}
\def\maxb#1{\max \brack{#1}}
\def\minb#1{\min \brack{#1}}
\def\mod#1{\left \vert #1 \right \vert}
\def\norm#1{\left \Vert #1 \right \Vert}
\def\paren(#1){\left( #1 \right)}
\def\qed{\hfill \hbox{$\Box$} \smallskip}
\def\sbrack#1{\Bigl \{ #1 \Bigr \} }
\def\ssbrack#1{ \{ #1 \} }
\def\smod#1{\Bigl \vert #1 \Bigr \vert}
\def\smmod#1{\bigl \vert #1 \bigr \vert}
\def\ssmod#1{\vert #1 \vert}
\def\sspmod#1{\vert\, #1 \, \vert}
\def\snorm#1{\Bigl \Vert #1 \Bigr \Vert}
\def\ssnorm#1{\Vert #1 \Vert}
\def\sparen(#1){\Bigl ( #1 \Bigr )}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% These environments allow you to get nice numbered headings
%  for your Theorems, Definitions etc.  
%
%  Environments
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}[theorem]{Question}
\newtheorem{notation}[theorem]{Notation}
\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  If you've got some funny special words that LaTeX might not
% hyphenate properly, you can give it a helping hand:
%
\hyphenation{Mar-cin-kie-wicz Rade-macher}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% OK...Now we get to some actual input.  The first part sets up
% the title etc that will appear on the front page
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Title?}

\authornameonly{Justin Clarke}

\author{\Authornameonly\\{\bigskip}Supervisor: Associate Professor Yanan Fan}

\copyrightfalse
\figurespagefalse
\tablespagefalse

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  And now the document begins
%  The \beforepreface and \afterpreface commands puts the
%  contents page etc in
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\beforepreface

\afterpage{\blankpage}

% plagiarism

\prefacesection{Plagiarism statement}

\vskip 10pc \noindent I declare that this thesis is my
own work, except where acknowledged, and has not been submitted for
academic credit elsewhere. 

\vskip 2pc  \noindent I acknowledge that the assessor of this
thesis may, for the purpose of assessing it:
\begin{itemize}
\item Reproduce it and provide a copy to another member of the University; and/or,
\item Communicate a copy of it to a plagiarism checking service (which may then retain a copy of it on its database for the purpose of future plagiarism checking).
\end{itemize}

\vskip 2pc \noindent I certify that I have read and understood the University Rules in
respect of Student Academic Misconduct, and am aware of any potential plagiarism penalties which may 
apply.\vspace{24pt}

\vskip 2pc \noindent By signing 
this declaration I am
agreeing to the statements and conditions above.
\vskip 2pc \noindent
Signed: Justin Clarke \hfill Date: 21/07/2023 \newline
\vskip 1pc

\afterpage{\blankpage}

% Acknowledgements are optional


\prefacesection{Acknowledgements}

% {\bigskip}By far the greatest thanks must go to my supervisor for
% the guidance, care and support they provided. 

% {\bigskip\noindent}Thanks 
% must also go to Emily, Michelle, John and Alex who helped by
% proof-reading the document in the final stages of preparation.

% {\bigskip\noindent}Although
% I have not lived with them for a number of years, my family also deserve
% many thanks for their encouragement.

% {\bigskip\noindent} Thanks go to Robert Taggart for allowing his thesis
% style to be shamelessly copied.

% {\bigskip\bigskip\bigskip\noindent} Fred Flintstone, 2 November 2015.

\afterpage{\blankpage}

% Abstract

\prefacesection{Abstract}

Graph sparification techniques for graph neural networks have traditionally been used to 
accelerate training and inference on real-world graphs which have billions of paramaters.
There are also many different climate models which use complex mathematical models to model 
the interactions between energy and matter over the world. Many of these models share 
components and the structure of these relationships is not easily found due to the complexity of
these climate models. The space of all possible graphs grows super-exponentially with the number 
of nodes and as such any correlation or causality is difficult to find. In this paper, I attempt 
to quantify these relationships with graph sparsification techniques.
(Talk more about climate?)
\afterpage{\blankpage}


\afterpreface

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Now we can start on the first chapter
% Within chapters we have sections, subsections and so forth
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\afterpage{\blankpage}

\chapter{Introduction}\label{s-intro}

{\section{Motivation}}\label{motivation}

{\noindent} The simplest climate modeles have existed since the 1950's with the very first computers modelling small two-dimensional climates. 
Modern models have become increasingly more complex in part due to the increasing computational power available today and the large amount of data available
worldwide to train these models on. Many of these models have become unexplainable due to the sheer complexity and number of their parts yet many share components
and frameworks. One of the most important questions that climate science is attempting to answer today is what impact have humans had on the future of the climate.
The prediction of climate change is important as it can guide us on the potential harms we may be causing to environment and life around us. As such, many models
and 'scenario runs' have been developed which predict various outcomes in temperature, precipitation, air pressure and solar radiation given a certain level of 
societal development. On the lower end, SSP126 assumes an increasingly sustainable world where consumption is oriented towards minimising material resource and energy usage
while SSP585 assumes a worst case scenario where fossil fuel usage and an energy-intensive lifestyle intensifies.
(Talk more about the math behind these models? Stochastic Differential models or talk about a few of the main models in use today?)

{\noindent} In recent years, Graph neural networks have become the premier method of processing data with non-cartesian structure. Much of this data exists in the world
in applications such as chemical analysis, social networks and link prediction (Insert references for each from reading). The main feature of GNNs is the message passing framework,
where information from features on each node is passed to neighbouring nodes then aggregated and embedded. This is then propagated through a neural network structure to
perform a range of tasks on the entire graph, individual nodes and edges.        

{\section{Approach}}\label{approach}
Overview on my goal and how I am testing this goal. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background and Related Techniques}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In this chapter we will give a brief overview on how the climate works and review 
current standards in climate modelling along with the basics
behind neural networks and the extensions towards graph neural networks.

This will basically be the section on literature review. Current methods and techniques being
used etc. A lot of summaries of the papers saved in the papers folder need to be done to 
finish this section.


%%%%%%%%%%%
\section{Climate Models}\label{climate}
%%%%%%%%%%%
What climate models are used for etc.
Use Yanan's climate papers.


%%%%%%%%%%%%
\section{Deep Learning}\label{nn}
%%%%%%%%%%%%
Traditional machine learning techiques generally require meaningful data cleaning and feature creation which was costly to develop and 
often had many errors. The advent of deep learning provided algorithms that could automatically extract higher-level features from raw data.\cite{deng-deep-learning} The MLP 
The multi-layer perceptron\cite{rumelhart1986learning} is formulated using linked layers of nodes which transforms a set of inputs into an output. For a regression task,
the MLP attempts to approximate some ground-truth function which may also be non-linear. This can be represented as

\begin{equation}
    f(x) = \sigma(\Theta^T X)  \; \text{where}  \; \Theta= \begin{bmatrix} b \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix} \text{and} \; X = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_n \end{bmatrix}
\end{equation}

\noindent Where $\sigma(.)$ is some activation function such as ReLu, hyperbolic tangent or logistic function, $X$ is the data, $\Theta$ is the learned parameters and $b$ represents a bias term.
These parameters are set to some initial values and are iteratively updated in a back-propagation training process,

\begin{equation}
    \theta^{t+1} = \theta^{t} - \eta\frac{\partial E(X, \theta^t)}{\partial\theta}
\end{equation}

\noindent Where $E(.)$ is some loss function and $\eta$ is the learning rate.

The next advancement in the deep learning space came with the Convolutional Neural Network (CNN) which was a regularised MLP that could
handle data with data with structure and multiple dimensions far better than the traditional MLP due to its use of weight sharing, sampling
and local receptive fields.
%%%%%%%%%%%%
\section{Graph Neural Networks}\label{gnn}
%%%%%%%%%%%%
Summarise the extension of GNN's from NN's and how they are useful in certain applications.
Also detail the maths behind graph sparsification.

Towards sparisification of GNN's \cite{peng2023sparsification}
and unified lottery ticket hypothesis \cite{chen2021unified} for gnn's should be reviewed here.




%%%%%%%%%%%%
\section{Variational Autoencoder}\label{vae}
This section should be done with more time if the original sparsification section is completed.
Show how this is an alternative in graph discovery.
%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Framework}\label{framework}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%
\section{Dataset}\label{data}
The datasets used are from the CIMP6 scenario runs made available on the KNMI Climate Explorer website. The KNMI is part of the
World Meteoriological Organization (WMO)\cite{}. The scenario runs include monthly predictions for temperature, min temperature, max temperature
precipitation, radiation and pressure all over the globe in a 192x144 grid between 1850-2100 for 40 different models. For simplicity and brievity, 
this was filtered to just temperature during the 1960-1970 period in just Australia. 

Talk a bit more about how these models in the dataset are all related by certain parts.
%%%%%%%%%%%%%%%%

{\section{Problem Formulation}}\label{problem-formulation}
We define a graph as ${\mathcal{G}} = ({\mathcal{V}}, \textbf{A})$, where $\mathcal{V}$ represents a set of verticies which contains a list of nodes
$\{ v_1, \dots, v_n \}$ and $\textbf{A} \in \mathbb{R}^{n \times n}$ the adjacentcy matrix which contains information on the graph topology. If an edge exists
between two node $v_i$ and $v_j$, then $\textbf{A}_{ij} = 1$ else, $\textbf{A}_{ij} = 0$. We also define the degree matrix as $\mathbf{D} = \sum_{j}A_{ij}$
where each entry on the diagonal is equal to the row sum of the adjacency matrix $\textbf{A}$.  
Each node has a {p}-dimensional feature vector ${x_i} \in \mathbb{R}^{p}$
which describes some information about the node in the graph. By combining all ${n}$ feature vectors from all nodes, we have a feature matrix
$\textbf{X} \in \mathbb{R}^{n\times p}$. The graph also has a regression target ${Y} \in \mathbb{R}$ which refers to the historical temperature that each model from the
graph is attempting to predict. As mentioned earlier, other variables such as precipitation, pressure and radiation are available but for simplicity, just temperature
is currently being used. The two-layer GNN from \cite{kipf2017semisupervised} can be expressed as

\begin{equation}
f(\mathbf{A, X}) = \sigma_2( \mathbf{ \hat{A} }_2 \sigma_1 (\mathbf{ \hat{A} }_1 \mathbf{X} W^{(0)}) W^{(1)})
\end{equation}

\noindent where $\sigma_1(.)$ and $\sigma_2(.)$ are an activation function such as ReLU, and $\hat{A} = \tilde{D}^{-1/2} (A + I) \tilde{D}^{-1/2}$
is the symmetrically normalised adjacency matrix. The final regression problem can be formulated as

\begin{equation}
f : L \times X \rightarrow Y
\end{equation}

\noindent where $f$ denotes the learning function, $L$ the graph, $X$ denotes the time series input and $Y$ the regression target.

Need to describe the math behind graph sparsification. More about shrinkage see Xiongwens.

{\section{Computational features}}\label{computation}
computation of neural network models. See georges paper 
{\section{Implementation}}\label{implementation}

Need to finish code to finish this section.

\chapter{Results}\label{results}

{\section{Model verification}}\label{model-verification}
If the VGAE section is completed, we can compared the sparsified graph with the VGAE
produced graph to determine how good graph sparsification is when used for graph discovery
and thereby correlation in a graph structure.

{\section{Model results}}\label{model-results}
Is there some way we can test the models results depending on how sparse we make the graph etc.
Research required to find some quantitative measure for this.

Some figures of the NN structure would also be helpful for this.
Need to use nx or some other graph representation tool in python for this.

\chapter{Discussion}\label{dis}

\chapter{Conclusion}\label{ccl}

\chapter{Appendix}\label{app}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\addcontentsline{toc}{chapter}{References}
\bibliography{bibliography}{}
\bibliographystyle{plain}

\end{document}





