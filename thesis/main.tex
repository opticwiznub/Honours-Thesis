%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  A small sample UNSW Honours Thesis file.
%  Any questions to Ian Doust i.doust@unsw.edu.au
%
% Edited CSG 11.9.2015, use some of Gery's ideas for front matter; add a conclusion chapter.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  The first part pulls in a UNSW Thesis class file.  This one is
%  slightly nonstandard and has been set up to do a couple of
%  things automatically
%
 
\documentclass[honours,12pt]{unswthesis}
\linespread{1}
\usepackage{cite}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{latexsym,amsmath}
\usepackage{graphicx}
\usepackage{afterpage}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  The following are some simple LaTeX macros to give some
%  commonly used letters in funny fonts. You may need more or less of
%  these
%
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\B}{\mathfrak{B}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\M}{\mathfrak{M}}
\newcommand{\X}{\mathfrak{X}}
\newcommand{\Y}{\mathfrak{Y}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\ZZ}{\mathcal{Z}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% The following are much more esoteric commands that I have left in
% so that this file still processes. Use or delete as you see fit
%
\newcommand{\bv}[1]{\mbox{BV($#1$)}}
\newcommand{\comb}[2]{\left(\!\!\!\begin{array}{c}#1\\#2\end{array}\!\!\!\right)
}
\newcommand{\Lat}{{\rm Lat}}
\newcommand{\var}{\mathop{\rm var}}
\newcommand{\Pt}{{\mathcal P}}
\def\tr(#1){{\rm trace}(#1)}
\def\Exp(#1){{\mathbb E}(#1)}
\def\Exps(#1){{\mathbb E}\sparen(#1)}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\hatt}[1]{\widehat #1}
\newcommand{\modeq}[3]{#1 \equiv #2 \,(\text{mod}\, #3)}
\newcommand{\rmod}{\,\mathrm{mod}\,}
\newcommand{\p}{\hphantom{+}}
\newcommand{\vect}[1]{\mbox{\boldmath $ #1 $}}
\newcommand{\reff}[2]{\ref{#1}.\ref{#2}}
\newcommand{\psum}[2]{\sum_{#1}^{#2}\!\!\!'\,\,}
\newcommand{\bin}[2]{\left( \begin{array}{@{}c@{}}
				#1 \\ #2
			\end{array}\right)	}
%
%  Macros - some of these are in plain TeX (gasp!)
%
\newcommand{\be}{($\beta$)}
\newcommand{\eqp}{\mathrel{{=}_p}}
\newcommand{\ltp}{\mathrel{{\prec}_p}}
\newcommand{\lep}{\mathrel{{\preceq}_p}}
\def\brack#1{\left \{ #1 \right \}}
\def\bul{$\bullet$\ }
\def\cl{{\rm cl}}
\let\del=\partial
\def\enditem{\par\smallskip\noindent}
\def\implies{\Rightarrow}
\def\inpr#1,#2{\t \hbox{\langle #1 , #2 \rangle} \t}
\def\ip<#1,#2>{\langle #1,#2 \rangle}
\def\lp{\ell^p}
\def\maxb#1{\max \brack{#1}}
\def\minb#1{\min \brack{#1}}
\def\mod#1{\left \vert #1 \right \vert}
\def\norm#1{\left \Vert #1 \right \Vert}
\def\paren(#1){\left( #1 \right)}
\def\qed{\hfill \hbox{$\Box$} \smallskip}
\def\sbrack#1{\Bigl \{ #1 \Bigr \} }
\def\ssbrack#1{ \{ #1 \} }
\def\smod#1{\Bigl \vert #1 \Bigr \vert}
\def\smmod#1{\bigl \vert #1 \bigr \vert}
\def\ssmod#1{\vert #1 \vert}
\def\sspmod#1{\vert\, #1 \, \vert}
\def\snorm#1{\Bigl \Vert #1 \Bigr \Vert}
\def\ssnorm#1{\Vert #1 \Vert}
\def\sparen(#1){\Bigl ( #1 \Bigr )}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% These environments allow you to get nice numbered headings
%  for your Theorems, Definitions etc.  
%
%  Environments
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}[theorem]{Question}
\newtheorem{notation}[theorem]{Notation}
\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  If you've got some funny special words that LaTeX might not
% hyphenate properly, you can give it a helping hand:
%
\hyphenation{Mar-cin-kie-wicz Rade-macher}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% OK...Now we get to some actual input.  The first part sets up
% the title etc that will appear on the front page
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Graph Learning Methods for Climate Models with Graph Sparsification}

\authornameonly{Justin Clarke}

\author{\Authornameonly\\{\bigskip}Supervisor: Associate Professor Yanan Fan}

\copyrightfalse
\figurespagefalse
\tablespagefalse

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  And now the document begins
%  The \beforepreface and \afterpreface commands puts the
%  contents page etc in
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\graphicspath{ {C:/Users/jqcla/Documents/GitHub/Honours-Thesis/figures/} }

\beforepreface

\afterpage{\blankpage}

% plagiarism

\prefacesection{Plagiarism statement}

\vskip 10pc \noindent I declare that this thesis is my
own work, except where acknowledged, and has not been submitted for
academic credit elsewhere. 

\vskip 2pc  \noindent I acknowledge that the assessor of this
thesis may, for the purpose of assessing it:
\begin{itemize}
\item Reproduce it and provide a copy to another member of the University; and/or,
\item Communicate a copy of it to a plagiarism checking service (which may then retain a copy of it on its database for the purpose of future plagiarism checking).
\end{itemize}

\vskip 2pc \noindent I certify that I have read and understood the University Rules in
respect of Student Academic Misconduct, and am aware of any potential plagiarism penalties which may 
apply.\vspace{24pt}

\vskip 2pc \noindent By signing 
this declaration I am
agreeing to the statements and conditions above.
\vskip 2pc \noindent
Signed: Justin Clarke \hfill Date: 17/11/2023 \newline
\vskip 1pc

\afterpage{\blankpage}

% Acknowledgements are optional


\prefacesection{Acknowledgements}

% {\bigskip}By far the greatest thanks must go to my supervisor for
% the guidance, care and support they provided. 

% {\bigskip\noindent}Thanks 
% must also go to Emily, Michelle, John and Alex who helped by
% proof-reading the document in the final stages of preparation.

% {\bigskip\noindent}Although
% I have not lived with them for a number of years, my family also deserve
% many thanks for their encouragement.

% {\bigskip\noindent} Thanks go to Robert Taggart for allowing his thesis
% style to be shamelessly copied.

% {\bigskip\bigskip\bigskip\noindent} Fred Flintstone, 2 November 2015.

\afterpage{\blankpage}

% Abstract

\prefacesection{Abstract}

Graph sparification techniques for graph neural networks have traditionally been used to 
accelerate training and inference on real-world graphs which have billions of paramaters.
There are also many different climate models which use complex mathematical models to model 
the interactions between energy and matter over the world. Many of these models share 
components and the structure of these relationships is not easily found due to the complexity of
these climate models. The space of all possible graphs grows super-exponentially with the number 
of nodes and as such any correlation or causality is difficult to find. In this paper, I attempt 
to quantify these relationships with graph sparsification techniques.
(Talk more about climate?)
\afterpage{\blankpage}


\afterpreface

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Now we can start on the first chapter
% Within chapters we have sections, subsections and so forth
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\afterpage{\blankpage}

\chapter{Introduction}\label{s-intro}

{\section{Climate Models}}\label{climate-introduction}

{\noindent}Climate models are based on well-known scientific processes and attempt to simulate the movement of fluids and energy throughout a system. 
The simplest of these have existed since the 1950's with the very first computers modelling simple variables on small two-dimensional climates\cite{edwards2011history}.
Modern Global Climate Models (GCMs) have been scaled up into globally sized three dimensional sizes and most model the complex interactions between various physical, chemical, biological and geological processes. 
These models are constantly being updated with new data as many different groups and institutions implement higher spatial and temporal resolutions.
This has in part been driven by developments in computational techniques and the vast amount of data available worldwide to train these models.\cite{Overpeck2011}
A common approach to studying the complex system of Earth's climate has been through sophisticated mathematical modelling with a range of temporal and spatial data\cite{Kulinich_2022}
and at the centre of most of these approaches are the Navier-Stokes equations which are used describe the movements of liquids and gases in our oceans and atmosphere.\cite{Palmer2008} \\

{\noindent}The primary focus of these models has been forecasting time series data, as climate science aims to address a fundamental question: 
What impact have contemporary human greenhouse gas emissions had on the Earth and its future?
Understanding this is important as it can inform us on the potential harms to society and the environment and guide the population and policy makers who are implementing change.
The Coupled Model Intercomparison Project is a collaborative project between many meteoriological institutions that aims to improve our understanding of climate change.
The sixth iteration of these models or CMIP6 are the premier models for this task but while it was expected to contain around 100 models made by 49 separate groups, 
delays have caused only 40 have been published so far.
However, the results from these 40 models so far indicates a far greater sensitivity to increases in greenhouse gases when compared to the previous generation of CMIP5 models.\cite{Harrisson_2021}
% This suggests a far greater impact from the newer processes that were quantified in CMIP6. \\

{\noindent}The basis behind the Intergovernmental Panel on Climate Change and its 2021 IPCC sixth assessment from the Coupled Model Intercomparison Project (CMIP6) were the Shared Socioeconomic Pathways (SSP).\cite{lee2021future}
These SSP's represent a broad set of possible changes in population, economic and technological growth, and urbanisation that would influence future changes to the climte.\cite{RIAHI2017153}  
These are directly related to Representative Concentration Pathways (RCP) introduced by the previous CMIP5 which are categorisations based on the 
estimated future concentrations of greenhouse gases in the atmosphere.\cite{Harrisson_2021_SSP}.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{SSP} & \textbf{Description}                                                                                                                                                                                                                                                                      \\ \hline
        SSP1         & Sustainability: The world shifts gradually, but pervasively, toward a more sustainable path, emphasizing more inclusive development that respects perceived environmental boundaries.                                                                                                     \\ \hline
        SSP2         & Middle of the road: The world follows a path in which social, economic, and technological trends do not shift markedly from historical patterns.                                                                                                                                          \\ \hline
        SSP3         & Regional rivalry: A resurgent nationalism, concerns about competitiveness and security, and regional conflicts push countries to increasingly focus on domestic or, at most, regional issues.                                                                                             \\ \hline
        SSP4         & Inequality: Highly unequal investments in human capital, combined with increasing disparities in economic opportunity and political power, lead to increasing inequalities and stratification both across and within countries.                                                           \\ \hline
        SSP5         & Fossil-fueled development: This world places increasing faith in competitive markets, innovation and participatory societies to produce rapid technological progress and development of human capital as the path to sustainable development. Global markets are increasingly integrated. \\ \hline    
    \end{tabularx}
    \vspace{1pt}
    \caption{Shared Socioeconomic Pathways Descriptions}
\end{table}

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|}
        \hline
        \textbf{RCP} & \textbf{Temperature Increase (2081--2100)} & \textbf{Sea Level Rise (2081--2100)} \\ \hline
        2.6          & 1.0{\textdegree} C                                  & 0.4m                                   \\ \hline
        4.5          & 1.8{\textdegree} C                                  & 0.47m                                 \\ \hline
        6.0          & 2.2{\textdegree} C                                  & 0.48m                                  \\ \hline
        8.5          & 3.7{\textdegree} C                                  & 0.63m                                  \\ \hline
    \end{tabular}}
    \vspace{1pt}
    \caption{Representative Concentration Pathways}
\end{table}

{\noindent}Scenarios are named based on the conjunction of their SSP level and RCP values. 
For example, on the lower end, SSP126 assumes an increasingly sustainable world where consumption is oriented towards 
minimising material resource and energy usage while SSP585 assumes a worst case scenario where fossil fuel usage and an energy-intensive lifestyle intensifies.
The main output of CMIP6 models are the ``scenario runs'' which predict various outcomes in temperature, precipitation, air pressure and solar radiation given a certain SSP over time from 1850--2100.

{\section{Ensemble Models}}\label{ensemble}

{\noindent} Ensemble modelling is a process where multiple models are used in combination for a task and are more performant when the base models are diverse and independent\cite{KOTU201919}.
CMIP6 is known as an `ensemble of opportunity'\cite{Knutti2010}, where the makeup the ensemble is determined by the ability of each base model to contribute.\cite{ClimateModelDependenceandtheEnsembleDependenceTransformationofCMIPProjections}
This is the main benefit of ensemble methods as models can be weighted based on how accurate they are certain predictions.
However, as research has become far more interconnected in the modern era, many aspects such as expertise, code and literature are often shared between groups. 
As such, many of the models that contribute to CMIP6 are highly likely to be dependent of each other.\cite{ClimateModelDependenceandtheEnsembleDependenceTransformationofCMIPProjections}. 
The degree of dependence between these models is difficult to ascertain as this would require a qualitative investigation into the personel, code and references between each component of CMIP6.
Various novel approaches such as stochastic Markov chains\cite{Kulinich_2022} have been used to provide a more optimal ensemble mean which may account for this dependence.
However, one would expect there to be some ground truth graph structure that links all models together through some dependence.

{\section{Deep Learning}}\label{deep-learning}

{\noindent}There are many different kinds of machine learning but the advent of deep learning has led to countless advancements in many practical applications.
The name deep learning refers to the ability of deep learning models to extract high-level, abstract features from raw data by using many layers of simple, computer understandable representations.
Computers perform well on complex logical tasks such as arithmetic but often stuggle with more simplistic tasks that are harder to quantify such as visual recognition and language. 
The ability of deep learning models to quantify these simple tasks have allowed artifical intelligence to apporach near human level understanding.\cite{Goodfellow-et-al-2016} 
The first neural networks developed in the late 1950's sought to simulate how human brain learned and operated.\cite{Rosenblatt_1958}
The next development in neural networks also came from neuroscientific principles\cite{Hubel_Wiesel_1962} with Convolutional Neural Networks (CNN) which could train models to be equivariant to translations in data and process data with grid-like structure. 
In recent years, Graph Neural Networks (GNN) have become the premier method of processing data with non-cartesian structure as standard convolutions on a graph structure much harder to define.
The main feature of GNNs is the message passing framework, where information from features on each node is passed to neighbouring nodes then aggregated and embedded. 
This is then propagated through a neural network structure to perform a range of tasks on the entire graph, individual nodes or edges.
(INSERT A DIAGRAM OF CONVOLUTION VS GRAPH CONVOLUTION)
Much of this data exists in the world in applications such as chemical analysis\cite{xu2022chemistry}, social networks analysis\cite{rath2020detecting}, link prediction\cite{zhang2020revisiting} and unstructured data processing\cite{nouri2021improving}.

{\section{Sparsification of Deep Learning}}\label{sparsification}

An estimated 80 to 90 percent of the worlds' 79 zetabytes of data is unstructured and graphs make up a significant proportion of this data\cite{inmon2007tapping}.
Traditional neural networks are not able to extract meaninful insights from this unstructured data without significant cleaning.
The modern age has also produced many advancements in computing such as Massively Parallel Processing (MPP)\cite{nordstrom1992using} 
and big data which has led to an exponential growth in the size and complexity of these deep learning models.
The well-known Generative-Pretrained Transformer 3 (GPT-3) model by OpenAI commonly used for ChatGPT had variants with 175 billion parameters which required 800 gigabytes to store.\cite{radford2018improving}
Although Deep Neural Networks (DNN) tend to generalise well even when overparameterised\cite{Cao_Gu_2020}, this level of overparameterisation makes inference 
and prediction highly costly when the same performance could be achieved on a far more simple model. To address this, the concept of the
Lottery Ticket Hypothesis (LTH)\cite{frankle2020linear} was introduced which explored the possibility of simplifying redundant models by trainable sparse subnetworks whilst still training to full accuracy.
Chen et.al.\cite{chen2021unified} extended the LTH to Graph Neural Networks by co-optimising graph and neural networks weights and zeroing out edges with the lowest magnitude. 
This reduced computational costs by over 85\% depending on the size of the graph whilst maintaining a strong baseline accuracy.

{\section{Motivation}}\label{motivation}

The goal of this thesis is to investigate whether these graph sparisifcation techniques can be used to determine some dependence structure within a graph of models which
are all attempting to model the same scenario in the climate. Graph structure and dependence learning is already possible with unsupervised methods such as Variational Graph Autoencoders (VGAE)\cite{pmlr-v97-yu19a}
but to our best knowledge, graph sparsification has not been used before as a method for graph structure learning or infering dependence. Existing methods for determining multiple correlation such as 
partial correlation and multiple correlation coefficient $R^2$ are linear methods. Graph sparsification is primarily used for simplifying graphs which have been overparameterised and grown too large but 
by removing these edges, the edges that remain should theoretically hold some relation to dependence in the graph.  

{\section{Outline}}\label{outline}

{\noindent}The thesis is structured as follows. 
In Chapter 2, we will review the studies relating to neural networks, the extensions towards graph neural networks and the lottery ticket hypothesis (LTH).
Current methods along with the benefits and shortcomings will be discussed and the terminology and definitions for will also be outlined in this section. \\

{\noindent} In Chapter 3, we will perform an exploratory data analysis (EDA) and provide a high-level overview of the dataset. This will provide a more
in-depth understanding of the scenarios and CMIP6 models. There will also be a more comprehensive analysis of an individual climate modeL? (pick an example?)
the climate modelling process and ensemble weighting method? We then introduce our problem formulation with by formalising the regression problem we will be using
to perform the sparsification algorithm. \\

In Chapter 4, we visualise the results of the sparsification algorithm and we compare it to various correlation and partial correlation matrices. 
We may also look at Mutual Information Criterion (MIC) and compare with Variational Graph Autoencoder?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Methods and Related Techniques}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% In this chapter we will give a brief overview on how the climate works and review 
% current standards in climate modelling along with the basics
% behind neural networks and the extensions towards graph neural networks.

% This will basically be the section on literature review. Current methods and techniques being
% used etc. A lot of summaries of the papers saved in the papers folder need to be done to 
% finish this section.


%%%%%%%%%%%%
\section{Deep Learning}\label{nn}
%%%%%%%%%%%%
The advent of deep learning provided algorithms that could automatically extract higher-level features from raw data.\cite{deng-deep-learning}
The multi-layer perceptron\cite{rumelhart1986learning} is formulated using linked layers of nodes which transforms a set of inputs into an output.
The single layer version of this model can be represented as

\begin{equation}
    f(x) = \sigma(\Theta^T X)  \; \text{where}  \; \Theta= \begin{bmatrix} b \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix} \text{and} \; X = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_n \end{bmatrix}
\end{equation}

\noindent Where $\sigma(.)$ is some activation function such as ReLu, hyperbolic tangent or logistic function, $X$ is the data, $\Theta$ is the learned parameters and $b$ represents a bias term.
These parameters are set to some initial values and are iteratively updated in a back-propagation training process,

\begin{equation}
    \theta^{t+1} = \theta^{t} - \eta\frac{\partial E(X, \theta^t)}{\partial\theta}
\end{equation}

\noindent Where $E(.)$ is some loss function and $\eta$ is the learning rate. \\

\noindent The next advancement in the deep learning space came with the Convolutional Neural Network (CNN) which was a regularised MLP that could
handle data with data with structure and multiple dimensions far better than the traditional MLP due to its use of weight sharing, 
sampling and local receptive fields.\cite{Goodfellow-et-al-2016}
Suppose we have an image or some other kind of data in two-dimensional matrix form. Let $\textbf{X} \in \mathbb{R}^{H\times W}$ be the input image and
$\textbf{W} \in \mathbb{R}^{h\times w}$ be the kernel or filter. By performing a convolution, we are effectively 'sliding' our weight matrix kernel
over our input and the resulting feature map $\textbf{Z} = \textbf{X} \ast \textbf{W}$,

\begin{equation}
    Z_{i, j} = \sum_{u=0}^{h-1}\sum_{v=0}^{w-1} x_{i+u, j+v} w_{u, v}
\end{equation}

\noindent The novelty of the convolutional layer compared to a linear layer is that the kernel is shared across all locations of the input and therefore
if a pattern in the input moves, the corresponding output will also follow this movement. This provides shift equivariance which is something that early MLP's
failed to achieve.\cite{pml1Book}

%%%%%%%%%%%%
\section{Graph Neural Networks}\label{gnn}
%%%%%%%%%%%%
When it comes to data in a graph-like structure, standard CNN's cannot be applied due to the non-euclidean nature of a graph.
In an image or a matrix, our kernel is generally a $n\times n$ matrix which can be applied to the entirety of the data. In graphs
this is not always possible due to the fact that any number of nodes can be connected by any number of edges.\cite{sanchez-lengeling2021a} 
The Graph Neural Network (GNN) was developed for this purpose and they can be broadly categorised into gating and attention based methods\cite{veličković2018graph} 
and spectral or spatial methods within Graph Convolutional Network (GCN) research.\cite{kipf2017semisupervised} \\

{\noindent}We define a graph as ${\mathcal{G}} = ({\mathcal{V}}, \textbf{A})$, where $\mathcal{V}$ represents a set of verticies which contains a list of nodes
$\{ v_1, \dots, v_n \}$ and $\textbf{A} \in \mathbb{R}^{n \times n}$ the adjacentcy matrix which contains information on the graph topology. 
If an edge exists between two node $v_i$ and $v_j$, then $\textbf{A}_{ij} = 1$ else, $\textbf{A}_{ij} = 0$. 
We also define the degree matrix as $\mathbf{D} = \sum_{j}A_{ij}$ where each entry on the diagonal is equal to the row sum of the adjacency matrix $\textbf{A}$.  
Each node has a {p}-dimensional feature vector ${x_i} \in \mathbb{R}^{p}$ which describes some information about the node in the graph. 
By combining all ${n}$ feature vectors from all nodes, we have a feature matrix $\textbf{X} \in \mathbb{R}^{n\times p}$. 
The graph also has a response ${Y} \in \mathbb{R}^p$ which is for a graph-level task but a node level task would simply have ${Y} \in \mathbb{R}^{n\times p}$.
The two-layer GNN from\cite{kipf2017semisupervised} can be expressed as

\begin{equation}
f(\mathbf{A, X}) = \sigma_2( \mathbf{ \hat{A} }_2 \sigma_1 (\mathbf{ \hat{A} }_1 \mathbf{X} \Theta^{(0)}) \Theta^{(1)})
\end{equation}

\noindent where $\sigma_1(.)$ and $\sigma_2(.)$ are an activation function such as ReLU, and $\hat{A} = \tilde{D}^{-1/2} (A + I) \tilde{D}^{-1/2}$
is the symmetrically normalised adjacency matrix. The propagation rule is then as follows:

\begin{equation}
    H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} \Theta^{(l)})
\end{equation}

\noindent Here, $\tilde{A} = A + I_N$ is the adjacency matrix of the undirected graph $\mathcal{G}$ with self-connections from the identity matrix $I_N$.
$\tilde{D_{ii}} = \sum_{j} \tilde{A}_{ij} $ is the row sum of the adjacency matrix and $W^{(l)}$ is a layer trainable weight matrix. 
$\sigma(.)$ is an activation function such as $\text{ReLU}(.) = \text{max}(0, .)$. $H^{(l)} \in \mathbb{R}^{N \times D}$ is the matrix of activations
in the lth layer with $H^0 = \textbf{X}$. 


\section{Sparsification of Graph Neural Networks}\label{sparse-learning}

Hornik\cite{HORNIK1991251} showed that in a single layer MLP and provided enough hidden neuron units, 
the neural network could model any smooth truth function as for each added neuron, 
the decision space can be segmented to added linear regions to conform to any response. 
Many experiments have also found that deep networks with many layers perform better than shallow ones\cite{NIPS2014_109d2dd3}\cite{pmlr-v70-raghu17a}
as the usage of many layers allows deeper layers to leverage features produced by earlier layers.
The noveltly of neural networks is that they do not tend to be affected greatly by overparameterisation\cite{Cao_Gu_2020}, and as they generally improve with more neurons
and layers these models have grown exponentially in size with some models using billions of parameters and most of these models having more parameters than training observations
which has made both inference and prediction incredibly costly.  (Reference some math from the number of linear decision regions and some graphs?) \\

{\noindent}The most basic approach to inducing model sparsity is through an $l_1$ penalty in the loss function.\cite{pml1Book}
Unlike the $l_2$ regularisation which penalises large magnitude weights, the $l_1$ regularisation minimises and zeros weights which encourages sparsity.
On a linear regression, this is done with a MAP estimation with a Laplace prior and is equivalent to optimising

\begin{equation}
    \mathcal{L}(\Theta) = ||\textbf{X}\Theta - \textbf{Y}||^2_2 + \lambda||\Theta||_1
\end{equation}
where $\lambda$ is some tunable sparsity parameter. This is easily extended to neural networks by applying this penalty to the weights in the layers of the network.\cite{ma2019transformed}
Despite the this benefit, modern GPUs are optimised for dense matrix multiplication and as such there aren't many computational benefits from regularisation if certain weights across the network are zero.
Methods that encourage \textit{group} sparsity are able to prune whole nodes and layers out of our model result in \textit{block sparse} weight matricies which provide much more substanial computational savings.
\cite{scardapane2017group}\cite{wen2016learning}\cite{molchanov2017variational}\cite{louizos2017bayesian} \\

{\noindent}The Lottery Ticket Hypothesis (LTH)\cite{frankle2020linear} explored the possibility of simplifying redundant models by trainable sparse subnetworks whilst
still training to full accuracy. Chen et. al.\cite{chen2021unified} extended the LTH to Graph Neural Networks by co-simplifying both the adjacentcy matrix of the graph and the weights in 
the network of the model. For a semi-supervised classification task, the objective function is:

\begin{equation}
    \mathcal{L}(\mathcal{G}, \Theta) = -\frac{1}{|\mathcal{V}_{\text{label}}|}\sum_{v_i \in \mathcal{V_{\text{label}}}} y_i \log{(z_i)},
\end{equation}
where $\mathcal{L}$ is the cross-entropy error of all samples and $y_i$ is the label vector of node $v_i$. The Unified GNN Sparsification (UGS)
framework then introduced two masks $m_g$ and $m_\theta$ with the same shape as the adjacency matrix $\textbf{A}$ and the weights matrix $\Theta$, which
gives the following objective function:

\begin{equation}\label{eqn:l_ugs}
    \mathcal{L}_{\text{UGS}} = \mathcal{L}(\{m_g \odot A, \textbf{X}\}, m_\theta \odot \Theta) + \gamma_1 ||m_g||_1 + \gamma_2 ||m_\theta||_1,
\end{equation}

\noindent where $\odot$ is the element-wise product, $\gamma_1$ and $\gamma_2$ are hyperparameters to control the shrinkage of $m_g$ and $m_\theta$.
After training, the lowest magnitude elements in $m_g$ and $m_\theta$ are set to zero with respect to some set values of $p_g$ and $p_\theta$.
These sparse masks are then applied which prune $\textbf{A}$ and $\Theta$. The algorithm is then as follows

\begin{algorithm}
    \caption{Unifed GNN Sparsification\cite{chen2021unified}}\label{alg:ugs}
    \textbf{Input:} \text{Graph} $\mathcal{G} = \{A, \textbf{X}\}$, GNN $f(\mathcal{G}, \Theta_0)$, weight initialisation $\Theta_0$, masks $m^0_g = A$ and $m^0_\theta = 1 \in \R^{||\Theta_0||_0}$, step size $\eta, \lambda_g$ and $\lambda_\theta$.\\
    \textbf{Output:} Sparse masks $m_g$ and $m_\theta$
    \begin{algorithmic}[1]
        \For{iteration $i = 0, 1, 2, \dots, N-1$}
            \State{Forward $f(\cdot, m^i_\theta \odot \Theta_i)$ with $\mathcal{G}=\{m_g^i\odot A, \textbf{X}\}$ \Comment{Computes \ref{eqn:l_ugs}}}
            \State{Backpropagate to update $\Theta_{i+1} \gets \Theta_i - \eta \nabla_{\Theta_i} \mathcal{L}_{UGS} $}
            \State{Update $m_g^{i+1} \gets m_g^i - \eta \nabla_{m_g^i} \mathcal{L}_{UGS}$}
            \State{Update $m_\theta^{i+1} \gets m_\theta^i - \eta \nabla_{m_\theta^i} \mathcal{L}_{UGS}$}
        \EndFor
        \State Set $p_g = 5\%$ of the lowest magnitude values in $m_g^N$ to 0 and others to 1, then obtain $m_g$.
        \State Set $p_g = 20\%$ of the lowest magnitude values in $m_\theta^N$ to 0 and others to 1, then obtain $m_\theta$.
    \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%
\section{Variational Autoencoder}\label{vgae}
This section should be done with more time if the original sparsification section is completed.
Show how this is an alternative in graph discovery.
%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Framework}\label{framework}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%
\section{Climate Models}\label{climate}
%%%%%%%%%%%
What climate models are used for etc.
Use Yanan's climate papers.

(Can we talk about how one model works?)

\section{Exploratory Data Analysis}\label{eda}
\begin{figure}[!h]
    \includegraphics[width=7cm]{model_agg_global_avg_tas}
    \includegraphics[width=7cm]{model_global_avg_tas}
    \centering
\end{figure}

\begin{figure}[t]
    \includegraphics[width=7cm]{model_corr}
    \centering
\end{figure}

{\noindent}Looking at a plot of all the models, there is a clear correlation between all the models and the correlation heatmap affirms this as the correlation between each
ranges from 0.96--1. 

%%%%%%%%%%%%%%%%
\section{Dataset}\label{data}
The datasets used are from the CMIP6 scenario runs made available on the KNMI Climate Explorer website. The KNMI is part of the
World Meteoriological Organization (WMO) and obtained from \url{https://climexp.knmi.nl/}. The scenario runs include monthly predictions for temperature, min temperature, max temperature
precipitation, radiation and pressure all over the globe in a 192$\times$144 grid between 1850--2100 for 40 different models. Due to the time and computational
limitations, this was filtered to just temperature during the 1960--1980 period in just Australia or latitudes -44{\textdegree} to -12{\textdegree} 
and longtitudes 288{\textdegree} to 336{\textdegree}. 

Talk a bit more about how these models in the dataset are all related by certain parts.
%%%%%%%%%%%%%%%%

{\section{Problem Formulation}}\label{problem-formulation}
\textit{Maybe add this to the background section and put more of the regression, diagrams of the process and shrinkage prior stuff here that it more specific to this thesis}

The final regression problem can be formulated as
As mentioned earlier, other variables such as precipitation, pressure and radiation are available but for simplicity, just temperature is currently being used.

\begin{equation}
f : L \times X \rightarrow Y
\end{equation}

\noindent where $f$ denotes the learning function, $L$ the graph, $X$ denotes the time series input and $Y$ the regression target.

Need to describe the math behind graph sparsification. More about shrinkage see Xiongwens.

{\section{Computational features}}\label{computation}
computation of neural network models. See georges paper 
{\section{Implementation}}\label{implementation}

Need to finish code to finish this section.

\chapter{Results}\label{results}

{\section{Model verification}}\label{model-verification}
If the VGAE section is completed, we can compared the sparsified graph with the VGAE
produced graph to determine how good graph sparsification is when used for graph discovery
and thereby correlation in a graph structure.

{\section{Model results}}\label{model-results}
Is there some way we can test the models results depending on how sparse we make the graph etc.
Research required to find some quantitative measure for this.

Some figures of the NN structure would also be helpful for this.
Need to use nx or some other graph representation tool in python for this.

\chapter{Discussion}\label{dis}

\chapter{Conclusion}\label{ccl}

\chapter{Appendix}\label{app}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\addcontentsline{toc}{chapter}{References}
\bibliography{bibliography}{}
\bibliographystyle{plain}

\end{document}





