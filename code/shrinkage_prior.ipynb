{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import ssp_data\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\\tas_scenario_245\\tas_mon_mod_ssp245_192_001.nc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ssp_obj \u001b[39m=\u001b[39m ssp_data()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ssp_obj\u001b[39m.\u001b[39mto_pickle(\u001b[39m'\u001b[39m\u001b[39mdata_pickle\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\data_generator.py:17\u001b[0m, in \u001b[0;36mssp_data.__init__\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtas_scenario_245\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtas_mon_mod_ssp245_192_000.nc\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_file_list \u001b[39m=\u001b[39m [item \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m glob(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtas_scenario_245\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtas_mon_mod_ssp245_192_*.nc\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m item \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_file]][\u001b[39m0\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn]\n\u001b[1;32m---> 17\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_df()\n\u001b[0;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_train_split()\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\data_generator.py:37\u001b[0m, in \u001b[0;36mssp_data.create_df\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProcessing\u001b[39m\u001b[39m'\u001b[39m, filename)\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mempty:\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_vector(filename)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(w\u001b[39m:=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_vector(filename)[\u001b[39m'\u001b[39m\u001b[39mtas\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\data_generator.py:55\u001b[0m, in \u001b[0;36mssp_data.create_vector\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m df \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto_dataframe()\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m     56\u001b[0m \u001b[39m# df = df.query('lat == -43.125 & lat == 288.750')\u001b[39;00m\n\u001b[0;32m     57\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mquery(\u001b[39m'\u001b[39m\u001b[39mlat >= -44 & lat <= -12 & lon >= 288 & lon <= 336\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\.conda\\lib\\site-packages\\xarray\\core\\dataset.py:6186\u001b[0m, in \u001b[0;36mDataset.to_dataframe\u001b[1;34m(self, dim_order)\u001b[0m\n\u001b[0;32m   6158\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert this dataset into a pandas.DataFrame.\u001b[39;00m\n\u001b[0;32m   6159\u001b[0m \n\u001b[0;32m   6160\u001b[0m \u001b[39mNon-index variables in this dataset form the columns of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6181\u001b[0m \n\u001b[0;32m   6182\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   6184\u001b[0m ordered_dims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_dim_order(dim_order\u001b[39m=\u001b[39mdim_order)\n\u001b[1;32m-> 6186\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_dataframe(ordered_dims\u001b[39m=\u001b[39;49mordered_dims)\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\.conda\\lib\\site-packages\\xarray\\core\\dataset.py:6154\u001b[0m, in \u001b[0;36mDataset._to_dataframe\u001b[1;34m(self, ordered_dims)\u001b[0m\n\u001b[0;32m   6149\u001b[0m columns \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims]\n\u001b[0;32m   6150\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[0;32m   6151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variables[k]\u001b[39m.\u001b[39mset_dims(ordered_dims)\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   6152\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m columns\n\u001b[0;32m   6153\u001b[0m ]\n\u001b[1;32m-> 6154\u001b[0m index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoords\u001b[39m.\u001b[39;49mto_index([\u001b[39m*\u001b[39;49mordered_dims])\n\u001b[0;32m   6155\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(columns, data)), index\u001b[39m=\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\.conda\\lib\\site-packages\\xarray\\core\\coordinates.py:150\u001b[0m, in \u001b[0;36mCoordinates.to_index\u001b[1;34m(self, ordered_dims)\u001b[0m\n\u001b[0;32m    147\u001b[0m     levels \u001b[39m=\u001b[39m [level]\n\u001b[0;32m    149\u001b[0m \u001b[39m# compute the cartesian product\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m code_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[0;32m    151\u001b[0m     np\u001b[39m.\u001b[39mtile(np\u001b[39m.\u001b[39mrepeat(code, repeat_counts[i]), tile_counts[i])\n\u001b[0;32m    152\u001b[0m     \u001b[39mfor\u001b[39;00m code \u001b[39min\u001b[39;00m codes\n\u001b[0;32m    153\u001b[0m ]\n\u001b[0;32m    154\u001b[0m level_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m levels\n\u001b[0;32m    155\u001b[0m names \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mnames\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\.conda\\lib\\site-packages\\xarray\\core\\coordinates.py:151\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    147\u001b[0m     levels \u001b[39m=\u001b[39m [level]\n\u001b[0;32m    149\u001b[0m \u001b[39m# compute the cartesian product\u001b[39;00m\n\u001b[0;32m    150\u001b[0m code_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[1;32m--> 151\u001b[0m     np\u001b[39m.\u001b[39mtile(np\u001b[39m.\u001b[39mrepeat(code, repeat_counts[i]), tile_counts[i])\n\u001b[0;32m    152\u001b[0m     \u001b[39mfor\u001b[39;00m code \u001b[39min\u001b[39;00m codes\n\u001b[0;32m    153\u001b[0m ]\n\u001b[0;32m    154\u001b[0m level_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m levels\n\u001b[0;32m    155\u001b[0m names \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mnames\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssp_obj = ssp_data()\n",
    "ssp_obj.to_pickle('data_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data_pickle', 'rb')\n",
    "ssp_obj = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[39, 163800], edge_index=[2, 1482], y=[163800])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssp_obj.train_data = ssp_obj.test_data\n",
    "ssp_obj.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([285.1003, 285.9076, 282.4460,  ..., 296.6862, 297.5055, 298.2751])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssp_obj.train_data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[39, 163800], edge_index=[2, 1482], y=[163800])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssp_obj.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "import torch\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.edge_weight = torch.nn.Parameter(torch.ones(ssp_obj.train_data.num_edges))\n",
    "        self.conv1 = GCNConv(-1, 36)\n",
    "        self.conv2 = GCNConv(36, 64)\n",
    "        # self.lin1 = torch.nn.Linear(64, 256)\n",
    "        self.lin1 = torch.nn.Linear(64, 163800)\n",
    "        # self.global_pool = global_mean_pool()\n",
    "\n",
    "    def forward(self, data, batch):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index, torch.minimum(self.edge_weight.abs(),torch.ones(data.num_edges)))\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, torch.minimum(self.edge_weight.abs(),torch.ones(data.num_edges)))\n",
    "        x = self.lin1(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = x.flatten()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(119825.8438, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(1.1555e+13, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(6.7426e+13, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(1.1333e+10, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(4.0541e+11, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(1.1107e+08, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(85242.1250, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(85325.8906, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(85411.3906, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(85496.3906, grad_fn=<MseLossBackward0>)\n",
      "10 tensor(85579.4922, grad_fn=<MseLossBackward0>)\n",
      "11 tensor(85659.7188, grad_fn=<MseLossBackward0>)\n",
      "12 tensor(85736.4688, grad_fn=<MseLossBackward0>)\n",
      "13 tensor(85809.3594, grad_fn=<MseLossBackward0>)\n",
      "14 tensor(85878.1875, grad_fn=<MseLossBackward0>)\n",
      "15 tensor(85942.8984, grad_fn=<MseLossBackward0>)\n",
      "16 tensor(86003.4375, grad_fn=<MseLossBackward0>)\n",
      "17 tensor(86059.9141, grad_fn=<MseLossBackward0>)\n",
      "18 tensor(86112.4219, grad_fn=<MseLossBackward0>)\n",
      "19 tensor(86161.1016, grad_fn=<MseLossBackward0>)\n",
      "20 tensor(86206.1172, grad_fn=<MseLossBackward0>)\n",
      "21 tensor(86247.6328, grad_fn=<MseLossBackward0>)\n",
      "22 tensor(86285.8359, grad_fn=<MseLossBackward0>)\n",
      "23 tensor(86320.9062, grad_fn=<MseLossBackward0>)\n",
      "24 tensor(86353.0391, grad_fn=<MseLossBackward0>)\n",
      "25 tensor(86382.3984, grad_fn=<MseLossBackward0>)\n",
      "26 tensor(86409.1797, grad_fn=<MseLossBackward0>)\n",
      "27 tensor(86433.5391, grad_fn=<MseLossBackward0>)\n",
      "28 tensor(86455.6562, grad_fn=<MseLossBackward0>)\n",
      "29 tensor(86475.6641, grad_fn=<MseLossBackward0>)\n",
      "30 tensor(86493.7266, grad_fn=<MseLossBackward0>)\n",
      "31 tensor(86509.9922, grad_fn=<MseLossBackward0>)\n",
      "32 tensor(86524.5703, grad_fn=<MseLossBackward0>)\n",
      "33 tensor(86537.5938, grad_fn=<MseLossBackward0>)\n",
      "34 tensor(86549.1953, grad_fn=<MseLossBackward0>)\n",
      "35 tensor(86559.4688, grad_fn=<MseLossBackward0>)\n",
      "36 tensor(86568.5156, grad_fn=<MseLossBackward0>)\n",
      "37 tensor(86576.4453, grad_fn=<MseLossBackward0>)\n",
      "38 tensor(86583.3281, grad_fn=<MseLossBackward0>)\n",
      "39 tensor(86589.2500, grad_fn=<MseLossBackward0>)\n",
      "40 tensor(86594.2734, grad_fn=<MseLossBackward0>)\n",
      "41 tensor(86598.5078, grad_fn=<MseLossBackward0>)\n",
      "42 tensor(86601.9844, grad_fn=<MseLossBackward0>)\n",
      "43 tensor(86604.7656, grad_fn=<MseLossBackward0>)\n",
      "44 tensor(86606.9219, grad_fn=<MseLossBackward0>)\n",
      "45 tensor(86608.4844, grad_fn=<MseLossBackward0>)\n",
      "46 tensor(86609.5156, grad_fn=<MseLossBackward0>)\n",
      "47 tensor(86610.0547, grad_fn=<MseLossBackward0>)\n",
      "48 tensor(86610.1328, grad_fn=<MseLossBackward0>)\n",
      "49 tensor(86609.8047, grad_fn=<MseLossBackward0>)\n",
      "50 tensor(86609.0703, grad_fn=<MseLossBackward0>)\n",
      "51 tensor(86608., grad_fn=<MseLossBackward0>)\n",
      "52 tensor(86606.5859, grad_fn=<MseLossBackward0>)\n",
      "53 tensor(86604.8906, grad_fn=<MseLossBackward0>)\n",
      "54 tensor(86602.9062, grad_fn=<MseLossBackward0>)\n",
      "55 tensor(86600.6641, grad_fn=<MseLossBackward0>)\n",
      "56 tensor(86598.1875, grad_fn=<MseLossBackward0>)\n",
      "57 tensor(86595.4922, grad_fn=<MseLossBackward0>)\n",
      "58 tensor(86592.5938, grad_fn=<MseLossBackward0>)\n",
      "59 tensor(86589.5000, grad_fn=<MseLossBackward0>)\n",
      "60 tensor(86586.2422, grad_fn=<MseLossBackward0>)\n",
      "61 tensor(86582.8125, grad_fn=<MseLossBackward0>)\n",
      "62 tensor(86579.2344, grad_fn=<MseLossBackward0>)\n",
      "63 tensor(86575.5234, grad_fn=<MseLossBackward0>)\n",
      "64 tensor(86571.6797, grad_fn=<MseLossBackward0>)\n",
      "65 tensor(86567.7188, grad_fn=<MseLossBackward0>)\n",
      "66 tensor(86563.6328, grad_fn=<MseLossBackward0>)\n",
      "67 tensor(86559.4453, grad_fn=<MseLossBackward0>)\n",
      "68 tensor(86555.1641, grad_fn=<MseLossBackward0>)\n",
      "69 tensor(86550.7812, grad_fn=<MseLossBackward0>)\n",
      "70 tensor(86546.3047, grad_fn=<MseLossBackward0>)\n",
      "71 tensor(86541.7578, grad_fn=<MseLossBackward0>)\n",
      "72 tensor(86537.1328, grad_fn=<MseLossBackward0>)\n",
      "73 tensor(86532.4297, grad_fn=<MseLossBackward0>)\n",
      "74 tensor(86527.6406, grad_fn=<MseLossBackward0>)\n",
      "75 tensor(86522.7969, grad_fn=<MseLossBackward0>)\n",
      "76 tensor(86517.8984, grad_fn=<MseLossBackward0>)\n",
      "77 tensor(86512.9297, grad_fn=<MseLossBackward0>)\n",
      "78 tensor(86507.8984, grad_fn=<MseLossBackward0>)\n",
      "79 tensor(86502.8125, grad_fn=<MseLossBackward0>)\n",
      "80 tensor(86497.6641, grad_fn=<MseLossBackward0>)\n",
      "81 tensor(86492.4766, grad_fn=<MseLossBackward0>)\n",
      "82 tensor(86487.2422, grad_fn=<MseLossBackward0>)\n",
      "83 tensor(86481.9609, grad_fn=<MseLossBackward0>)\n",
      "84 tensor(86476.6094, grad_fn=<MseLossBackward0>)\n",
      "85 tensor(86471.2344, grad_fn=<MseLossBackward0>)\n",
      "86 tensor(86465.8125, grad_fn=<MseLossBackward0>)\n",
      "87 tensor(86460.3359, grad_fn=<MseLossBackward0>)\n",
      "88 tensor(86454.8359, grad_fn=<MseLossBackward0>)\n",
      "89 tensor(86449.2891, grad_fn=<MseLossBackward0>)\n",
      "90 tensor(86443.6953, grad_fn=<MseLossBackward0>)\n",
      "91 tensor(86438.0625, grad_fn=<MseLossBackward0>)\n",
      "92 tensor(86432.3984, grad_fn=<MseLossBackward0>)\n",
      "93 tensor(86426.6875, grad_fn=<MseLossBackward0>)\n",
      "94 tensor(86420.9453, grad_fn=<MseLossBackward0>)\n",
      "95 tensor(86415.1719, grad_fn=<MseLossBackward0>)\n",
      "96 tensor(86409.3516, grad_fn=<MseLossBackward0>)\n",
      "97 tensor(86403.5078, grad_fn=<MseLossBackward0>)\n",
      "98 tensor(86397.6328, grad_fn=<MseLossBackward0>)\n",
      "99 tensor(86391.7031, grad_fn=<MseLossBackward0>)\n",
      "100 tensor(86385.7500, grad_fn=<MseLossBackward0>)\n",
      "101 tensor(86379.7656, grad_fn=<MseLossBackward0>)\n",
      "102 tensor(86373.7500, grad_fn=<MseLossBackward0>)\n",
      "103 tensor(86367.6953, grad_fn=<MseLossBackward0>)\n",
      "104 tensor(86361.6250, grad_fn=<MseLossBackward0>)\n",
      "105 tensor(86355.5078, grad_fn=<MseLossBackward0>)\n",
      "106 tensor(86349.3594, grad_fn=<MseLossBackward0>)\n",
      "107 tensor(86343.1797, grad_fn=<MseLossBackward0>)\n",
      "108 tensor(86336.9766, grad_fn=<MseLossBackward0>)\n",
      "109 tensor(86330.7344, grad_fn=<MseLossBackward0>)\n",
      "110 tensor(86324.4688, grad_fn=<MseLossBackward0>)\n",
      "111 tensor(86318.1641, grad_fn=<MseLossBackward0>)\n",
      "112 tensor(86311.8438, grad_fn=<MseLossBackward0>)\n",
      "113 tensor(86305.4688, grad_fn=<MseLossBackward0>)\n",
      "114 tensor(86299.0781, grad_fn=<MseLossBackward0>)\n",
      "115 tensor(86292.6641, grad_fn=<MseLossBackward0>)\n",
      "116 tensor(86286.2188, grad_fn=<MseLossBackward0>)\n",
      "117 tensor(86279.7422, grad_fn=<MseLossBackward0>)\n",
      "118 tensor(86273.2344, grad_fn=<MseLossBackward0>)\n",
      "119 tensor(86266.6953, grad_fn=<MseLossBackward0>)\n",
      "120 tensor(86260.1250, grad_fn=<MseLossBackward0>)\n",
      "121 tensor(86253.5234, grad_fn=<MseLossBackward0>)\n",
      "122 tensor(86246.9062, grad_fn=<MseLossBackward0>)\n",
      "123 tensor(86240.2578, grad_fn=<MseLossBackward0>)\n",
      "124 tensor(86233.5703, grad_fn=<MseLossBackward0>)\n",
      "125 tensor(86226.8672, grad_fn=<MseLossBackward0>)\n",
      "126 tensor(86220.1250, grad_fn=<MseLossBackward0>)\n",
      "127 tensor(86213.3750, grad_fn=<MseLossBackward0>)\n",
      "128 tensor(86206.5703, grad_fn=<MseLossBackward0>)\n",
      "129 tensor(86199.7578, grad_fn=<MseLossBackward0>)\n",
      "130 tensor(86192.9062, grad_fn=<MseLossBackward0>)\n",
      "131 tensor(86186.0312, grad_fn=<MseLossBackward0>)\n",
      "132 tensor(86179.1328, grad_fn=<MseLossBackward0>)\n",
      "133 tensor(86172.2031, grad_fn=<MseLossBackward0>)\n",
      "134 tensor(86165.2500, grad_fn=<MseLossBackward0>)\n",
      "135 tensor(86158.2578, grad_fn=<MseLossBackward0>)\n",
      "136 tensor(86151.2500, grad_fn=<MseLossBackward0>)\n",
      "137 tensor(86144.2188, grad_fn=<MseLossBackward0>)\n",
      "138 tensor(86137.1484, grad_fn=<MseLossBackward0>)\n",
      "139 tensor(86130.0625, grad_fn=<MseLossBackward0>)\n",
      "140 tensor(86122.9453, grad_fn=<MseLossBackward0>)\n",
      "141 tensor(86115.7969, grad_fn=<MseLossBackward0>)\n",
      "142 tensor(86108.6328, grad_fn=<MseLossBackward0>)\n",
      "143 tensor(86101.4297, grad_fn=<MseLossBackward0>)\n",
      "144 tensor(86094.2031, grad_fn=<MseLossBackward0>)\n",
      "145 tensor(86086.9609, grad_fn=<MseLossBackward0>)\n",
      "146 tensor(86079.6875, grad_fn=<MseLossBackward0>)\n",
      "147 tensor(86072.3906, grad_fn=<MseLossBackward0>)\n",
      "148 tensor(86065.0469, grad_fn=<MseLossBackward0>)\n",
      "149 tensor(86057.7109, grad_fn=<MseLossBackward0>)\n",
      "150 tensor(86050.3281, grad_fn=<MseLossBackward0>)\n",
      "151 tensor(86042.9219, grad_fn=<MseLossBackward0>)\n",
      "152 tensor(86035.4844, grad_fn=<MseLossBackward0>)\n",
      "153 tensor(86028.0312, grad_fn=<MseLossBackward0>)\n",
      "154 tensor(86020.5625, grad_fn=<MseLossBackward0>)\n",
      "155 tensor(86013.0469, grad_fn=<MseLossBackward0>)\n",
      "156 tensor(86005.5156, grad_fn=<MseLossBackward0>)\n",
      "157 tensor(85997.9609, grad_fn=<MseLossBackward0>)\n",
      "158 tensor(85990.3672, grad_fn=<MseLossBackward0>)\n",
      "159 tensor(85982.7656, grad_fn=<MseLossBackward0>)\n",
      "160 tensor(85975.1406, grad_fn=<MseLossBackward0>)\n",
      "161 tensor(85967.4844, grad_fn=<MseLossBackward0>)\n",
      "162 tensor(85959.7969, grad_fn=<MseLossBackward0>)\n",
      "163 tensor(85952.0938, grad_fn=<MseLossBackward0>)\n",
      "164 tensor(85944.3594, grad_fn=<MseLossBackward0>)\n",
      "165 tensor(85936.6094, grad_fn=<MseLossBackward0>)\n",
      "166 tensor(85928.8281, grad_fn=<MseLossBackward0>)\n",
      "167 tensor(85921.0234, grad_fn=<MseLossBackward0>)\n",
      "168 tensor(85913.1953, grad_fn=<MseLossBackward0>)\n",
      "169 tensor(85905.3359, grad_fn=<MseLossBackward0>)\n",
      "170 tensor(85897.4609, grad_fn=<MseLossBackward0>)\n",
      "171 tensor(85889.5625, grad_fn=<MseLossBackward0>)\n",
      "172 tensor(85881.6328, grad_fn=<MseLossBackward0>)\n",
      "173 tensor(85873.6797, grad_fn=<MseLossBackward0>)\n",
      "174 tensor(85865.7109, grad_fn=<MseLossBackward0>)\n",
      "175 tensor(85857.7109, grad_fn=<MseLossBackward0>)\n",
      "176 tensor(85849.6875, grad_fn=<MseLossBackward0>)\n",
      "177 tensor(85841.6406, grad_fn=<MseLossBackward0>)\n",
      "178 tensor(85833.5703, grad_fn=<MseLossBackward0>)\n",
      "179 tensor(85825.4766, grad_fn=<MseLossBackward0>)\n",
      "180 tensor(85817.3516, grad_fn=<MseLossBackward0>)\n",
      "181 tensor(85809.2188, grad_fn=<MseLossBackward0>)\n",
      "182 tensor(85801.0469, grad_fn=<MseLossBackward0>)\n",
      "183 tensor(85792.8672, grad_fn=<MseLossBackward0>)\n",
      "184 tensor(85784.6484, grad_fn=<MseLossBackward0>)\n",
      "185 tensor(85776.4141, grad_fn=<MseLossBackward0>)\n",
      "186 tensor(85768.1484, grad_fn=<MseLossBackward0>)\n",
      "187 tensor(85759.8750, grad_fn=<MseLossBackward0>)\n",
      "188 tensor(85751.5703, grad_fn=<MseLossBackward0>)\n",
      "189 tensor(85743.2422, grad_fn=<MseLossBackward0>)\n",
      "190 tensor(85734.8828, grad_fn=<MseLossBackward0>)\n",
      "191 tensor(85726.5078, grad_fn=<MseLossBackward0>)\n",
      "192 tensor(85718.1172, grad_fn=<MseLossBackward0>)\n",
      "193 tensor(85709.6875, grad_fn=<MseLossBackward0>)\n",
      "194 tensor(85701.2500, grad_fn=<MseLossBackward0>)\n",
      "195 tensor(85692.7812, grad_fn=<MseLossBackward0>)\n",
      "196 tensor(85684.3047, grad_fn=<MseLossBackward0>)\n",
      "197 tensor(85675.7812, grad_fn=<MseLossBackward0>)\n",
      "198 tensor(85667.2500, grad_fn=<MseLossBackward0>)\n",
      "199 tensor(85658.6875, grad_fn=<MseLossBackward0>)\n",
      "200 tensor(85650.1172, grad_fn=<MseLossBackward0>)\n",
      "201 tensor(85641.5078, grad_fn=<MseLossBackward0>)\n",
      "202 tensor(85632.8828, grad_fn=<MseLossBackward0>)\n",
      "203 tensor(85624.2344, grad_fn=<MseLossBackward0>)\n",
      "204 tensor(85615.5625, grad_fn=<MseLossBackward0>)\n",
      "205 tensor(85606.8750, grad_fn=<MseLossBackward0>)\n",
      "206 tensor(85598.1641, grad_fn=<MseLossBackward0>)\n",
      "207 tensor(85589.4141, grad_fn=<MseLossBackward0>)\n",
      "208 tensor(85580.6641, grad_fn=<MseLossBackward0>)\n",
      "209 tensor(85571.8828, grad_fn=<MseLossBackward0>)\n",
      "210 tensor(85563.0859, grad_fn=<MseLossBackward0>)\n",
      "211 tensor(85554.2500, grad_fn=<MseLossBackward0>)\n",
      "212 tensor(85545.4062, grad_fn=<MseLossBackward0>)\n",
      "213 tensor(85536.5391, grad_fn=<MseLossBackward0>)\n",
      "214 tensor(85527.6406, grad_fn=<MseLossBackward0>)\n",
      "215 tensor(85518.7344, grad_fn=<MseLossBackward0>)\n",
      "216 tensor(85509.7969, grad_fn=<MseLossBackward0>)\n",
      "217 tensor(85500.8359, grad_fn=<MseLossBackward0>)\n",
      "218 tensor(85491.8594, grad_fn=<MseLossBackward0>)\n",
      "219 tensor(85482.8594, grad_fn=<MseLossBackward0>)\n",
      "220 tensor(85473.8281, grad_fn=<MseLossBackward0>)\n",
      "221 tensor(85464.7969, grad_fn=<MseLossBackward0>)\n",
      "222 tensor(85455.7266, grad_fn=<MseLossBackward0>)\n",
      "223 tensor(85446.6406, grad_fn=<MseLossBackward0>)\n",
      "224 tensor(85437.5312, grad_fn=<MseLossBackward0>)\n",
      "225 tensor(85428.4062, grad_fn=<MseLossBackward0>)\n",
      "226 tensor(85419.2578, grad_fn=<MseLossBackward0>)\n",
      "227 tensor(85410.0781, grad_fn=<MseLossBackward0>)\n",
      "228 tensor(85400.8906, grad_fn=<MseLossBackward0>)\n",
      "229 tensor(85391.6719, grad_fn=<MseLossBackward0>)\n",
      "230 tensor(85382.4297, grad_fn=<MseLossBackward0>)\n",
      "231 tensor(85373.1797, grad_fn=<MseLossBackward0>)\n",
      "232 tensor(85363.9062, grad_fn=<MseLossBackward0>)\n",
      "233 tensor(85354.6016, grad_fn=<MseLossBackward0>)\n",
      "234 tensor(85345.2734, grad_fn=<MseLossBackward0>)\n",
      "235 tensor(85335.9375, grad_fn=<MseLossBackward0>)\n",
      "236 tensor(85326.5781, grad_fn=<MseLossBackward0>)\n",
      "237 tensor(85317.1953, grad_fn=<MseLossBackward0>)\n",
      "238 tensor(85307.7891, grad_fn=<MseLossBackward0>)\n",
      "239 tensor(85298.3594, grad_fn=<MseLossBackward0>)\n",
      "240 tensor(85288.9062, grad_fn=<MseLossBackward0>)\n",
      "241 tensor(85279.4453, grad_fn=<MseLossBackward0>)\n",
      "242 tensor(85269.9531, grad_fn=<MseLossBackward0>)\n",
      "243 tensor(85260.4531, grad_fn=<MseLossBackward0>)\n",
      "244 tensor(85250.9219, grad_fn=<MseLossBackward0>)\n",
      "245 tensor(85241.3828, grad_fn=<MseLossBackward0>)\n",
      "246 tensor(85231.8047, grad_fn=<MseLossBackward0>)\n",
      "247 tensor(85222.2031, grad_fn=<MseLossBackward0>)\n",
      "248 tensor(85212.6016, grad_fn=<MseLossBackward0>)\n",
      "249 tensor(85202.9766, grad_fn=<MseLossBackward0>)\n",
      "250 tensor(85193.3203, grad_fn=<MseLossBackward0>)\n",
      "251 tensor(85183.6406, grad_fn=<MseLossBackward0>)\n",
      "252 tensor(85173.9531, grad_fn=<MseLossBackward0>)\n",
      "253 tensor(85164.2422, grad_fn=<MseLossBackward0>)\n",
      "254 tensor(85154.5078, grad_fn=<MseLossBackward0>)\n",
      "255 tensor(85144.7500, grad_fn=<MseLossBackward0>)\n",
      "256 tensor(85134.9688, grad_fn=<MseLossBackward0>)\n",
      "257 tensor(85125.1797, grad_fn=<MseLossBackward0>)\n",
      "258 tensor(85115.3672, grad_fn=<MseLossBackward0>)\n",
      "259 tensor(85105.5312, grad_fn=<MseLossBackward0>)\n",
      "260 tensor(85095.6719, grad_fn=<MseLossBackward0>)\n",
      "261 tensor(85085.7969, grad_fn=<MseLossBackward0>)\n",
      "262 tensor(85075.9062, grad_fn=<MseLossBackward0>)\n",
      "263 tensor(85065.9844, grad_fn=<MseLossBackward0>)\n",
      "264 tensor(85056.0547, grad_fn=<MseLossBackward0>)\n",
      "265 tensor(85046.1016, grad_fn=<MseLossBackward0>)\n",
      "266 tensor(85036.1250, grad_fn=<MseLossBackward0>)\n",
      "267 tensor(85026.1250, grad_fn=<MseLossBackward0>)\n",
      "268 tensor(85016.1172, grad_fn=<MseLossBackward0>)\n",
      "269 tensor(85006.0781, grad_fn=<MseLossBackward0>)\n",
      "270 tensor(84996.0312, grad_fn=<MseLossBackward0>)\n",
      "271 tensor(84985.9609, grad_fn=<MseLossBackward0>)\n",
      "272 tensor(84975.8594, grad_fn=<MseLossBackward0>)\n",
      "273 tensor(84965.7500, grad_fn=<MseLossBackward0>)\n",
      "274 tensor(84955.6172, grad_fn=<MseLossBackward0>)\n",
      "275 tensor(84945.4609, grad_fn=<MseLossBackward0>)\n",
      "276 tensor(84935.2891, grad_fn=<MseLossBackward0>)\n",
      "277 tensor(84925.1016, grad_fn=<MseLossBackward0>)\n",
      "278 tensor(84914.8906, grad_fn=<MseLossBackward0>)\n",
      "279 tensor(84904.6562, grad_fn=<MseLossBackward0>)\n",
      "280 tensor(84894.4062, grad_fn=<MseLossBackward0>)\n",
      "281 tensor(84884.1328, grad_fn=<MseLossBackward0>)\n",
      "282 tensor(84873.8438, grad_fn=<MseLossBackward0>)\n",
      "283 tensor(84863.5469, grad_fn=<MseLossBackward0>)\n",
      "284 tensor(84853.2188, grad_fn=<MseLossBackward0>)\n",
      "285 tensor(84842.8672, grad_fn=<MseLossBackward0>)\n",
      "286 tensor(84832.5078, grad_fn=<MseLossBackward0>)\n",
      "287 tensor(84822.1172, grad_fn=<MseLossBackward0>)\n",
      "288 tensor(84811.7188, grad_fn=<MseLossBackward0>)\n",
      "289 tensor(84801.2891, grad_fn=<MseLossBackward0>)\n",
      "290 tensor(84790.8516, grad_fn=<MseLossBackward0>)\n",
      "291 tensor(84780.3828, grad_fn=<MseLossBackward0>)\n",
      "292 tensor(84769.9062, grad_fn=<MseLossBackward0>)\n",
      "293 tensor(84759.4062, grad_fn=<MseLossBackward0>)\n",
      "294 tensor(84748.8906, grad_fn=<MseLossBackward0>)\n",
      "295 tensor(84738.3516, grad_fn=<MseLossBackward0>)\n",
      "296 tensor(84727.7891, grad_fn=<MseLossBackward0>)\n",
      "297 tensor(84717.2188, grad_fn=<MseLossBackward0>)\n",
      "298 tensor(84706.6328, grad_fn=<MseLossBackward0>)\n",
      "299 tensor(84696.0078, grad_fn=<MseLossBackward0>)\n",
      "300 tensor(84685.3906, grad_fn=<MseLossBackward0>)\n",
      "301 tensor(84674.7266, grad_fn=<MseLossBackward0>)\n",
      "302 tensor(84664.0547, grad_fn=<MseLossBackward0>)\n",
      "303 tensor(84653.3672, grad_fn=<MseLossBackward0>)\n",
      "304 tensor(84642.6641, grad_fn=<MseLossBackward0>)\n",
      "305 tensor(84631.9375, grad_fn=<MseLossBackward0>)\n",
      "306 tensor(84621.1953, grad_fn=<MseLossBackward0>)\n",
      "307 tensor(84610.4297, grad_fn=<MseLossBackward0>)\n",
      "308 tensor(84599.6562, grad_fn=<MseLossBackward0>)\n",
      "309 tensor(84588.8516, grad_fn=<MseLossBackward0>)\n",
      "310 tensor(84578.0312, grad_fn=<MseLossBackward0>)\n",
      "311 tensor(84567.1953, grad_fn=<MseLossBackward0>)\n",
      "312 tensor(84556.3438, grad_fn=<MseLossBackward0>)\n",
      "313 tensor(84545.4688, grad_fn=<MseLossBackward0>)\n",
      "314 tensor(84534.5859, grad_fn=<MseLossBackward0>)\n",
      "315 tensor(84523.6641, grad_fn=<MseLossBackward0>)\n",
      "316 tensor(84512.7422, grad_fn=<MseLossBackward0>)\n",
      "317 tensor(84501.7891, grad_fn=<MseLossBackward0>)\n",
      "318 tensor(84490.8281, grad_fn=<MseLossBackward0>)\n",
      "319 tensor(84479.8359, grad_fn=<MseLossBackward0>)\n",
      "320 tensor(84468.8359, grad_fn=<MseLossBackward0>)\n",
      "321 tensor(84457.8203, grad_fn=<MseLossBackward0>)\n",
      "322 tensor(84446.7812, grad_fn=<MseLossBackward0>)\n",
      "323 tensor(84435.7109, grad_fn=<MseLossBackward0>)\n",
      "324 tensor(84424.6484, grad_fn=<MseLossBackward0>)\n",
      "325 tensor(84413.5469, grad_fn=<MseLossBackward0>)\n",
      "326 tensor(84402.4375, grad_fn=<MseLossBackward0>)\n",
      "327 tensor(84391.3047, grad_fn=<MseLossBackward0>)\n",
      "328 tensor(84380.1641, grad_fn=<MseLossBackward0>)\n",
      "329 tensor(84368.9922, grad_fn=<MseLossBackward0>)\n",
      "330 tensor(84357.8125, grad_fn=<MseLossBackward0>)\n",
      "331 tensor(84346.6094, grad_fn=<MseLossBackward0>)\n",
      "332 tensor(84335.3906, grad_fn=<MseLossBackward0>)\n",
      "333 tensor(84324.1484, grad_fn=<MseLossBackward0>)\n",
      "334 tensor(84312.8984, grad_fn=<MseLossBackward0>)\n",
      "335 tensor(84301.6250, grad_fn=<MseLossBackward0>)\n",
      "336 tensor(84290.3281, grad_fn=<MseLossBackward0>)\n",
      "337 tensor(84279.0234, grad_fn=<MseLossBackward0>)\n",
      "338 tensor(84267.6953, grad_fn=<MseLossBackward0>)\n",
      "339 tensor(84256.3438, grad_fn=<MseLossBackward0>)\n",
      "340 tensor(84244.9844, grad_fn=<MseLossBackward0>)\n",
      "341 tensor(84233.6016, grad_fn=<MseLossBackward0>)\n",
      "342 tensor(84222.2109, grad_fn=<MseLossBackward0>)\n",
      "343 tensor(84210.7969, grad_fn=<MseLossBackward0>)\n",
      "344 tensor(84199.3594, grad_fn=<MseLossBackward0>)\n",
      "345 tensor(84187.9062, grad_fn=<MseLossBackward0>)\n",
      "346 tensor(84176.4375, grad_fn=<MseLossBackward0>)\n",
      "347 tensor(84164.9531, grad_fn=<MseLossBackward0>)\n",
      "348 tensor(84153.4453, grad_fn=<MseLossBackward0>)\n",
      "349 tensor(84141.9219, grad_fn=<MseLossBackward0>)\n",
      "350 tensor(84130.3906, grad_fn=<MseLossBackward0>)\n",
      "351 tensor(84118.8281, grad_fn=<MseLossBackward0>)\n",
      "352 tensor(84107.2578, grad_fn=<MseLossBackward0>)\n",
      "353 tensor(84095.6719, grad_fn=<MseLossBackward0>)\n",
      "354 tensor(84084.0625, grad_fn=<MseLossBackward0>)\n",
      "355 tensor(84072.4297, grad_fn=<MseLossBackward0>)\n",
      "356 tensor(84060.7891, grad_fn=<MseLossBackward0>)\n",
      "357 tensor(84049.1328, grad_fn=<MseLossBackward0>)\n",
      "358 tensor(84037.4531, grad_fn=<MseLossBackward0>)\n",
      "359 tensor(84025.7500, grad_fn=<MseLossBackward0>)\n",
      "360 tensor(84014.0391, grad_fn=<MseLossBackward0>)\n",
      "361 tensor(84002.3125, grad_fn=<MseLossBackward0>)\n",
      "362 tensor(83990.5703, grad_fn=<MseLossBackward0>)\n",
      "363 tensor(83978.8047, grad_fn=<MseLossBackward0>)\n",
      "364 tensor(83967.0156, grad_fn=<MseLossBackward0>)\n",
      "365 tensor(83955.2188, grad_fn=<MseLossBackward0>)\n",
      "366 tensor(83943.4062, grad_fn=<MseLossBackward0>)\n",
      "367 tensor(83931.5625, grad_fn=<MseLossBackward0>)\n",
      "368 tensor(83919.7188, grad_fn=<MseLossBackward0>)\n",
      "369 tensor(83907.8516, grad_fn=<MseLossBackward0>)\n",
      "370 tensor(83895.9609, grad_fn=<MseLossBackward0>)\n",
      "371 tensor(83884.0703, grad_fn=<MseLossBackward0>)\n",
      "372 tensor(83872.1484, grad_fn=<MseLossBackward0>)\n",
      "373 tensor(83860.2109, grad_fn=<MseLossBackward0>)\n",
      "374 tensor(83848.2578, grad_fn=<MseLossBackward0>)\n",
      "375 tensor(83836.2891, grad_fn=<MseLossBackward0>)\n",
      "376 tensor(83824.3047, grad_fn=<MseLossBackward0>)\n",
      "377 tensor(83812.2969, grad_fn=<MseLossBackward0>)\n",
      "378 tensor(83800.2812, grad_fn=<MseLossBackward0>)\n",
      "379 tensor(83788.2422, grad_fn=<MseLossBackward0>)\n",
      "380 tensor(83776.1875, grad_fn=<MseLossBackward0>)\n",
      "381 tensor(83764.1094, grad_fn=<MseLossBackward0>)\n",
      "382 tensor(83752.0234, grad_fn=<MseLossBackward0>)\n",
      "383 tensor(83739.9219, grad_fn=<MseLossBackward0>)\n",
      "384 tensor(83727.8047, grad_fn=<MseLossBackward0>)\n",
      "385 tensor(83715.6562, grad_fn=<MseLossBackward0>)\n",
      "386 tensor(83703.5078, grad_fn=<MseLossBackward0>)\n",
      "387 tensor(83691.3359, grad_fn=<MseLossBackward0>)\n",
      "388 tensor(83679.1484, grad_fn=<MseLossBackward0>)\n",
      "389 tensor(83666.9375, grad_fn=<MseLossBackward0>)\n",
      "390 tensor(83654.7188, grad_fn=<MseLossBackward0>)\n",
      "391 tensor(83642.4844, grad_fn=<MseLossBackward0>)\n",
      "392 tensor(83630.2266, grad_fn=<MseLossBackward0>)\n",
      "393 tensor(83617.9609, grad_fn=<MseLossBackward0>)\n",
      "394 tensor(83605.6641, grad_fn=<MseLossBackward0>)\n",
      "395 tensor(83593.3672, grad_fn=<MseLossBackward0>)\n",
      "396 tensor(83581.0469, grad_fn=<MseLossBackward0>)\n",
      "397 tensor(83568.7031, grad_fn=<MseLossBackward0>)\n",
      "398 tensor(83556.3516, grad_fn=<MseLossBackward0>)\n",
      "399 tensor(83543.9766, grad_fn=<MseLossBackward0>)\n",
      "400 tensor(83531.5938, grad_fn=<MseLossBackward0>)\n",
      "401 tensor(83519.1875, grad_fn=<MseLossBackward0>)\n",
      "402 tensor(83506.7734, grad_fn=<MseLossBackward0>)\n",
      "403 tensor(83494.3359, grad_fn=<MseLossBackward0>)\n",
      "404 tensor(83481.8906, grad_fn=<MseLossBackward0>)\n",
      "405 tensor(83469.4141, grad_fn=<MseLossBackward0>)\n",
      "406 tensor(83456.9219, grad_fn=<MseLossBackward0>)\n",
      "407 tensor(83444.4219, grad_fn=<MseLossBackward0>)\n",
      "408 tensor(83431.9062, grad_fn=<MseLossBackward0>)\n",
      "409 tensor(83419.3750, grad_fn=<MseLossBackward0>)\n",
      "410 tensor(83406.8281, grad_fn=<MseLossBackward0>)\n",
      "411 tensor(83394.2578, grad_fn=<MseLossBackward0>)\n",
      "412 tensor(83381.6719, grad_fn=<MseLossBackward0>)\n",
      "413 tensor(83369.0781, grad_fn=<MseLossBackward0>)\n",
      "414 tensor(83356.4609, grad_fn=<MseLossBackward0>)\n",
      "415 tensor(83343.8281, grad_fn=<MseLossBackward0>)\n",
      "416 tensor(83331.1875, grad_fn=<MseLossBackward0>)\n",
      "417 tensor(83318.5234, grad_fn=<MseLossBackward0>)\n",
      "418 tensor(83305.8359, grad_fn=<MseLossBackward0>)\n",
      "419 tensor(83293.1484, grad_fn=<MseLossBackward0>)\n",
      "420 tensor(83280.4297, grad_fn=<MseLossBackward0>)\n",
      "421 tensor(83267.7109, grad_fn=<MseLossBackward0>)\n",
      "422 tensor(83254.9609, grad_fn=<MseLossBackward0>)\n",
      "423 tensor(83242.1953, grad_fn=<MseLossBackward0>)\n",
      "424 tensor(83229.4297, grad_fn=<MseLossBackward0>)\n",
      "425 tensor(83216.6328, grad_fn=<MseLossBackward0>)\n",
      "426 tensor(83203.8203, grad_fn=<MseLossBackward0>)\n",
      "427 tensor(83191.0078, grad_fn=<MseLossBackward0>)\n",
      "428 tensor(83178.1562, grad_fn=<MseLossBackward0>)\n",
      "429 tensor(83165.3047, grad_fn=<MseLossBackward0>)\n",
      "430 tensor(83152.4375, grad_fn=<MseLossBackward0>)\n",
      "431 tensor(83139.5469, grad_fn=<MseLossBackward0>)\n",
      "432 tensor(83126.6484, grad_fn=<MseLossBackward0>)\n",
      "433 tensor(83113.7266, grad_fn=<MseLossBackward0>)\n",
      "434 tensor(83100.7969, grad_fn=<MseLossBackward0>)\n",
      "435 tensor(83087.8438, grad_fn=<MseLossBackward0>)\n",
      "436 tensor(83074.8750, grad_fn=<MseLossBackward0>)\n",
      "437 tensor(83061.8906, grad_fn=<MseLossBackward0>)\n",
      "438 tensor(83048.8984, grad_fn=<MseLossBackward0>)\n",
      "439 tensor(83035.8906, grad_fn=<MseLossBackward0>)\n",
      "440 tensor(83022.8594, grad_fn=<MseLossBackward0>)\n",
      "441 tensor(83009.8125, grad_fn=<MseLossBackward0>)\n",
      "442 tensor(82996.7500, grad_fn=<MseLossBackward0>)\n",
      "443 tensor(82983.6875, grad_fn=<MseLossBackward0>)\n",
      "444 tensor(82970.5781, grad_fn=<MseLossBackward0>)\n",
      "445 tensor(82957.4766, grad_fn=<MseLossBackward0>)\n",
      "446 tensor(82944.3594, grad_fn=<MseLossBackward0>)\n",
      "447 tensor(82931.2188, grad_fn=<MseLossBackward0>)\n",
      "448 tensor(82918.0625, grad_fn=<MseLossBackward0>)\n",
      "449 tensor(82904.8906, grad_fn=<MseLossBackward0>)\n",
      "450 tensor(82891.7109, grad_fn=<MseLossBackward0>)\n",
      "451 tensor(82878.5078, grad_fn=<MseLossBackward0>)\n",
      "452 tensor(82865.2969, grad_fn=<MseLossBackward0>)\n",
      "453 tensor(82852.0625, grad_fn=<MseLossBackward0>)\n",
      "454 tensor(82838.8281, grad_fn=<MseLossBackward0>)\n",
      "455 tensor(82825.5547, grad_fn=<MseLossBackward0>)\n",
      "456 tensor(82812.2812, grad_fn=<MseLossBackward0>)\n",
      "457 tensor(82798.9922, grad_fn=<MseLossBackward0>)\n",
      "458 tensor(82785.6719, grad_fn=<MseLossBackward0>)\n",
      "459 tensor(82772.3594, grad_fn=<MseLossBackward0>)\n",
      "460 tensor(82759.0078, grad_fn=<MseLossBackward0>)\n",
      "461 tensor(82745.6484, grad_fn=<MseLossBackward0>)\n",
      "462 tensor(82732.2891, grad_fn=<MseLossBackward0>)\n",
      "463 tensor(82718.8984, grad_fn=<MseLossBackward0>)\n",
      "464 tensor(82705.5078, grad_fn=<MseLossBackward0>)\n",
      "465 tensor(82692.0938, grad_fn=<MseLossBackward0>)\n",
      "466 tensor(82678.6641, grad_fn=<MseLossBackward0>)\n",
      "467 tensor(82665.2109, grad_fn=<MseLossBackward0>)\n",
      "468 tensor(82651.7500, grad_fn=<MseLossBackward0>)\n",
      "469 tensor(82638.2734, grad_fn=<MseLossBackward0>)\n",
      "470 tensor(82624.7812, grad_fn=<MseLossBackward0>)\n",
      "471 tensor(82611.2812, grad_fn=<MseLossBackward0>)\n",
      "472 tensor(82597.7578, grad_fn=<MseLossBackward0>)\n",
      "473 tensor(82584.2188, grad_fn=<MseLossBackward0>)\n",
      "474 tensor(82570.6797, grad_fn=<MseLossBackward0>)\n",
      "475 tensor(82557.1016, grad_fn=<MseLossBackward0>)\n",
      "476 tensor(82543.5234, grad_fn=<MseLossBackward0>)\n",
      "477 tensor(82529.9219, grad_fn=<MseLossBackward0>)\n",
      "478 tensor(82516.3125, grad_fn=<MseLossBackward0>)\n",
      "479 tensor(82502.6875, grad_fn=<MseLossBackward0>)\n",
      "480 tensor(82489.0469, grad_fn=<MseLossBackward0>)\n",
      "481 tensor(82475.3828, grad_fn=<MseLossBackward0>)\n",
      "482 tensor(82461.7188, grad_fn=<MseLossBackward0>)\n",
      "483 tensor(82448.0312, grad_fn=<MseLossBackward0>)\n",
      "484 tensor(82434.3281, grad_fn=<MseLossBackward0>)\n",
      "485 tensor(82420.6094, grad_fn=<MseLossBackward0>)\n",
      "486 tensor(82406.8828, grad_fn=<MseLossBackward0>)\n",
      "487 tensor(82393.1328, grad_fn=<MseLossBackward0>)\n",
      "488 tensor(82379.3672, grad_fn=<MseLossBackward0>)\n",
      "489 tensor(82365.5938, grad_fn=<MseLossBackward0>)\n",
      "490 tensor(82351.8047, grad_fn=<MseLossBackward0>)\n",
      "491 tensor(82338., grad_fn=<MseLossBackward0>)\n",
      "492 tensor(82324.1719, grad_fn=<MseLossBackward0>)\n",
      "493 tensor(82310.3438, grad_fn=<MseLossBackward0>)\n",
      "494 tensor(82296.4922, grad_fn=<MseLossBackward0>)\n",
      "495 tensor(82282.6172, grad_fn=<MseLossBackward0>)\n",
      "496 tensor(82268.7500, grad_fn=<MseLossBackward0>)\n",
      "497 tensor(82254.8594, grad_fn=<MseLossBackward0>)\n",
      "498 tensor(82240.9453, grad_fn=<MseLossBackward0>)\n",
      "499 tensor(82227.0234, grad_fn=<MseLossBackward0>)\n",
      "500 tensor(82213.0938, grad_fn=<MseLossBackward0>)\n",
      "501 tensor(82199.1328, grad_fn=<MseLossBackward0>)\n",
      "502 tensor(82185.1641, grad_fn=<MseLossBackward0>)\n",
      "503 tensor(82171.1719, grad_fn=<MseLossBackward0>)\n",
      "504 tensor(82157.1797, grad_fn=<MseLossBackward0>)\n",
      "505 tensor(82143.1641, grad_fn=<MseLossBackward0>)\n",
      "506 tensor(82129.1484, grad_fn=<MseLossBackward0>)\n",
      "507 tensor(82115.1016, grad_fn=<MseLossBackward0>)\n",
      "508 tensor(82101.0469, grad_fn=<MseLossBackward0>)\n",
      "509 tensor(82086.9766, grad_fn=<MseLossBackward0>)\n",
      "510 tensor(82072.8906, grad_fn=<MseLossBackward0>)\n",
      "511 tensor(82058.7969, grad_fn=<MseLossBackward0>)\n",
      "512 tensor(82044.6797, grad_fn=<MseLossBackward0>)\n",
      "513 tensor(82030.5547, grad_fn=<MseLossBackward0>)\n",
      "514 tensor(82016.4141, grad_fn=<MseLossBackward0>)\n",
      "515 tensor(82002.2500, grad_fn=<MseLossBackward0>)\n",
      "516 tensor(81988.0781, grad_fn=<MseLossBackward0>)\n",
      "517 tensor(81973.8984, grad_fn=<MseLossBackward0>)\n",
      "518 tensor(81959.6953, grad_fn=<MseLossBackward0>)\n",
      "519 tensor(81945.4844, grad_fn=<MseLossBackward0>)\n",
      "520 tensor(81931.2578, grad_fn=<MseLossBackward0>)\n",
      "521 tensor(81917.0156, grad_fn=<MseLossBackward0>)\n",
      "522 tensor(81902.7578, grad_fn=<MseLossBackward0>)\n",
      "523 tensor(81888.4922, grad_fn=<MseLossBackward0>)\n",
      "524 tensor(81874.2031, grad_fn=<MseLossBackward0>)\n",
      "525 tensor(81859.8828, grad_fn=<MseLossBackward0>)\n",
      "526 tensor(81845.5859, grad_fn=<MseLossBackward0>)\n",
      "527 tensor(81831.2500, grad_fn=<MseLossBackward0>)\n",
      "528 tensor(81816.9062, grad_fn=<MseLossBackward0>)\n",
      "529 tensor(81802.5547, grad_fn=<MseLossBackward0>)\n",
      "530 tensor(81788.1797, grad_fn=<MseLossBackward0>)\n",
      "531 tensor(81773.7969, grad_fn=<MseLossBackward0>)\n",
      "532 tensor(81759.3984, grad_fn=<MseLossBackward0>)\n",
      "533 tensor(81744.9922, grad_fn=<MseLossBackward0>)\n",
      "534 tensor(81730.5547, grad_fn=<MseLossBackward0>)\n",
      "535 tensor(81716.1094, grad_fn=<MseLossBackward0>)\n",
      "536 tensor(81701.6562, grad_fn=<MseLossBackward0>)\n",
      "537 tensor(81687.1797, grad_fn=<MseLossBackward0>)\n",
      "538 tensor(81672.7031, grad_fn=<MseLossBackward0>)\n",
      "539 tensor(81658.2031, grad_fn=<MseLossBackward0>)\n",
      "540 tensor(81643.6875, grad_fn=<MseLossBackward0>)\n",
      "541 tensor(81629.1641, grad_fn=<MseLossBackward0>)\n",
      "542 tensor(81614.6172, grad_fn=<MseLossBackward0>)\n",
      "543 tensor(81600.0625, grad_fn=<MseLossBackward0>)\n",
      "544 tensor(81585.5078, grad_fn=<MseLossBackward0>)\n",
      "545 tensor(81570.9141, grad_fn=<MseLossBackward0>)\n",
      "546 tensor(81556.3203, grad_fn=<MseLossBackward0>)\n",
      "547 tensor(81541.7109, grad_fn=<MseLossBackward0>)\n",
      "548 tensor(81527.0859, grad_fn=<MseLossBackward0>)\n",
      "549 tensor(81512.4375, grad_fn=<MseLossBackward0>)\n",
      "550 tensor(81497.7812, grad_fn=<MseLossBackward0>)\n",
      "551 tensor(81483.1328, grad_fn=<MseLossBackward0>)\n",
      "552 tensor(81468.4453, grad_fn=<MseLossBackward0>)\n",
      "553 tensor(81453.7500, grad_fn=<MseLossBackward0>)\n",
      "554 tensor(81439.0391, grad_fn=<MseLossBackward0>)\n",
      "555 tensor(81424.3203, grad_fn=<MseLossBackward0>)\n",
      "556 tensor(81409.5859, grad_fn=<MseLossBackward0>)\n",
      "557 tensor(81394.8359, grad_fn=<MseLossBackward0>)\n",
      "558 tensor(81380.0703, grad_fn=<MseLossBackward0>)\n",
      "559 tensor(81365.2969, grad_fn=<MseLossBackward0>)\n",
      "560 tensor(81350.5078, grad_fn=<MseLossBackward0>)\n",
      "561 tensor(81335.7031, grad_fn=<MseLossBackward0>)\n",
      "562 tensor(81320.8750, grad_fn=<MseLossBackward0>)\n",
      "563 tensor(81306.0469, grad_fn=<MseLossBackward0>)\n",
      "564 tensor(81291.2031, grad_fn=<MseLossBackward0>)\n",
      "565 tensor(81276.3359, grad_fn=<MseLossBackward0>)\n",
      "566 tensor(81261.4688, grad_fn=<MseLossBackward0>)\n",
      "567 tensor(81246.5781, grad_fn=<MseLossBackward0>)\n",
      "568 tensor(81231.6797, grad_fn=<MseLossBackward0>)\n",
      "569 tensor(81216.7656, grad_fn=<MseLossBackward0>)\n",
      "570 tensor(81201.8359, grad_fn=<MseLossBackward0>)\n",
      "571 tensor(81186.8984, grad_fn=<MseLossBackward0>)\n",
      "572 tensor(81171.9375, grad_fn=<MseLossBackward0>)\n",
      "573 tensor(81156.9609, grad_fn=<MseLossBackward0>)\n",
      "574 tensor(81141.9922, grad_fn=<MseLossBackward0>)\n",
      "575 tensor(81126.9922, grad_fn=<MseLossBackward0>)\n",
      "576 tensor(81111.9844, grad_fn=<MseLossBackward0>)\n",
      "577 tensor(81096.9531, grad_fn=<MseLossBackward0>)\n",
      "578 tensor(81081.9297, grad_fn=<MseLossBackward0>)\n",
      "579 tensor(81066.8750, grad_fn=<MseLossBackward0>)\n",
      "580 tensor(81051.8125, grad_fn=<MseLossBackward0>)\n",
      "581 tensor(81036.7344, grad_fn=<MseLossBackward0>)\n",
      "582 tensor(81021.6328, grad_fn=<MseLossBackward0>)\n",
      "583 tensor(81006.5312, grad_fn=<MseLossBackward0>)\n",
      "584 tensor(80991.4141, grad_fn=<MseLossBackward0>)\n",
      "585 tensor(80976.2812, grad_fn=<MseLossBackward0>)\n",
      "586 tensor(80961.1406, grad_fn=<MseLossBackward0>)\n",
      "587 tensor(80945.9844, grad_fn=<MseLossBackward0>)\n",
      "588 tensor(80930.8047, grad_fn=<MseLossBackward0>)\n",
      "589 tensor(80915.6328, grad_fn=<MseLossBackward0>)\n",
      "590 tensor(80900.4375, grad_fn=<MseLossBackward0>)\n",
      "591 tensor(80885.2188, grad_fn=<MseLossBackward0>)\n",
      "592 tensor(80869.9922, grad_fn=<MseLossBackward0>)\n",
      "593 tensor(80854.7578, grad_fn=<MseLossBackward0>)\n",
      "594 tensor(80839.5078, grad_fn=<MseLossBackward0>)\n",
      "595 tensor(80824.2422, grad_fn=<MseLossBackward0>)\n",
      "596 tensor(80808.9688, grad_fn=<MseLossBackward0>)\n",
      "597 tensor(80793.6797, grad_fn=<MseLossBackward0>)\n",
      "598 tensor(80778.3594, grad_fn=<MseLossBackward0>)\n",
      "599 tensor(80763.0547, grad_fn=<MseLossBackward0>)\n",
      "600 tensor(80747.7266, grad_fn=<MseLossBackward0>)\n",
      "601 tensor(80732.3750, grad_fn=<MseLossBackward0>)\n",
      "602 tensor(80717.0234, grad_fn=<MseLossBackward0>)\n",
      "603 tensor(80701.6484, grad_fn=<MseLossBackward0>)\n",
      "604 tensor(80686.2656, grad_fn=<MseLossBackward0>)\n",
      "605 tensor(80670.8672, grad_fn=<MseLossBackward0>)\n",
      "606 tensor(80655.4609, grad_fn=<MseLossBackward0>)\n",
      "607 tensor(80640.0391, grad_fn=<MseLossBackward0>)\n",
      "608 tensor(80624.6016, grad_fn=<MseLossBackward0>)\n",
      "609 tensor(80609.1562, grad_fn=<MseLossBackward0>)\n",
      "610 tensor(80593.7031, grad_fn=<MseLossBackward0>)\n",
      "611 tensor(80578.2188, grad_fn=<MseLossBackward0>)\n",
      "612 tensor(80562.7266, grad_fn=<MseLossBackward0>)\n",
      "613 tensor(80547.2188, grad_fn=<MseLossBackward0>)\n",
      "614 tensor(80531.7109, grad_fn=<MseLossBackward0>)\n",
      "615 tensor(80516.1797, grad_fn=<MseLossBackward0>)\n",
      "616 tensor(80500.6406, grad_fn=<MseLossBackward0>)\n",
      "617 tensor(80485.0859, grad_fn=<MseLossBackward0>)\n",
      "618 tensor(80469.5234, grad_fn=<MseLossBackward0>)\n",
      "619 tensor(80453.9453, grad_fn=<MseLossBackward0>)\n",
      "620 tensor(80438.3516, grad_fn=<MseLossBackward0>)\n",
      "621 tensor(80422.7500, grad_fn=<MseLossBackward0>)\n",
      "622 tensor(80407.1328, grad_fn=<MseLossBackward0>)\n",
      "623 tensor(80391.5000, grad_fn=<MseLossBackward0>)\n",
      "624 tensor(80375.8594, grad_fn=<MseLossBackward0>)\n",
      "625 tensor(80360.1953, grad_fn=<MseLossBackward0>)\n",
      "626 tensor(80344.5312, grad_fn=<MseLossBackward0>)\n",
      "627 tensor(80328.8359, grad_fn=<MseLossBackward0>)\n",
      "628 tensor(80313.1484, grad_fn=<MseLossBackward0>)\n",
      "629 tensor(80297.4375, grad_fn=<MseLossBackward0>)\n",
      "630 tensor(80281.7188, grad_fn=<MseLossBackward0>)\n",
      "631 tensor(80265.9844, grad_fn=<MseLossBackward0>)\n",
      "632 tensor(80250.2422, grad_fn=<MseLossBackward0>)\n",
      "633 tensor(80234.4766, grad_fn=<MseLossBackward0>)\n",
      "634 tensor(80218.7031, grad_fn=<MseLossBackward0>)\n",
      "635 tensor(80202.9219, grad_fn=<MseLossBackward0>)\n",
      "636 tensor(80187.1172, grad_fn=<MseLossBackward0>)\n",
      "637 tensor(80171.3203, grad_fn=<MseLossBackward0>)\n",
      "638 tensor(80155.4922, grad_fn=<MseLossBackward0>)\n",
      "639 tensor(80139.6562, grad_fn=<MseLossBackward0>)\n",
      "640 tensor(80123.7969, grad_fn=<MseLossBackward0>)\n",
      "641 tensor(80107.9453, grad_fn=<MseLossBackward0>)\n",
      "642 tensor(80092.0625, grad_fn=<MseLossBackward0>)\n",
      "643 tensor(80076.1875, grad_fn=<MseLossBackward0>)\n",
      "644 tensor(80060.2812, grad_fn=<MseLossBackward0>)\n",
      "645 tensor(80044.3750, grad_fn=<MseLossBackward0>)\n",
      "646 tensor(80028.4453, grad_fn=<MseLossBackward0>)\n",
      "647 tensor(80012.5078, grad_fn=<MseLossBackward0>)\n",
      "648 tensor(79996.5625, grad_fn=<MseLossBackward0>)\n",
      "649 tensor(79980.6016, grad_fn=<MseLossBackward0>)\n",
      "650 tensor(79964.6250, grad_fn=<MseLossBackward0>)\n",
      "651 tensor(79948.6328, grad_fn=<MseLossBackward0>)\n",
      "652 tensor(79932.6328, grad_fn=<MseLossBackward0>)\n",
      "653 tensor(79916.6250, grad_fn=<MseLossBackward0>)\n",
      "654 tensor(79900.5938, grad_fn=<MseLossBackward0>)\n",
      "655 tensor(79884.5625, grad_fn=<MseLossBackward0>)\n",
      "656 tensor(79868.5078, grad_fn=<MseLossBackward0>)\n",
      "657 tensor(79852.4531, grad_fn=<MseLossBackward0>)\n",
      "658 tensor(79836.3750, grad_fn=<MseLossBackward0>)\n",
      "659 tensor(79820.2812, grad_fn=<MseLossBackward0>)\n",
      "660 tensor(79804.1797, grad_fn=<MseLossBackward0>)\n",
      "661 tensor(79788.0781, grad_fn=<MseLossBackward0>)\n",
      "662 tensor(79771.9453, grad_fn=<MseLossBackward0>)\n",
      "663 tensor(79755.8125, grad_fn=<MseLossBackward0>)\n",
      "664 tensor(79739.6562, grad_fn=<MseLossBackward0>)\n",
      "665 tensor(79723.5000, grad_fn=<MseLossBackward0>)\n",
      "666 tensor(79707.3203, grad_fn=<MseLossBackward0>)\n",
      "667 tensor(79691.1328, grad_fn=<MseLossBackward0>)\n",
      "668 tensor(79674.9375, grad_fn=<MseLossBackward0>)\n",
      "669 tensor(79658.7266, grad_fn=<MseLossBackward0>)\n",
      "670 tensor(79642.5078, grad_fn=<MseLossBackward0>)\n",
      "671 tensor(79626.2656, grad_fn=<MseLossBackward0>)\n",
      "672 tensor(79610.0156, grad_fn=<MseLossBackward0>)\n",
      "673 tensor(79593.7578, grad_fn=<MseLossBackward0>)\n",
      "674 tensor(79577.4844, grad_fn=<MseLossBackward0>)\n",
      "675 tensor(79561.1953, grad_fn=<MseLossBackward0>)\n",
      "676 tensor(79544.8984, grad_fn=<MseLossBackward0>)\n",
      "677 tensor(79528.5938, grad_fn=<MseLossBackward0>)\n",
      "678 tensor(79512.2734, grad_fn=<MseLossBackward0>)\n",
      "679 tensor(79495.9297, grad_fn=<MseLossBackward0>)\n",
      "680 tensor(79479.5938, grad_fn=<MseLossBackward0>)\n",
      "681 tensor(79463.2344, grad_fn=<MseLossBackward0>)\n",
      "682 tensor(79446.8750, grad_fn=<MseLossBackward0>)\n",
      "683 tensor(79430.4766, grad_fn=<MseLossBackward0>)\n",
      "684 tensor(79414.0859, grad_fn=<MseLossBackward0>)\n",
      "685 tensor(79397.6875, grad_fn=<MseLossBackward0>)\n",
      "686 tensor(79381.2578, grad_fn=<MseLossBackward0>)\n",
      "687 tensor(79364.8281, grad_fn=<MseLossBackward0>)\n",
      "688 tensor(79348.3828, grad_fn=<MseLossBackward0>)\n",
      "689 tensor(79331.9375, grad_fn=<MseLossBackward0>)\n",
      "690 tensor(79315.4609, grad_fn=<MseLossBackward0>)\n",
      "691 tensor(79298.9844, grad_fn=<MseLossBackward0>)\n",
      "692 tensor(79282.5000, grad_fn=<MseLossBackward0>)\n",
      "693 tensor(79265.9922, grad_fn=<MseLossBackward0>)\n",
      "694 tensor(79249.4766, grad_fn=<MseLossBackward0>)\n",
      "695 tensor(79232.9453, grad_fn=<MseLossBackward0>)\n",
      "696 tensor(79216.4062, grad_fn=<MseLossBackward0>)\n",
      "697 tensor(79199.8594, grad_fn=<MseLossBackward0>)\n",
      "698 tensor(79183.2969, grad_fn=<MseLossBackward0>)\n",
      "699 tensor(79166.7188, grad_fn=<MseLossBackward0>)\n",
      "700 tensor(79150.1328, grad_fn=<MseLossBackward0>)\n",
      "701 tensor(79133.5312, grad_fn=<MseLossBackward0>)\n",
      "702 tensor(79116.9297, grad_fn=<MseLossBackward0>)\n",
      "703 tensor(79100.2969, grad_fn=<MseLossBackward0>)\n",
      "704 tensor(79083.6641, grad_fn=<MseLossBackward0>)\n",
      "705 tensor(79067.0156, grad_fn=<MseLossBackward0>)\n",
      "706 tensor(79050.3594, grad_fn=<MseLossBackward0>)\n",
      "707 tensor(79033.6875, grad_fn=<MseLossBackward0>)\n",
      "708 tensor(79017., grad_fn=<MseLossBackward0>)\n",
      "709 tensor(79000.2969, grad_fn=<MseLossBackward0>)\n",
      "710 tensor(78983.6016, grad_fn=<MseLossBackward0>)\n",
      "711 tensor(78966.8828, grad_fn=<MseLossBackward0>)\n",
      "712 tensor(78950.1484, grad_fn=<MseLossBackward0>)\n",
      "713 tensor(78933.4062, grad_fn=<MseLossBackward0>)\n",
      "714 tensor(78916.6562, grad_fn=<MseLossBackward0>)\n",
      "715 tensor(78899.8984, grad_fn=<MseLossBackward0>)\n",
      "716 tensor(78883.1172, grad_fn=<MseLossBackward0>)\n",
      "717 tensor(78866.3281, grad_fn=<MseLossBackward0>)\n",
      "718 tensor(78849.5391, grad_fn=<MseLossBackward0>)\n",
      "719 tensor(78832.7188, grad_fn=<MseLossBackward0>)\n",
      "720 tensor(78815.8906, grad_fn=<MseLossBackward0>)\n",
      "721 tensor(78799.0547, grad_fn=<MseLossBackward0>)\n",
      "722 tensor(78782.2109, grad_fn=<MseLossBackward0>)\n",
      "723 tensor(78765.3516, grad_fn=<MseLossBackward0>)\n",
      "724 tensor(78748.4844, grad_fn=<MseLossBackward0>)\n",
      "725 tensor(78731.6016, grad_fn=<MseLossBackward0>)\n",
      "726 tensor(78714.7031, grad_fn=<MseLossBackward0>)\n",
      "727 tensor(78697.8047, grad_fn=<MseLossBackward0>)\n",
      "728 tensor(78680.8828, grad_fn=<MseLossBackward0>)\n",
      "729 tensor(78663.9609, grad_fn=<MseLossBackward0>)\n",
      "730 tensor(78647.0156, grad_fn=<MseLossBackward0>)\n",
      "731 tensor(78630.0703, grad_fn=<MseLossBackward0>)\n",
      "732 tensor(78613.1016, grad_fn=<MseLossBackward0>)\n",
      "733 tensor(78596.1250, grad_fn=<MseLossBackward0>)\n",
      "734 tensor(78579.1484, grad_fn=<MseLossBackward0>)\n",
      "735 tensor(78562.1406, grad_fn=<MseLossBackward0>)\n",
      "736 tensor(78545.1406, grad_fn=<MseLossBackward0>)\n",
      "737 tensor(78528.1172, grad_fn=<MseLossBackward0>)\n",
      "738 tensor(78511.0938, grad_fn=<MseLossBackward0>)\n",
      "739 tensor(78494.0469, grad_fn=<MseLossBackward0>)\n",
      "740 tensor(78476.9844, grad_fn=<MseLossBackward0>)\n",
      "741 tensor(78459.9141, grad_fn=<MseLossBackward0>)\n",
      "742 tensor(78442.8438, grad_fn=<MseLossBackward0>)\n",
      "743 tensor(78425.7500, grad_fn=<MseLossBackward0>)\n",
      "744 tensor(78408.6484, grad_fn=<MseLossBackward0>)\n",
      "745 tensor(78391.5312, grad_fn=<MseLossBackward0>)\n",
      "746 tensor(78374.4062, grad_fn=<MseLossBackward0>)\n",
      "747 tensor(78357.2812, grad_fn=<MseLossBackward0>)\n",
      "748 tensor(78340.1250, grad_fn=<MseLossBackward0>)\n",
      "749 tensor(78322.9688, grad_fn=<MseLossBackward0>)\n",
      "750 tensor(78305.8047, grad_fn=<MseLossBackward0>)\n",
      "751 tensor(78288.6250, grad_fn=<MseLossBackward0>)\n",
      "752 tensor(78271.4375, grad_fn=<MseLossBackward0>)\n",
      "753 tensor(78254.2266, grad_fn=<MseLossBackward0>)\n",
      "754 tensor(78237.0156, grad_fn=<MseLossBackward0>)\n",
      "755 tensor(78219.7812, grad_fn=<MseLossBackward0>)\n",
      "756 tensor(78202.5469, grad_fn=<MseLossBackward0>)\n",
      "757 tensor(78185.2969, grad_fn=<MseLossBackward0>)\n",
      "758 tensor(78168.0469, grad_fn=<MseLossBackward0>)\n",
      "759 tensor(78150.7734, grad_fn=<MseLossBackward0>)\n",
      "760 tensor(78133.4922, grad_fn=<MseLossBackward0>)\n",
      "761 tensor(78116.1953, grad_fn=<MseLossBackward0>)\n",
      "762 tensor(78098.8906, grad_fn=<MseLossBackward0>)\n",
      "763 tensor(78081.5781, grad_fn=<MseLossBackward0>)\n",
      "764 tensor(78064.2500, grad_fn=<MseLossBackward0>)\n",
      "765 tensor(78046.9141, grad_fn=<MseLossBackward0>)\n",
      "766 tensor(78029.5625, grad_fn=<MseLossBackward0>)\n",
      "767 tensor(78012.2109, grad_fn=<MseLossBackward0>)\n",
      "768 tensor(77994.8281, grad_fn=<MseLossBackward0>)\n",
      "769 tensor(77977.4531, grad_fn=<MseLossBackward0>)\n",
      "770 tensor(77960.0547, grad_fn=<MseLossBackward0>)\n",
      "771 tensor(77942.6562, grad_fn=<MseLossBackward0>)\n",
      "772 tensor(77925.2344, grad_fn=<MseLossBackward0>)\n",
      "773 tensor(77907.8047, grad_fn=<MseLossBackward0>)\n",
      "774 tensor(77890.3750, grad_fn=<MseLossBackward0>)\n",
      "775 tensor(77872.9219, grad_fn=<MseLossBackward0>)\n",
      "776 tensor(77855.4609, grad_fn=<MseLossBackward0>)\n",
      "777 tensor(77837.9922, grad_fn=<MseLossBackward0>)\n",
      "778 tensor(77820.5078, grad_fn=<MseLossBackward0>)\n",
      "779 tensor(77803.0156, grad_fn=<MseLossBackward0>)\n",
      "780 tensor(77785.5234, grad_fn=<MseLossBackward0>)\n",
      "781 tensor(77768., grad_fn=<MseLossBackward0>)\n",
      "782 tensor(77750.4688, grad_fn=<MseLossBackward0>)\n",
      "783 tensor(77732.9375, grad_fn=<MseLossBackward0>)\n",
      "784 tensor(77715.3984, grad_fn=<MseLossBackward0>)\n",
      "785 tensor(77697.8281, grad_fn=<MseLossBackward0>)\n",
      "786 tensor(77680.2500, grad_fn=<MseLossBackward0>)\n",
      "787 tensor(77662.6875, grad_fn=<MseLossBackward0>)\n",
      "788 tensor(77645.0859, grad_fn=<MseLossBackward0>)\n",
      "789 tensor(77627.4844, grad_fn=<MseLossBackward0>)\n",
      "790 tensor(77609.8750, grad_fn=<MseLossBackward0>)\n",
      "791 tensor(77592.2500, grad_fn=<MseLossBackward0>)\n",
      "792 tensor(77574.6172, grad_fn=<MseLossBackward0>)\n",
      "793 tensor(77556.9688, grad_fn=<MseLossBackward0>)\n",
      "794 tensor(77539.3203, grad_fn=<MseLossBackward0>)\n",
      "795 tensor(77521.6484, grad_fn=<MseLossBackward0>)\n",
      "796 tensor(77503.9688, grad_fn=<MseLossBackward0>)\n",
      "797 tensor(77486.2812, grad_fn=<MseLossBackward0>)\n",
      "798 tensor(77468.5859, grad_fn=<MseLossBackward0>)\n",
      "799 tensor(77450.8672, grad_fn=<MseLossBackward0>)\n",
      "800 tensor(77433.1406, grad_fn=<MseLossBackward0>)\n",
      "801 tensor(77415.4141, grad_fn=<MseLossBackward0>)\n",
      "802 tensor(77397.6641, grad_fn=<MseLossBackward0>)\n",
      "803 tensor(77379.9141, grad_fn=<MseLossBackward0>)\n",
      "804 tensor(77362.1484, grad_fn=<MseLossBackward0>)\n",
      "805 tensor(77344.3750, grad_fn=<MseLossBackward0>)\n",
      "806 tensor(77326.5859, grad_fn=<MseLossBackward0>)\n",
      "807 tensor(77308.7969, grad_fn=<MseLossBackward0>)\n",
      "808 tensor(77290.9922, grad_fn=<MseLossBackward0>)\n",
      "809 tensor(77273.1641, grad_fn=<MseLossBackward0>)\n",
      "810 tensor(77255.3359, grad_fn=<MseLossBackward0>)\n",
      "811 tensor(77237.5000, grad_fn=<MseLossBackward0>)\n",
      "812 tensor(77219.6562, grad_fn=<MseLossBackward0>)\n",
      "813 tensor(77201.7891, grad_fn=<MseLossBackward0>)\n",
      "814 tensor(77183.9219, grad_fn=<MseLossBackward0>)\n",
      "815 tensor(77166.0391, grad_fn=<MseLossBackward0>)\n",
      "816 tensor(77148.1484, grad_fn=<MseLossBackward0>)\n",
      "817 tensor(77130.2422, grad_fn=<MseLossBackward0>)\n",
      "818 tensor(77112.3281, grad_fn=<MseLossBackward0>)\n",
      "819 tensor(77094.4062, grad_fn=<MseLossBackward0>)\n",
      "820 tensor(77076.4688, grad_fn=<MseLossBackward0>)\n",
      "821 tensor(77058.5234, grad_fn=<MseLossBackward0>)\n",
      "822 tensor(77040.5625, grad_fn=<MseLossBackward0>)\n",
      "823 tensor(77022.6094, grad_fn=<MseLossBackward0>)\n",
      "824 tensor(77004.6250, grad_fn=<MseLossBackward0>)\n",
      "825 tensor(76986.6406, grad_fn=<MseLossBackward0>)\n",
      "826 tensor(76968.6484, grad_fn=<MseLossBackward0>)\n",
      "827 tensor(76950.6406, grad_fn=<MseLossBackward0>)\n",
      "828 tensor(76932.6172, grad_fn=<MseLossBackward0>)\n",
      "829 tensor(76914.5938, grad_fn=<MseLossBackward0>)\n",
      "830 tensor(76896.5469, grad_fn=<MseLossBackward0>)\n",
      "831 tensor(76878.5000, grad_fn=<MseLossBackward0>)\n",
      "832 tensor(76860.4375, grad_fn=<MseLossBackward0>)\n",
      "833 tensor(76842.3750, grad_fn=<MseLossBackward0>)\n",
      "834 tensor(76824.2891, grad_fn=<MseLossBackward0>)\n",
      "835 tensor(76806.1953, grad_fn=<MseLossBackward0>)\n",
      "836 tensor(76788.0938, grad_fn=<MseLossBackward0>)\n",
      "837 tensor(76769.9844, grad_fn=<MseLossBackward0>)\n",
      "838 tensor(76751.8594, grad_fn=<MseLossBackward0>)\n",
      "839 tensor(76733.7344, grad_fn=<MseLossBackward0>)\n",
      "840 tensor(76715.5938, grad_fn=<MseLossBackward0>)\n",
      "841 tensor(76697.4297, grad_fn=<MseLossBackward0>)\n",
      "842 tensor(76679.2734, grad_fn=<MseLossBackward0>)\n",
      "843 tensor(76661.0938, grad_fn=<MseLossBackward0>)\n",
      "844 tensor(76642.9141, grad_fn=<MseLossBackward0>)\n",
      "845 tensor(76624.7188, grad_fn=<MseLossBackward0>)\n",
      "846 tensor(76606.5156, grad_fn=<MseLossBackward0>)\n",
      "847 tensor(76588.2969, grad_fn=<MseLossBackward0>)\n",
      "848 tensor(76570.0703, grad_fn=<MseLossBackward0>)\n",
      "849 tensor(76551.8438, grad_fn=<MseLossBackward0>)\n",
      "850 tensor(76533.6016, grad_fn=<MseLossBackward0>)\n",
      "851 tensor(76515.3438, grad_fn=<MseLossBackward0>)\n",
      "852 tensor(76497.0781, grad_fn=<MseLossBackward0>)\n",
      "853 tensor(76478.8125, grad_fn=<MseLossBackward0>)\n",
      "854 tensor(76460.5156, grad_fn=<MseLossBackward0>)\n",
      "855 tensor(76442.2266, grad_fn=<MseLossBackward0>)\n",
      "856 tensor(76423.9062, grad_fn=<MseLossBackward0>)\n",
      "857 tensor(76405.6016, grad_fn=<MseLossBackward0>)\n",
      "858 tensor(76387.2656, grad_fn=<MseLossBackward0>)\n",
      "859 tensor(76368.9297, grad_fn=<MseLossBackward0>)\n",
      "860 tensor(76350.5938, grad_fn=<MseLossBackward0>)\n",
      "861 tensor(76332.2344, grad_fn=<MseLossBackward0>)\n",
      "862 tensor(76313.8672, grad_fn=<MseLossBackward0>)\n",
      "863 tensor(76295.4922, grad_fn=<MseLossBackward0>)\n",
      "864 tensor(76277.1016, grad_fn=<MseLossBackward0>)\n",
      "865 tensor(76258.6953, grad_fn=<MseLossBackward0>)\n",
      "866 tensor(76240.3047, grad_fn=<MseLossBackward0>)\n",
      "867 tensor(76221.8828, grad_fn=<MseLossBackward0>)\n",
      "868 tensor(76203.4531, grad_fn=<MseLossBackward0>)\n",
      "869 tensor(76185.0156, grad_fn=<MseLossBackward0>)\n",
      "870 tensor(76166.5781, grad_fn=<MseLossBackward0>)\n",
      "871 tensor(76148.1172, grad_fn=<MseLossBackward0>)\n",
      "872 tensor(76129.6484, grad_fn=<MseLossBackward0>)\n",
      "873 tensor(76111.1719, grad_fn=<MseLossBackward0>)\n",
      "874 tensor(76092.6797, grad_fn=<MseLossBackward0>)\n",
      "875 tensor(76074.1875, grad_fn=<MseLossBackward0>)\n",
      "876 tensor(76055.6797, grad_fn=<MseLossBackward0>)\n",
      "877 tensor(76037.1719, grad_fn=<MseLossBackward0>)\n",
      "878 tensor(76018.6406, grad_fn=<MseLossBackward0>)\n",
      "879 tensor(76000.1016, grad_fn=<MseLossBackward0>)\n",
      "880 tensor(75981.5625, grad_fn=<MseLossBackward0>)\n",
      "881 tensor(75963., grad_fn=<MseLossBackward0>)\n",
      "882 tensor(75944.4375, grad_fn=<MseLossBackward0>)\n",
      "883 tensor(75925.8594, grad_fn=<MseLossBackward0>)\n",
      "884 tensor(75907.2734, grad_fn=<MseLossBackward0>)\n",
      "885 tensor(75888.6797, grad_fn=<MseLossBackward0>)\n",
      "886 tensor(75870.0703, grad_fn=<MseLossBackward0>)\n",
      "887 tensor(75851.4688, grad_fn=<MseLossBackward0>)\n",
      "888 tensor(75832.8359, grad_fn=<MseLossBackward0>)\n",
      "889 tensor(75814.2031, grad_fn=<MseLossBackward0>)\n",
      "890 tensor(75795.5547, grad_fn=<MseLossBackward0>)\n",
      "891 tensor(75776.9062, grad_fn=<MseLossBackward0>)\n",
      "892 tensor(75758.2344, grad_fn=<MseLossBackward0>)\n",
      "893 tensor(75739.5625, grad_fn=<MseLossBackward0>)\n",
      "894 tensor(75720.8828, grad_fn=<MseLossBackward0>)\n",
      "895 tensor(75702.2031, grad_fn=<MseLossBackward0>)\n",
      "896 tensor(75683.5000, grad_fn=<MseLossBackward0>)\n",
      "897 tensor(75664.7812, grad_fn=<MseLossBackward0>)\n",
      "898 tensor(75646.0625, grad_fn=<MseLossBackward0>)\n",
      "899 tensor(75627.3281, grad_fn=<MseLossBackward0>)\n",
      "900 tensor(75608.5859, grad_fn=<MseLossBackward0>)\n",
      "901 tensor(75589.8359, grad_fn=<MseLossBackward0>)\n",
      "902 tensor(75571.0781, grad_fn=<MseLossBackward0>)\n",
      "903 tensor(75552.3125, grad_fn=<MseLossBackward0>)\n",
      "904 tensor(75533.5312, grad_fn=<MseLossBackward0>)\n",
      "905 tensor(75514.7422, grad_fn=<MseLossBackward0>)\n",
      "906 tensor(75495.9375, grad_fn=<MseLossBackward0>)\n",
      "907 tensor(75477.1406, grad_fn=<MseLossBackward0>)\n",
      "908 tensor(75458.3125, grad_fn=<MseLossBackward0>)\n",
      "909 tensor(75439.4844, grad_fn=<MseLossBackward0>)\n",
      "910 tensor(75420.6484, grad_fn=<MseLossBackward0>)\n",
      "911 tensor(75401.8125, grad_fn=<MseLossBackward0>)\n",
      "912 tensor(75382.9453, grad_fn=<MseLossBackward0>)\n",
      "913 tensor(75364.0859, grad_fn=<MseLossBackward0>)\n",
      "914 tensor(75345.2109, grad_fn=<MseLossBackward0>)\n",
      "915 tensor(75326.3281, grad_fn=<MseLossBackward0>)\n",
      "916 tensor(75307.4375, grad_fn=<MseLossBackward0>)\n",
      "917 tensor(75288.5312, grad_fn=<MseLossBackward0>)\n",
      "918 tensor(75269.6172, grad_fn=<MseLossBackward0>)\n",
      "919 tensor(75250.7031, grad_fn=<MseLossBackward0>)\n",
      "920 tensor(75231.7734, grad_fn=<MseLossBackward0>)\n",
      "921 tensor(75212.8281, grad_fn=<MseLossBackward0>)\n",
      "922 tensor(75193.8750, grad_fn=<MseLossBackward0>)\n",
      "923 tensor(75174.9141, grad_fn=<MseLossBackward0>)\n",
      "924 tensor(75155.9453, grad_fn=<MseLossBackward0>)\n",
      "925 tensor(75136.9766, grad_fn=<MseLossBackward0>)\n",
      "926 tensor(75117.9766, grad_fn=<MseLossBackward0>)\n",
      "927 tensor(75098.9844, grad_fn=<MseLossBackward0>)\n",
      "928 tensor(75079.9766, grad_fn=<MseLossBackward0>)\n",
      "929 tensor(75060.9609, grad_fn=<MseLossBackward0>)\n",
      "930 tensor(75041.9453, grad_fn=<MseLossBackward0>)\n",
      "931 tensor(75022.9062, grad_fn=<MseLossBackward0>)\n",
      "932 tensor(75003.8594, grad_fn=<MseLossBackward0>)\n",
      "933 tensor(74984.8047, grad_fn=<MseLossBackward0>)\n",
      "934 tensor(74965.7422, grad_fn=<MseLossBackward0>)\n",
      "935 tensor(74946.6797, grad_fn=<MseLossBackward0>)\n",
      "936 tensor(74927.5938, grad_fn=<MseLossBackward0>)\n",
      "937 tensor(74908.5078, grad_fn=<MseLossBackward0>)\n",
      "938 tensor(74889.4062, grad_fn=<MseLossBackward0>)\n",
      "939 tensor(74870.3047, grad_fn=<MseLossBackward0>)\n",
      "940 tensor(74851.1797, grad_fn=<MseLossBackward0>)\n",
      "941 tensor(74832.0547, grad_fn=<MseLossBackward0>)\n",
      "942 tensor(74812.9219, grad_fn=<MseLossBackward0>)\n",
      "943 tensor(74793.7734, grad_fn=<MseLossBackward0>)\n",
      "944 tensor(74774.6172, grad_fn=<MseLossBackward0>)\n",
      "945 tensor(74755.4609, grad_fn=<MseLossBackward0>)\n",
      "946 tensor(74736.2812, grad_fn=<MseLossBackward0>)\n",
      "947 tensor(74717.1016, grad_fn=<MseLossBackward0>)\n",
      "948 tensor(74697.9219, grad_fn=<MseLossBackward0>)\n",
      "949 tensor(74678.7109, grad_fn=<MseLossBackward0>)\n",
      "950 tensor(74659.5000, grad_fn=<MseLossBackward0>)\n",
      "951 tensor(74640.2891, grad_fn=<MseLossBackward0>)\n",
      "952 tensor(74621.0625, grad_fn=<MseLossBackward0>)\n",
      "953 tensor(74601.8281, grad_fn=<MseLossBackward0>)\n",
      "954 tensor(74582.5859, grad_fn=<MseLossBackward0>)\n",
      "955 tensor(74563.3281, grad_fn=<MseLossBackward0>)\n",
      "956 tensor(74544.0625, grad_fn=<MseLossBackward0>)\n",
      "957 tensor(74524.7969, grad_fn=<MseLossBackward0>)\n",
      "958 tensor(74505.5156, grad_fn=<MseLossBackward0>)\n",
      "959 tensor(74486.2188, grad_fn=<MseLossBackward0>)\n",
      "960 tensor(74466.9219, grad_fn=<MseLossBackward0>)\n",
      "961 tensor(74447.6094, grad_fn=<MseLossBackward0>)\n",
      "962 tensor(74428.3047, grad_fn=<MseLossBackward0>)\n",
      "963 tensor(74408.9766, grad_fn=<MseLossBackward0>)\n",
      "964 tensor(74389.6328, grad_fn=<MseLossBackward0>)\n",
      "965 tensor(74370.2969, grad_fn=<MseLossBackward0>)\n",
      "966 tensor(74350.9375, grad_fn=<MseLossBackward0>)\n",
      "967 tensor(74331.5781, grad_fn=<MseLossBackward0>)\n",
      "968 tensor(74312.2031, grad_fn=<MseLossBackward0>)\n",
      "969 tensor(74292.8281, grad_fn=<MseLossBackward0>)\n",
      "970 tensor(74273.4375, grad_fn=<MseLossBackward0>)\n",
      "971 tensor(74254.0547, grad_fn=<MseLossBackward0>)\n",
      "972 tensor(74234.6328, grad_fn=<MseLossBackward0>)\n",
      "973 tensor(74215.2188, grad_fn=<MseLossBackward0>)\n",
      "974 tensor(74195.8047, grad_fn=<MseLossBackward0>)\n",
      "975 tensor(74176.3672, grad_fn=<MseLossBackward0>)\n",
      "976 tensor(74156.9375, grad_fn=<MseLossBackward0>)\n",
      "977 tensor(74137.4766, grad_fn=<MseLossBackward0>)\n",
      "978 tensor(74118.0234, grad_fn=<MseLossBackward0>)\n",
      "979 tensor(74098.5625, grad_fn=<MseLossBackward0>)\n",
      "980 tensor(74079.0781, grad_fn=<MseLossBackward0>)\n",
      "981 tensor(74059.5938, grad_fn=<MseLossBackward0>)\n",
      "982 tensor(74040.0938, grad_fn=<MseLossBackward0>)\n",
      "983 tensor(74020.5938, grad_fn=<MseLossBackward0>)\n",
      "984 tensor(74001.0859, grad_fn=<MseLossBackward0>)\n",
      "985 tensor(73981.5625, grad_fn=<MseLossBackward0>)\n",
      "986 tensor(73962.0469, grad_fn=<MseLossBackward0>)\n",
      "987 tensor(73942.5000, grad_fn=<MseLossBackward0>)\n",
      "988 tensor(73922.9609, grad_fn=<MseLossBackward0>)\n",
      "989 tensor(73903.4062, grad_fn=<MseLossBackward0>)\n",
      "990 tensor(73883.8359, grad_fn=<MseLossBackward0>)\n",
      "991 tensor(73864.2578, grad_fn=<MseLossBackward0>)\n",
      "992 tensor(73844.6797, grad_fn=<MseLossBackward0>)\n",
      "993 tensor(73825.0938, grad_fn=<MseLossBackward0>)\n",
      "994 tensor(73805.5000, grad_fn=<MseLossBackward0>)\n",
      "995 tensor(73785.8906, grad_fn=<MseLossBackward0>)\n",
      "996 tensor(73766.2812, grad_fn=<MseLossBackward0>)\n",
      "997 tensor(73746.6562, grad_fn=<MseLossBackward0>)\n",
      "998 tensor(73727.0234, grad_fn=<MseLossBackward0>)\n",
      "999 tensor(73707.3828, grad_fn=<MseLossBackward0>)\n",
      "1000 tensor(73687.7266, grad_fn=<MseLossBackward0>)\n",
      "1001 tensor(73668.0703, grad_fn=<MseLossBackward0>)\n",
      "1002 tensor(73648.4062, grad_fn=<MseLossBackward0>)\n",
      "1003 tensor(73628.7344, grad_fn=<MseLossBackward0>)\n",
      "1004 tensor(73609.0469, grad_fn=<MseLossBackward0>)\n",
      "1005 tensor(73589.3516, grad_fn=<MseLossBackward0>)\n",
      "1006 tensor(73569.6641, grad_fn=<MseLossBackward0>)\n",
      "1007 tensor(73549.9531, grad_fn=<MseLossBackward0>)\n",
      "1008 tensor(73530.2422, grad_fn=<MseLossBackward0>)\n",
      "1009 tensor(73510.5078, grad_fn=<MseLossBackward0>)\n",
      "1010 tensor(73490.7734, grad_fn=<MseLossBackward0>)\n",
      "1011 tensor(73471.0391, grad_fn=<MseLossBackward0>)\n",
      "1012 tensor(73451.2891, grad_fn=<MseLossBackward0>)\n",
      "1013 tensor(73431.5312, grad_fn=<MseLossBackward0>)\n",
      "1014 tensor(73411.7578, grad_fn=<MseLossBackward0>)\n",
      "1015 tensor(73391.9766, grad_fn=<MseLossBackward0>)\n",
      "1016 tensor(73372.1875, grad_fn=<MseLossBackward0>)\n",
      "1017 tensor(73352.3984, grad_fn=<MseLossBackward0>)\n",
      "1018 tensor(73332.6016, grad_fn=<MseLossBackward0>)\n",
      "1019 tensor(73312.7891, grad_fn=<MseLossBackward0>)\n",
      "1020 tensor(73292.9766, grad_fn=<MseLossBackward0>)\n",
      "1021 tensor(73273.1484, grad_fn=<MseLossBackward0>)\n",
      "1022 tensor(73253.3125, grad_fn=<MseLossBackward0>)\n",
      "1023 tensor(73233.4766, grad_fn=<MseLossBackward0>)\n",
      "1024 tensor(73213.6172, grad_fn=<MseLossBackward0>)\n",
      "1025 tensor(73193.7656, grad_fn=<MseLossBackward0>)\n",
      "1026 tensor(73173.8906, grad_fn=<MseLossBackward0>)\n",
      "1027 tensor(73154.0234, grad_fn=<MseLossBackward0>)\n",
      "1028 tensor(73134.1406, grad_fn=<MseLossBackward0>)\n",
      "1029 tensor(73114.2422, grad_fn=<MseLossBackward0>)\n",
      "1030 tensor(73094.3359, grad_fn=<MseLossBackward0>)\n",
      "1031 tensor(73074.4375, grad_fn=<MseLossBackward0>)\n",
      "1032 tensor(73054.5156, grad_fn=<MseLossBackward0>)\n",
      "1033 tensor(73034.5938, grad_fn=<MseLossBackward0>)\n",
      "1034 tensor(73014.6641, grad_fn=<MseLossBackward0>)\n",
      "1035 tensor(72994.7266, grad_fn=<MseLossBackward0>)\n",
      "1036 tensor(72974.7656, grad_fn=<MseLossBackward0>)\n",
      "1037 tensor(72954.8203, grad_fn=<MseLossBackward0>)\n",
      "1038 tensor(72934.8516, grad_fn=<MseLossBackward0>)\n",
      "1039 tensor(72914.8750, grad_fn=<MseLossBackward0>)\n",
      "1040 tensor(72894.8984, grad_fn=<MseLossBackward0>)\n",
      "1041 tensor(72874.9062, grad_fn=<MseLossBackward0>)\n",
      "1042 tensor(72854.9062, grad_fn=<MseLossBackward0>)\n",
      "1043 tensor(72834.8984, grad_fn=<MseLossBackward0>)\n",
      "1044 tensor(72814.8906, grad_fn=<MseLossBackward0>)\n",
      "1045 tensor(72794.8594, grad_fn=<MseLossBackward0>)\n",
      "1046 tensor(72774.8359, grad_fn=<MseLossBackward0>)\n",
      "1047 tensor(72754.7969, grad_fn=<MseLossBackward0>)\n",
      "1048 tensor(72734.7500, grad_fn=<MseLossBackward0>)\n",
      "1049 tensor(72714.6875, grad_fn=<MseLossBackward0>)\n",
      "1050 tensor(72694.6328, grad_fn=<MseLossBackward0>)\n",
      "1051 tensor(72674.5703, grad_fn=<MseLossBackward0>)\n",
      "1052 tensor(72654.4922, grad_fn=<MseLossBackward0>)\n",
      "1053 tensor(72634.3984, grad_fn=<MseLossBackward0>)\n",
      "1054 tensor(72614.3125, grad_fn=<MseLossBackward0>)\n",
      "1055 tensor(72594.2109, grad_fn=<MseLossBackward0>)\n",
      "1056 tensor(72574.0938, grad_fn=<MseLossBackward0>)\n",
      "1057 tensor(72553.9766, grad_fn=<MseLossBackward0>)\n",
      "1058 tensor(72533.8438, grad_fn=<MseLossBackward0>)\n",
      "1059 tensor(72513.7188, grad_fn=<MseLossBackward0>)\n",
      "1060 tensor(72493.5703, grad_fn=<MseLossBackward0>)\n",
      "1061 tensor(72473.4219, grad_fn=<MseLossBackward0>)\n",
      "1062 tensor(72453.2734, grad_fn=<MseLossBackward0>)\n",
      "1063 tensor(72433.1016, grad_fn=<MseLossBackward0>)\n",
      "1064 tensor(72412.9297, grad_fn=<MseLossBackward0>)\n",
      "1065 tensor(72392.7422, grad_fn=<MseLossBackward0>)\n",
      "1066 tensor(72372.5547, grad_fn=<MseLossBackward0>)\n",
      "1067 tensor(72352.3672, grad_fn=<MseLossBackward0>)\n",
      "1068 tensor(72332.1562, grad_fn=<MseLossBackward0>)\n",
      "1069 tensor(72311.9453, grad_fn=<MseLossBackward0>)\n",
      "1070 tensor(72291.7344, grad_fn=<MseLossBackward0>)\n",
      "1071 tensor(72271.4922, grad_fn=<MseLossBackward0>)\n",
      "1072 tensor(72251.2656, grad_fn=<MseLossBackward0>)\n",
      "1073 tensor(72231.0234, grad_fn=<MseLossBackward0>)\n",
      "1074 tensor(72210.7734, grad_fn=<MseLossBackward0>)\n",
      "1075 tensor(72190.5156, grad_fn=<MseLossBackward0>)\n",
      "1076 tensor(72170.2422, grad_fn=<MseLossBackward0>)\n",
      "1077 tensor(72149.9688, grad_fn=<MseLossBackward0>)\n",
      "1078 tensor(72129.6875, grad_fn=<MseLossBackward0>)\n",
      "1079 tensor(72109.4062, grad_fn=<MseLossBackward0>)\n",
      "1080 tensor(72089.1094, grad_fn=<MseLossBackward0>)\n",
      "1081 tensor(72068.8047, grad_fn=<MseLossBackward0>)\n",
      "1082 tensor(72048.4922, grad_fn=<MseLossBackward0>)\n",
      "1083 tensor(72028.1641, grad_fn=<MseLossBackward0>)\n",
      "1084 tensor(72007.8438, grad_fn=<MseLossBackward0>)\n",
      "1085 tensor(71987.5078, grad_fn=<MseLossBackward0>)\n",
      "1086 tensor(71967.1562, grad_fn=<MseLossBackward0>)\n",
      "1087 tensor(71946.8125, grad_fn=<MseLossBackward0>)\n",
      "1088 tensor(71926.4609, grad_fn=<MseLossBackward0>)\n",
      "1089 tensor(71906.0938, grad_fn=<MseLossBackward0>)\n",
      "1090 tensor(71885.7109, grad_fn=<MseLossBackward0>)\n",
      "1091 tensor(71865.3359, grad_fn=<MseLossBackward0>)\n",
      "1092 tensor(71844.9531, grad_fn=<MseLossBackward0>)\n",
      "1093 tensor(71824.5469, grad_fn=<MseLossBackward0>)\n",
      "1094 tensor(71804.1406, grad_fn=<MseLossBackward0>)\n",
      "1095 tensor(71783.7344, grad_fn=<MseLossBackward0>)\n",
      "1096 tensor(71763.3125, grad_fn=<MseLossBackward0>)\n",
      "1097 tensor(71742.8906, grad_fn=<MseLossBackward0>)\n",
      "1098 tensor(71722.4531, grad_fn=<MseLossBackward0>)\n",
      "1099 tensor(71702.0156, grad_fn=<MseLossBackward0>)\n",
      "1100 tensor(71681.5781, grad_fn=<MseLossBackward0>)\n",
      "1101 tensor(71661.1094, grad_fn=<MseLossBackward0>)\n",
      "1102 tensor(71640.6562, grad_fn=<MseLossBackward0>)\n",
      "1103 tensor(71620.1797, grad_fn=<MseLossBackward0>)\n",
      "1104 tensor(71599.6953, grad_fn=<MseLossBackward0>)\n",
      "1105 tensor(71579.2266, grad_fn=<MseLossBackward0>)\n",
      "1106 tensor(71558.7266, grad_fn=<MseLossBackward0>)\n",
      "1107 tensor(71538.2266, grad_fn=<MseLossBackward0>)\n",
      "1108 tensor(71517.7109, grad_fn=<MseLossBackward0>)\n",
      "1109 tensor(71497.2031, grad_fn=<MseLossBackward0>)\n",
      "1110 tensor(71476.6797, grad_fn=<MseLossBackward0>)\n",
      "1111 tensor(71456.1484, grad_fn=<MseLossBackward0>)\n",
      "1112 tensor(71435.6172, grad_fn=<MseLossBackward0>)\n",
      "1113 tensor(71415.0703, grad_fn=<MseLossBackward0>)\n",
      "1114 tensor(71394.5156, grad_fn=<MseLossBackward0>)\n",
      "1115 tensor(71373.9609, grad_fn=<MseLossBackward0>)\n",
      "1116 tensor(71353.3984, grad_fn=<MseLossBackward0>)\n",
      "1117 tensor(71332.8203, grad_fn=<MseLossBackward0>)\n",
      "1118 tensor(71312.2422, grad_fn=<MseLossBackward0>)\n",
      "1119 tensor(71291.6562, grad_fn=<MseLossBackward0>)\n",
      "1120 tensor(71271.0625, grad_fn=<MseLossBackward0>)\n",
      "1121 tensor(71250.4609, grad_fn=<MseLossBackward0>)\n",
      "1122 tensor(71229.8516, grad_fn=<MseLossBackward0>)\n",
      "1123 tensor(71209.2266, grad_fn=<MseLossBackward0>)\n",
      "1124 tensor(71188.6016, grad_fn=<MseLossBackward0>)\n",
      "1125 tensor(71167.9844, grad_fn=<MseLossBackward0>)\n",
      "1126 tensor(71147.3359, grad_fn=<MseLossBackward0>)\n",
      "1127 tensor(71126.6875, grad_fn=<MseLossBackward0>)\n",
      "1128 tensor(71106.0469, grad_fn=<MseLossBackward0>)\n",
      "1129 tensor(71085.3828, grad_fn=<MseLossBackward0>)\n",
      "1130 tensor(71064.7109, grad_fn=<MseLossBackward0>)\n",
      "1131 tensor(71044.0391, grad_fn=<MseLossBackward0>)\n",
      "1132 tensor(71023.3594, grad_fn=<MseLossBackward0>)\n",
      "1133 tensor(71002.6719, grad_fn=<MseLossBackward0>)\n",
      "1134 tensor(70981.9766, grad_fn=<MseLossBackward0>)\n",
      "1135 tensor(70961.2734, grad_fn=<MseLossBackward0>)\n",
      "1136 tensor(70940.5703, grad_fn=<MseLossBackward0>)\n",
      "1137 tensor(70919.8516, grad_fn=<MseLossBackward0>)\n",
      "1138 tensor(70899.1250, grad_fn=<MseLossBackward0>)\n",
      "1139 tensor(70878.3984, grad_fn=<MseLossBackward0>)\n",
      "1140 tensor(70857.6641, grad_fn=<MseLossBackward0>)\n",
      "1141 tensor(70836.9062, grad_fn=<MseLossBackward0>)\n",
      "1142 tensor(70816.1562, grad_fn=<MseLossBackward0>)\n",
      "1143 tensor(70795.4062, grad_fn=<MseLossBackward0>)\n",
      "1144 tensor(70774.6328, grad_fn=<MseLossBackward0>)\n",
      "1145 tensor(70753.8672, grad_fn=<MseLossBackward0>)\n",
      "1146 tensor(70733.0938, grad_fn=<MseLossBackward0>)\n",
      "1147 tensor(70712.3047, grad_fn=<MseLossBackward0>)\n",
      "1148 tensor(70691.5078, grad_fn=<MseLossBackward0>)\n",
      "1149 tensor(70670.7109, grad_fn=<MseLossBackward0>)\n",
      "1150 tensor(70649.8984, grad_fn=<MseLossBackward0>)\n",
      "1151 tensor(70629.0859, grad_fn=<MseLossBackward0>)\n",
      "1152 tensor(70608.2656, grad_fn=<MseLossBackward0>)\n",
      "1153 tensor(70587.4375, grad_fn=<MseLossBackward0>)\n",
      "1154 tensor(70566.6094, grad_fn=<MseLossBackward0>)\n",
      "1155 tensor(70545.7578, grad_fn=<MseLossBackward0>)\n",
      "1156 tensor(70524.9219, grad_fn=<MseLossBackward0>)\n",
      "1157 tensor(70504.0547, grad_fn=<MseLossBackward0>)\n",
      "1158 tensor(70483.1953, grad_fn=<MseLossBackward0>)\n",
      "1159 tensor(70462.3281, grad_fn=<MseLossBackward0>)\n",
      "1160 tensor(70441.4453, grad_fn=<MseLossBackward0>)\n",
      "1161 tensor(70420.5703, grad_fn=<MseLossBackward0>)\n",
      "1162 tensor(70399.6875, grad_fn=<MseLossBackward0>)\n",
      "1163 tensor(70378.7812, grad_fn=<MseLossBackward0>)\n",
      "1164 tensor(70357.8828, grad_fn=<MseLossBackward0>)\n",
      "1165 tensor(70336.9688, grad_fn=<MseLossBackward0>)\n",
      "1166 tensor(70316.0547, grad_fn=<MseLossBackward0>)\n",
      "1167 tensor(70295.1328, grad_fn=<MseLossBackward0>)\n",
      "1168 tensor(70274.2031, grad_fn=<MseLossBackward0>)\n",
      "1169 tensor(70253.2734, grad_fn=<MseLossBackward0>)\n",
      "1170 tensor(70232.3203, grad_fn=<MseLossBackward0>)\n",
      "1171 tensor(70211.3828, grad_fn=<MseLossBackward0>)\n",
      "1172 tensor(70190.4141, grad_fn=<MseLossBackward0>)\n",
      "1173 tensor(70169.4531, grad_fn=<MseLossBackward0>)\n",
      "1174 tensor(70148.4922, grad_fn=<MseLossBackward0>)\n",
      "1175 tensor(70127.5078, grad_fn=<MseLossBackward0>)\n",
      "1176 tensor(70106.5234, grad_fn=<MseLossBackward0>)\n",
      "1177 tensor(70085.5391, grad_fn=<MseLossBackward0>)\n",
      "1178 tensor(70064.5469, grad_fn=<MseLossBackward0>)\n",
      "1179 tensor(70043.5391, grad_fn=<MseLossBackward0>)\n",
      "1180 tensor(70022.5312, grad_fn=<MseLossBackward0>)\n",
      "1181 tensor(70001.5156, grad_fn=<MseLossBackward0>)\n",
      "1182 tensor(69980.4922, grad_fn=<MseLossBackward0>)\n",
      "1183 tensor(69959.4688, grad_fn=<MseLossBackward0>)\n",
      "1184 tensor(69938.4219, grad_fn=<MseLossBackward0>)\n",
      "1185 tensor(69917.3906, grad_fn=<MseLossBackward0>)\n",
      "1186 tensor(69896.3359, grad_fn=<MseLossBackward0>)\n",
      "1187 tensor(69875.2812, grad_fn=<MseLossBackward0>)\n",
      "1188 tensor(69854.2266, grad_fn=<MseLossBackward0>)\n",
      "1189 tensor(69833.1562, grad_fn=<MseLossBackward0>)\n",
      "1190 tensor(69812.0781, grad_fn=<MseLossBackward0>)\n",
      "1191 tensor(69791., grad_fn=<MseLossBackward0>)\n",
      "1192 tensor(69769.9062, grad_fn=<MseLossBackward0>)\n",
      "1193 tensor(69748.8125, grad_fn=<MseLossBackward0>)\n",
      "1194 tensor(69727.7188, grad_fn=<MseLossBackward0>)\n",
      "1195 tensor(69706.6094, grad_fn=<MseLossBackward0>)\n",
      "1196 tensor(69685.5000, grad_fn=<MseLossBackward0>)\n",
      "1197 tensor(69664.3828, grad_fn=<MseLossBackward0>)\n",
      "1198 tensor(69643.2500, grad_fn=<MseLossBackward0>)\n",
      "1199 tensor(69622.1172, grad_fn=<MseLossBackward0>)\n",
      "1200 tensor(69600.9766, grad_fn=<MseLossBackward0>)\n",
      "1201 tensor(69579.8359, grad_fn=<MseLossBackward0>)\n",
      "1202 tensor(69558.6875, grad_fn=<MseLossBackward0>)\n",
      "1203 tensor(69537.5234, grad_fn=<MseLossBackward0>)\n",
      "1204 tensor(69516.3672, grad_fn=<MseLossBackward0>)\n",
      "1205 tensor(69495.1953, grad_fn=<MseLossBackward0>)\n",
      "1206 tensor(69474.0234, grad_fn=<MseLossBackward0>)\n",
      "1207 tensor(69452.8359, grad_fn=<MseLossBackward0>)\n",
      "1208 tensor(69431.6406, grad_fn=<MseLossBackward0>)\n",
      "1209 tensor(69410.4531, grad_fn=<MseLossBackward0>)\n",
      "1210 tensor(69389.2500, grad_fn=<MseLossBackward0>)\n",
      "1211 tensor(69368.0391, grad_fn=<MseLossBackward0>)\n",
      "1212 tensor(69346.8203, grad_fn=<MseLossBackward0>)\n",
      "1213 tensor(69325.6094, grad_fn=<MseLossBackward0>)\n",
      "1214 tensor(69304.3750, grad_fn=<MseLossBackward0>)\n",
      "1215 tensor(69283.1406, grad_fn=<MseLossBackward0>)\n",
      "1216 tensor(69261.9062, grad_fn=<MseLossBackward0>)\n",
      "1217 tensor(69240.6562, grad_fn=<MseLossBackward0>)\n",
      "1218 tensor(69219.4141, grad_fn=<MseLossBackward0>)\n",
      "1219 tensor(69198.1406, grad_fn=<MseLossBackward0>)\n",
      "1220 tensor(69176.8906, grad_fn=<MseLossBackward0>)\n",
      "1221 tensor(69155.6250, grad_fn=<MseLossBackward0>)\n",
      "1222 tensor(69134.3438, grad_fn=<MseLossBackward0>)\n",
      "1223 tensor(69113.0625, grad_fn=<MseLossBackward0>)\n",
      "1224 tensor(69091.7734, grad_fn=<MseLossBackward0>)\n",
      "1225 tensor(69070.4766, grad_fn=<MseLossBackward0>)\n",
      "1226 tensor(69049.1797, grad_fn=<MseLossBackward0>)\n",
      "1227 tensor(69027.8750, grad_fn=<MseLossBackward0>)\n",
      "1228 tensor(69006.5625, grad_fn=<MseLossBackward0>)\n",
      "1229 tensor(68985.2422, grad_fn=<MseLossBackward0>)\n",
      "1230 tensor(68963.9219, grad_fn=<MseLossBackward0>)\n",
      "1231 tensor(68942.5938, grad_fn=<MseLossBackward0>)\n",
      "1232 tensor(68921.2500, grad_fn=<MseLossBackward0>)\n",
      "1233 tensor(68899.9062, grad_fn=<MseLossBackward0>)\n",
      "1234 tensor(68878.5625, grad_fn=<MseLossBackward0>)\n",
      "1235 tensor(68857.2109, grad_fn=<MseLossBackward0>)\n",
      "1236 tensor(68835.8438, grad_fn=<MseLossBackward0>)\n",
      "1237 tensor(68814.4766, grad_fn=<MseLossBackward0>)\n",
      "1238 tensor(68793.0938, grad_fn=<MseLossBackward0>)\n",
      "1239 tensor(68771.7266, grad_fn=<MseLossBackward0>)\n",
      "1240 tensor(68750.3438, grad_fn=<MseLossBackward0>)\n",
      "1241 tensor(68728.9453, grad_fn=<MseLossBackward0>)\n",
      "1242 tensor(68707.5547, grad_fn=<MseLossBackward0>)\n",
      "1243 tensor(68686.1562, grad_fn=<MseLossBackward0>)\n",
      "1244 tensor(68664.7422, grad_fn=<MseLossBackward0>)\n",
      "1245 tensor(68643.3359, grad_fn=<MseLossBackward0>)\n",
      "1246 tensor(68621.9141, grad_fn=<MseLossBackward0>)\n",
      "1247 tensor(68600.4844, grad_fn=<MseLossBackward0>)\n",
      "1248 tensor(68579.0547, grad_fn=<MseLossBackward0>)\n",
      "1249 tensor(68557.6094, grad_fn=<MseLossBackward0>)\n",
      "1250 tensor(68536.1719, grad_fn=<MseLossBackward0>)\n",
      "1251 tensor(68514.7266, grad_fn=<MseLossBackward0>)\n",
      "1252 tensor(68493.2734, grad_fn=<MseLossBackward0>)\n",
      "1253 tensor(68471.8125, grad_fn=<MseLossBackward0>)\n",
      "1254 tensor(68450.3516, grad_fn=<MseLossBackward0>)\n",
      "1255 tensor(68428.8672, grad_fn=<MseLossBackward0>)\n",
      "1256 tensor(68407.3984, grad_fn=<MseLossBackward0>)\n",
      "1257 tensor(68385.9062, grad_fn=<MseLossBackward0>)\n",
      "1258 tensor(68364.4297, grad_fn=<MseLossBackward0>)\n",
      "1259 tensor(68342.9297, grad_fn=<MseLossBackward0>)\n",
      "1260 tensor(68321.4297, grad_fn=<MseLossBackward0>)\n",
      "1261 tensor(68299.9219, grad_fn=<MseLossBackward0>)\n",
      "1262 tensor(68278.4141, grad_fn=<MseLossBackward0>)\n",
      "1263 tensor(68256.8906, grad_fn=<MseLossBackward0>)\n",
      "1264 tensor(68235.3750, grad_fn=<MseLossBackward0>)\n",
      "1265 tensor(68213.8438, grad_fn=<MseLossBackward0>)\n",
      "1266 tensor(68192.3047, grad_fn=<MseLossBackward0>)\n",
      "1267 tensor(68170.7656, grad_fn=<MseLossBackward0>)\n",
      "1268 tensor(68149.2109, grad_fn=<MseLossBackward0>)\n",
      "1269 tensor(68127.6719, grad_fn=<MseLossBackward0>)\n",
      "1270 tensor(68106.1172, grad_fn=<MseLossBackward0>)\n",
      "1271 tensor(68084.5469, grad_fn=<MseLossBackward0>)\n",
      "1272 tensor(68062.9766, grad_fn=<MseLossBackward0>)\n",
      "1273 tensor(68041.3984, grad_fn=<MseLossBackward0>)\n",
      "1274 tensor(68019.8281, grad_fn=<MseLossBackward0>)\n",
      "1275 tensor(67998.2344, grad_fn=<MseLossBackward0>)\n",
      "1276 tensor(67976.6484, grad_fn=<MseLossBackward0>)\n",
      "1277 tensor(67955.0547, grad_fn=<MseLossBackward0>)\n",
      "1278 tensor(67933.4453, grad_fn=<MseLossBackward0>)\n",
      "1279 tensor(67911.8438, grad_fn=<MseLossBackward0>)\n",
      "1280 tensor(67890.2344, grad_fn=<MseLossBackward0>)\n",
      "1281 tensor(67868.6094, grad_fn=<MseLossBackward0>)\n",
      "1282 tensor(67846.9844, grad_fn=<MseLossBackward0>)\n",
      "1283 tensor(67825.3594, grad_fn=<MseLossBackward0>)\n",
      "1284 tensor(67803.7266, grad_fn=<MseLossBackward0>)\n",
      "1285 tensor(67782.0859, grad_fn=<MseLossBackward0>)\n",
      "1286 tensor(67760.4375, grad_fn=<MseLossBackward0>)\n",
      "1287 tensor(67738.7891, grad_fn=<MseLossBackward0>)\n",
      "1288 tensor(67717.1328, grad_fn=<MseLossBackward0>)\n",
      "1289 tensor(67695.4688, grad_fn=<MseLossBackward0>)\n",
      "1290 tensor(67673.8125, grad_fn=<MseLossBackward0>)\n",
      "1291 tensor(67652.1328, grad_fn=<MseLossBackward0>)\n",
      "1292 tensor(67630.4531, grad_fn=<MseLossBackward0>)\n",
      "1293 tensor(67608.7734, grad_fn=<MseLossBackward0>)\n",
      "1294 tensor(67587.0781, grad_fn=<MseLossBackward0>)\n",
      "1295 tensor(67565.3984, grad_fn=<MseLossBackward0>)\n",
      "1296 tensor(67543.6953, grad_fn=<MseLossBackward0>)\n",
      "1297 tensor(67521.9844, grad_fn=<MseLossBackward0>)\n",
      "1298 tensor(67500.2812, grad_fn=<MseLossBackward0>)\n",
      "1299 tensor(67478.5625, grad_fn=<MseLossBackward0>)\n",
      "1300 tensor(67456.8438, grad_fn=<MseLossBackward0>)\n",
      "1301 tensor(67435.1172, grad_fn=<MseLossBackward0>)\n",
      "1302 tensor(67413.3828, grad_fn=<MseLossBackward0>)\n",
      "1303 tensor(67391.6562, grad_fn=<MseLossBackward0>)\n",
      "1304 tensor(67369.9141, grad_fn=<MseLossBackward0>)\n",
      "1305 tensor(67348.1641, grad_fn=<MseLossBackward0>)\n",
      "1306 tensor(67326.4141, grad_fn=<MseLossBackward0>)\n",
      "1307 tensor(67304.6562, grad_fn=<MseLossBackward0>)\n",
      "1308 tensor(67282.8984, grad_fn=<MseLossBackward0>)\n",
      "1309 tensor(67261.1328, grad_fn=<MseLossBackward0>)\n",
      "1310 tensor(67239.3594, grad_fn=<MseLossBackward0>)\n",
      "1311 tensor(67217.5859, grad_fn=<MseLossBackward0>)\n",
      "1312 tensor(67195.7891, grad_fn=<MseLossBackward0>)\n",
      "1313 tensor(67174.0156, grad_fn=<MseLossBackward0>)\n",
      "1314 tensor(67152.2109, grad_fn=<MseLossBackward0>)\n",
      "1315 tensor(67130.4219, grad_fn=<MseLossBackward0>)\n",
      "1316 tensor(67108.6172, grad_fn=<MseLossBackward0>)\n",
      "1317 tensor(67086.8125, grad_fn=<MseLossBackward0>)\n",
      "1318 tensor(67065., grad_fn=<MseLossBackward0>)\n",
      "1319 tensor(67043.1719, grad_fn=<MseLossBackward0>)\n",
      "1320 tensor(67021.3516, grad_fn=<MseLossBackward0>)\n",
      "1321 tensor(66999.5234, grad_fn=<MseLossBackward0>)\n",
      "1322 tensor(66977.6797, grad_fn=<MseLossBackward0>)\n",
      "1323 tensor(66955.8438, grad_fn=<MseLossBackward0>)\n",
      "1324 tensor(66934., grad_fn=<MseLossBackward0>)\n",
      "1325 tensor(66912.1641, grad_fn=<MseLossBackward0>)\n",
      "1326 tensor(66890.3203, grad_fn=<MseLossBackward0>)\n",
      "1327 tensor(66868.4531, grad_fn=<MseLossBackward0>)\n",
      "1328 tensor(66846.5859, grad_fn=<MseLossBackward0>)\n",
      "1329 tensor(66824.7266, grad_fn=<MseLossBackward0>)\n",
      "1330 tensor(66802.8438, grad_fn=<MseLossBackward0>)\n",
      "1331 tensor(66780.9688, grad_fn=<MseLossBackward0>)\n",
      "1332 tensor(66759.0859, grad_fn=<MseLossBackward0>)\n",
      "1333 tensor(66737.2031, grad_fn=<MseLossBackward0>)\n",
      "1334 tensor(66715.2969, grad_fn=<MseLossBackward0>)\n",
      "1335 tensor(66693.3984, grad_fn=<MseLossBackward0>)\n",
      "1336 tensor(66671.5000, grad_fn=<MseLossBackward0>)\n",
      "1337 tensor(66649.5938, grad_fn=<MseLossBackward0>)\n",
      "1338 tensor(66627.6797, grad_fn=<MseLossBackward0>)\n",
      "1339 tensor(66605.7578, grad_fn=<MseLossBackward0>)\n",
      "1340 tensor(66583.8438, grad_fn=<MseLossBackward0>)\n",
      "1341 tensor(66561.9141, grad_fn=<MseLossBackward0>)\n",
      "1342 tensor(66539.9844, grad_fn=<MseLossBackward0>)\n",
      "1343 tensor(66518.0547, grad_fn=<MseLossBackward0>)\n",
      "1344 tensor(66496.1094, grad_fn=<MseLossBackward0>)\n",
      "1345 tensor(66474.1641, grad_fn=<MseLossBackward0>)\n",
      "1346 tensor(66452.2109, grad_fn=<MseLossBackward0>)\n",
      "1347 tensor(66430.2578, grad_fn=<MseLossBackward0>)\n",
      "1348 tensor(66408.2969, grad_fn=<MseLossBackward0>)\n",
      "1349 tensor(66386.3359, grad_fn=<MseLossBackward0>)\n",
      "1350 tensor(66364.3594, grad_fn=<MseLossBackward0>)\n",
      "1351 tensor(66342.3828, grad_fn=<MseLossBackward0>)\n",
      "1352 tensor(66320.4062, grad_fn=<MseLossBackward0>)\n",
      "1353 tensor(66298.4219, grad_fn=<MseLossBackward0>)\n",
      "1354 tensor(66276.4297, grad_fn=<MseLossBackward0>)\n",
      "1355 tensor(66254.4453, grad_fn=<MseLossBackward0>)\n",
      "1356 tensor(66232.4453, grad_fn=<MseLossBackward0>)\n",
      "1357 tensor(66210.4375, grad_fn=<MseLossBackward0>)\n",
      "1358 tensor(66188.4375, grad_fn=<MseLossBackward0>)\n",
      "1359 tensor(66166.4219, grad_fn=<MseLossBackward0>)\n",
      "1360 tensor(66144.4141, grad_fn=<MseLossBackward0>)\n",
      "1361 tensor(66122.3828, grad_fn=<MseLossBackward0>)\n",
      "1362 tensor(66100.3594, grad_fn=<MseLossBackward0>)\n",
      "1363 tensor(66078.3359, grad_fn=<MseLossBackward0>)\n",
      "1364 tensor(66056.2969, grad_fn=<MseLossBackward0>)\n",
      "1365 tensor(66034.2578, grad_fn=<MseLossBackward0>)\n",
      "1366 tensor(66012.2188, grad_fn=<MseLossBackward0>)\n",
      "1367 tensor(65990.1641, grad_fn=<MseLossBackward0>)\n",
      "1368 tensor(65968.1172, grad_fn=<MseLossBackward0>)\n",
      "1369 tensor(65946.0547, grad_fn=<MseLossBackward0>)\n",
      "1370 tensor(65923.9922, grad_fn=<MseLossBackward0>)\n",
      "1371 tensor(65901.9297, grad_fn=<MseLossBackward0>)\n",
      "1372 tensor(65879.8516, grad_fn=<MseLossBackward0>)\n",
      "1373 tensor(65857.7812, grad_fn=<MseLossBackward0>)\n",
      "1374 tensor(65835.7031, grad_fn=<MseLossBackward0>)\n",
      "1375 tensor(65813.6094, grad_fn=<MseLossBackward0>)\n",
      "1376 tensor(65791.5312, grad_fn=<MseLossBackward0>)\n",
      "1377 tensor(65769.4297, grad_fn=<MseLossBackward0>)\n",
      "1378 tensor(65747.3359, grad_fn=<MseLossBackward0>)\n",
      "1379 tensor(65725.2344, grad_fn=<MseLossBackward0>)\n",
      "1380 tensor(65703.1250, grad_fn=<MseLossBackward0>)\n",
      "1381 tensor(65681.0156, grad_fn=<MseLossBackward0>)\n",
      "1382 tensor(65658.9062, grad_fn=<MseLossBackward0>)\n",
      "1383 tensor(65636.7812, grad_fn=<MseLossBackward0>)\n",
      "1384 tensor(65614.6641, grad_fn=<MseLossBackward0>)\n",
      "1385 tensor(65592.5234, grad_fn=<MseLossBackward0>)\n",
      "1386 tensor(65570.3984, grad_fn=<MseLossBackward0>)\n",
      "1387 tensor(65548.2656, grad_fn=<MseLossBackward0>)\n",
      "1388 tensor(65526.1211, grad_fn=<MseLossBackward0>)\n",
      "1389 tensor(65503.9805, grad_fn=<MseLossBackward0>)\n",
      "1390 tensor(65481.8320, grad_fn=<MseLossBackward0>)\n",
      "1391 tensor(65459.6758, grad_fn=<MseLossBackward0>)\n",
      "1392 tensor(65437.5117, grad_fn=<MseLossBackward0>)\n",
      "1393 tensor(65415.3516, grad_fn=<MseLossBackward0>)\n",
      "1394 tensor(65393.1914, grad_fn=<MseLossBackward0>)\n",
      "1395 tensor(65371.0156, grad_fn=<MseLossBackward0>)\n",
      "1396 tensor(65348.8438, grad_fn=<MseLossBackward0>)\n",
      "1397 tensor(65326.6602, grad_fn=<MseLossBackward0>)\n",
      "1398 tensor(65304.4805, grad_fn=<MseLossBackward0>)\n",
      "1399 tensor(65282.2930, grad_fn=<MseLossBackward0>)\n",
      "1400 tensor(65260.0938, grad_fn=<MseLossBackward0>)\n",
      "1401 tensor(65237.9023, grad_fn=<MseLossBackward0>)\n",
      "1402 tensor(65215.6953, grad_fn=<MseLossBackward0>)\n",
      "1403 tensor(65193.5039, grad_fn=<MseLossBackward0>)\n",
      "1404 tensor(65171.2930, grad_fn=<MseLossBackward0>)\n",
      "1405 tensor(65149.0820, grad_fn=<MseLossBackward0>)\n",
      "1406 tensor(65126.8633, grad_fn=<MseLossBackward0>)\n",
      "1407 tensor(65104.6445, grad_fn=<MseLossBackward0>)\n",
      "1408 tensor(65082.4141, grad_fn=<MseLossBackward0>)\n",
      "1409 tensor(65060.1914, grad_fn=<MseLossBackward0>)\n",
      "1410 tensor(65037.9531, grad_fn=<MseLossBackward0>)\n",
      "1411 tensor(65015.7227, grad_fn=<MseLossBackward0>)\n",
      "1412 tensor(64993.4805, grad_fn=<MseLossBackward0>)\n",
      "1413 tensor(64971.2383, grad_fn=<MseLossBackward0>)\n",
      "1414 tensor(64948.9883, grad_fn=<MseLossBackward0>)\n",
      "1415 tensor(64926.7344, grad_fn=<MseLossBackward0>)\n",
      "1416 tensor(64904.4766, grad_fn=<MseLossBackward0>)\n",
      "1417 tensor(64882.2148, grad_fn=<MseLossBackward0>)\n",
      "1418 tensor(64859.9492, grad_fn=<MseLossBackward0>)\n",
      "1419 tensor(64837.6797, grad_fn=<MseLossBackward0>)\n",
      "1420 tensor(64815.4102, grad_fn=<MseLossBackward0>)\n",
      "1421 tensor(64793.1328, grad_fn=<MseLossBackward0>)\n",
      "1422 tensor(64770.8516, grad_fn=<MseLossBackward0>)\n",
      "1423 tensor(64748.5625, grad_fn=<MseLossBackward0>)\n",
      "1424 tensor(64726.2773, grad_fn=<MseLossBackward0>)\n",
      "1425 tensor(64703.9844, grad_fn=<MseLossBackward0>)\n",
      "1426 tensor(64681.6836, grad_fn=<MseLossBackward0>)\n",
      "1427 tensor(64659.3867, grad_fn=<MseLossBackward0>)\n",
      "1428 tensor(64637.0820, grad_fn=<MseLossBackward0>)\n",
      "1429 tensor(64614.7734, grad_fn=<MseLossBackward0>)\n",
      "1430 tensor(64592.4648, grad_fn=<MseLossBackward0>)\n",
      "1431 tensor(64570.1523, grad_fn=<MseLossBackward0>)\n",
      "1432 tensor(64547.8320, grad_fn=<MseLossBackward0>)\n",
      "1433 tensor(64525.5039, grad_fn=<MseLossBackward0>)\n",
      "1434 tensor(64503.1797, grad_fn=<MseLossBackward0>)\n",
      "1435 tensor(64480.8477, grad_fn=<MseLossBackward0>)\n",
      "1436 tensor(64458.5117, grad_fn=<MseLossBackward0>)\n",
      "1437 tensor(64436.1758, grad_fn=<MseLossBackward0>)\n",
      "1438 tensor(64413.8320, grad_fn=<MseLossBackward0>)\n",
      "1439 tensor(64391.4883, grad_fn=<MseLossBackward0>)\n",
      "1440 tensor(64369.1406, grad_fn=<MseLossBackward0>)\n",
      "1441 tensor(64346.7852, grad_fn=<MseLossBackward0>)\n",
      "1442 tensor(64324.4297, grad_fn=<MseLossBackward0>)\n",
      "1443 tensor(64302.0664, grad_fn=<MseLossBackward0>)\n",
      "1444 tensor(64279.6992, grad_fn=<MseLossBackward0>)\n",
      "1445 tensor(64257.3359, grad_fn=<MseLossBackward0>)\n",
      "1446 tensor(64234.9648, grad_fn=<MseLossBackward0>)\n",
      "1447 tensor(64212.5898, grad_fn=<MseLossBackward0>)\n",
      "1448 tensor(64190.2070, grad_fn=<MseLossBackward0>)\n",
      "1449 tensor(64167.8203, grad_fn=<MseLossBackward0>)\n",
      "1450 tensor(64145.4336, grad_fn=<MseLossBackward0>)\n",
      "1451 tensor(64123.0469, grad_fn=<MseLossBackward0>)\n",
      "1452 tensor(64100.6602, grad_fn=<MseLossBackward0>)\n",
      "1453 tensor(64078.2578, grad_fn=<MseLossBackward0>)\n",
      "1454 tensor(64055.8555, grad_fn=<MseLossBackward0>)\n",
      "1455 tensor(64033.4570, grad_fn=<MseLossBackward0>)\n",
      "1456 tensor(64011.0469, grad_fn=<MseLossBackward0>)\n",
      "1457 tensor(63988.6328, grad_fn=<MseLossBackward0>)\n",
      "1458 tensor(63966.2227, grad_fn=<MseLossBackward0>)\n",
      "1459 tensor(63943.7969, grad_fn=<MseLossBackward0>)\n",
      "1460 tensor(63921.3789, grad_fn=<MseLossBackward0>)\n",
      "1461 tensor(63898.9570, grad_fn=<MseLossBackward0>)\n",
      "1462 tensor(63876.5312, grad_fn=<MseLossBackward0>)\n",
      "1463 tensor(63854.0938, grad_fn=<MseLossBackward0>)\n",
      "1464 tensor(63831.6641, grad_fn=<MseLossBackward0>)\n",
      "1465 tensor(63809.2148, grad_fn=<MseLossBackward0>)\n",
      "1466 tensor(63786.7773, grad_fn=<MseLossBackward0>)\n",
      "1467 tensor(63764.3281, grad_fn=<MseLossBackward0>)\n",
      "1468 tensor(63741.8867, grad_fn=<MseLossBackward0>)\n",
      "1469 tensor(63719.4297, grad_fn=<MseLossBackward0>)\n",
      "1470 tensor(63696.9766, grad_fn=<MseLossBackward0>)\n",
      "1471 tensor(63674.5156, grad_fn=<MseLossBackward0>)\n",
      "1472 tensor(63652.0508, grad_fn=<MseLossBackward0>)\n",
      "1473 tensor(63629.5898, grad_fn=<MseLossBackward0>)\n",
      "1474 tensor(63607.1172, grad_fn=<MseLossBackward0>)\n",
      "1475 tensor(63584.6406, grad_fn=<MseLossBackward0>)\n",
      "1476 tensor(63562.1680, grad_fn=<MseLossBackward0>)\n",
      "1477 tensor(63539.6875, grad_fn=<MseLossBackward0>)\n",
      "1478 tensor(63517.2070, grad_fn=<MseLossBackward0>)\n",
      "1479 tensor(63494.7188, grad_fn=<MseLossBackward0>)\n",
      "1480 tensor(63472.2344, grad_fn=<MseLossBackward0>)\n",
      "1481 tensor(63449.7422, grad_fn=<MseLossBackward0>)\n",
      "1482 tensor(63427.2461, grad_fn=<MseLossBackward0>)\n",
      "1483 tensor(63404.7422, grad_fn=<MseLossBackward0>)\n",
      "1484 tensor(63382.2383, grad_fn=<MseLossBackward0>)\n",
      "1485 tensor(63359.7305, grad_fn=<MseLossBackward0>)\n",
      "1486 tensor(63337.2266, grad_fn=<MseLossBackward0>)\n",
      "1487 tensor(63314.7148, grad_fn=<MseLossBackward0>)\n",
      "1488 tensor(63292.2031, grad_fn=<MseLossBackward0>)\n",
      "1489 tensor(63269.6797, grad_fn=<MseLossBackward0>)\n",
      "1490 tensor(63247.1523, grad_fn=<MseLossBackward0>)\n",
      "1491 tensor(63224.6367, grad_fn=<MseLossBackward0>)\n",
      "1492 tensor(63202.1055, grad_fn=<MseLossBackward0>)\n",
      "1493 tensor(63179.5742, grad_fn=<MseLossBackward0>)\n",
      "1494 tensor(63157.0391, grad_fn=<MseLossBackward0>)\n",
      "1495 tensor(63134.5078, grad_fn=<MseLossBackward0>)\n",
      "1496 tensor(63111.9570, grad_fn=<MseLossBackward0>)\n",
      "1497 tensor(63089.4219, grad_fn=<MseLossBackward0>)\n",
      "1498 tensor(63066.8711, grad_fn=<MseLossBackward0>)\n",
      "1499 tensor(63044.3242, grad_fn=<MseLossBackward0>)\n",
      "1500 tensor(63021.7734, grad_fn=<MseLossBackward0>)\n",
      "1501 tensor(62999.2109, grad_fn=<MseLossBackward0>)\n",
      "1502 tensor(62976.6562, grad_fn=<MseLossBackward0>)\n",
      "1503 tensor(62954.0938, grad_fn=<MseLossBackward0>)\n",
      "1504 tensor(62931.5273, grad_fn=<MseLossBackward0>)\n",
      "1505 tensor(62908.9570, grad_fn=<MseLossBackward0>)\n",
      "1506 tensor(62886.3906, grad_fn=<MseLossBackward0>)\n",
      "1507 tensor(62863.8164, grad_fn=<MseLossBackward0>)\n",
      "1508 tensor(62841.2344, grad_fn=<MseLossBackward0>)\n",
      "1509 tensor(62818.6562, grad_fn=<MseLossBackward0>)\n",
      "1510 tensor(62796.0742, grad_fn=<MseLossBackward0>)\n",
      "1511 tensor(62773.4883, grad_fn=<MseLossBackward0>)\n",
      "1512 tensor(62750.9023, grad_fn=<MseLossBackward0>)\n",
      "1513 tensor(62728.3164, grad_fn=<MseLossBackward0>)\n",
      "1514 tensor(62705.7148, grad_fn=<MseLossBackward0>)\n",
      "1515 tensor(62683.1172, grad_fn=<MseLossBackward0>)\n",
      "1516 tensor(62660.5156, grad_fn=<MseLossBackward0>)\n",
      "1517 tensor(62637.9180, grad_fn=<MseLossBackward0>)\n",
      "1518 tensor(62615.3125, grad_fn=<MseLossBackward0>)\n",
      "1519 tensor(62592.6992, grad_fn=<MseLossBackward0>)\n",
      "1520 tensor(62570.0898, grad_fn=<MseLossBackward0>)\n",
      "1521 tensor(62547.4688, grad_fn=<MseLossBackward0>)\n",
      "1522 tensor(62524.8594, grad_fn=<MseLossBackward0>)\n",
      "1523 tensor(62502.2422, grad_fn=<MseLossBackward0>)\n",
      "1524 tensor(62479.6172, grad_fn=<MseLossBackward0>)\n",
      "1525 tensor(62456.9844, grad_fn=<MseLossBackward0>)\n",
      "1526 tensor(62434.3633, grad_fn=<MseLossBackward0>)\n",
      "1527 tensor(62411.7305, grad_fn=<MseLossBackward0>)\n",
      "1528 tensor(62389.0938, grad_fn=<MseLossBackward0>)\n",
      "1529 tensor(62366.4648, grad_fn=<MseLossBackward0>)\n",
      "1530 tensor(62343.8203, grad_fn=<MseLossBackward0>)\n",
      "1531 tensor(62321.1836, grad_fn=<MseLossBackward0>)\n",
      "1532 tensor(62298.5352, grad_fn=<MseLossBackward0>)\n",
      "1533 tensor(62275.8906, grad_fn=<MseLossBackward0>)\n",
      "1534 tensor(62253.2344, grad_fn=<MseLossBackward0>)\n",
      "1535 tensor(62230.5859, grad_fn=<MseLossBackward0>)\n",
      "1536 tensor(62207.9258, grad_fn=<MseLossBackward0>)\n",
      "1537 tensor(62185.2695, grad_fn=<MseLossBackward0>)\n",
      "1538 tensor(62162.6094, grad_fn=<MseLossBackward0>)\n",
      "1539 tensor(62139.9453, grad_fn=<MseLossBackward0>)\n",
      "1540 tensor(62117.2773, grad_fn=<MseLossBackward0>)\n",
      "1541 tensor(62094.6094, grad_fn=<MseLossBackward0>)\n",
      "1542 tensor(62071.9414, grad_fn=<MseLossBackward0>)\n",
      "1543 tensor(62049.2656, grad_fn=<MseLossBackward0>)\n",
      "1544 tensor(62026.5938, grad_fn=<MseLossBackward0>)\n",
      "1545 tensor(62003.9141, grad_fn=<MseLossBackward0>)\n",
      "1546 tensor(61981.2305, grad_fn=<MseLossBackward0>)\n",
      "1547 tensor(61958.5469, grad_fn=<MseLossBackward0>)\n",
      "1548 tensor(61935.8594, grad_fn=<MseLossBackward0>)\n",
      "1549 tensor(61913.1719, grad_fn=<MseLossBackward0>)\n",
      "1550 tensor(61890.4805, grad_fn=<MseLossBackward0>)\n",
      "1551 tensor(61867.7930, grad_fn=<MseLossBackward0>)\n",
      "1552 tensor(61845.0938, grad_fn=<MseLossBackward0>)\n",
      "1553 tensor(61822.3945, grad_fn=<MseLossBackward0>)\n",
      "1554 tensor(61799.6875, grad_fn=<MseLossBackward0>)\n",
      "1555 tensor(61776.9883, grad_fn=<MseLossBackward0>)\n",
      "1556 tensor(61754.2812, grad_fn=<MseLossBackward0>)\n",
      "1557 tensor(61731.5703, grad_fn=<MseLossBackward0>)\n",
      "1558 tensor(61708.8672, grad_fn=<MseLossBackward0>)\n",
      "1559 tensor(61686.1484, grad_fn=<MseLossBackward0>)\n",
      "1560 tensor(61663.4297, grad_fn=<MseLossBackward0>)\n",
      "1561 tensor(61640.7109, grad_fn=<MseLossBackward0>)\n",
      "1562 tensor(61617.9922, grad_fn=<MseLossBackward0>)\n",
      "1563 tensor(61595.2695, grad_fn=<MseLossBackward0>)\n",
      "1564 tensor(61572.5430, grad_fn=<MseLossBackward0>)\n",
      "1565 tensor(61549.8125, grad_fn=<MseLossBackward0>)\n",
      "1566 tensor(61527.0820, grad_fn=<MseLossBackward0>)\n",
      "1567 tensor(61504.3516, grad_fn=<MseLossBackward0>)\n",
      "1568 tensor(61481.6211, grad_fn=<MseLossBackward0>)\n",
      "1569 tensor(61458.8867, grad_fn=<MseLossBackward0>)\n",
      "1570 tensor(61436.1367, grad_fn=<MseLossBackward0>)\n",
      "1571 tensor(61413.3984, grad_fn=<MseLossBackward0>)\n",
      "1572 tensor(61390.6641, grad_fn=<MseLossBackward0>)\n",
      "1573 tensor(61367.9141, grad_fn=<MseLossBackward0>)\n",
      "1574 tensor(61345.1641, grad_fn=<MseLossBackward0>)\n",
      "1575 tensor(61322.4102, grad_fn=<MseLossBackward0>)\n",
      "1576 tensor(61299.6641, grad_fn=<MseLossBackward0>)\n",
      "1577 tensor(61276.9102, grad_fn=<MseLossBackward0>)\n",
      "1578 tensor(61254.1484, grad_fn=<MseLossBackward0>)\n",
      "1579 tensor(61231.3945, grad_fn=<MseLossBackward0>)\n",
      "1580 tensor(61208.6328, grad_fn=<MseLossBackward0>)\n",
      "1581 tensor(61185.8711, grad_fn=<MseLossBackward0>)\n",
      "1582 tensor(61163.0938, grad_fn=<MseLossBackward0>)\n",
      "1583 tensor(61140.3398, grad_fn=<MseLossBackward0>)\n",
      "1584 tensor(61117.5664, grad_fn=<MseLossBackward0>)\n",
      "1585 tensor(61094.7891, grad_fn=<MseLossBackward0>)\n",
      "1586 tensor(61072.0156, grad_fn=<MseLossBackward0>)\n",
      "1587 tensor(61049.2422, grad_fn=<MseLossBackward0>)\n",
      "1588 tensor(61026.4688, grad_fn=<MseLossBackward0>)\n",
      "1589 tensor(61003.6875, grad_fn=<MseLossBackward0>)\n",
      "1590 tensor(60980.9023, grad_fn=<MseLossBackward0>)\n",
      "1591 tensor(60958.1211, grad_fn=<MseLossBackward0>)\n",
      "1592 tensor(60935.3320, grad_fn=<MseLossBackward0>)\n",
      "1593 tensor(60912.5469, grad_fn=<MseLossBackward0>)\n",
      "1594 tensor(60889.7539, grad_fn=<MseLossBackward0>)\n",
      "1595 tensor(60866.9609, grad_fn=<MseLossBackward0>)\n",
      "1596 tensor(60844.1680, grad_fn=<MseLossBackward0>)\n",
      "1597 tensor(60821.3672, grad_fn=<MseLossBackward0>)\n",
      "1598 tensor(60798.5742, grad_fn=<MseLossBackward0>)\n",
      "1599 tensor(60775.7734, grad_fn=<MseLossBackward0>)\n",
      "1600 tensor(60752.9688, grad_fn=<MseLossBackward0>)\n",
      "1601 tensor(60730.1641, grad_fn=<MseLossBackward0>)\n",
      "1602 tensor(60707.3594, grad_fn=<MseLossBackward0>)\n",
      "1603 tensor(60684.5547, grad_fn=<MseLossBackward0>)\n",
      "1604 tensor(60661.7344, grad_fn=<MseLossBackward0>)\n",
      "1605 tensor(60638.9297, grad_fn=<MseLossBackward0>)\n",
      "1606 tensor(60616.1172, grad_fn=<MseLossBackward0>)\n",
      "1607 tensor(60593.3008, grad_fn=<MseLossBackward0>)\n",
      "1608 tensor(60570.4805, grad_fn=<MseLossBackward0>)\n",
      "1609 tensor(60547.6641, grad_fn=<MseLossBackward0>)\n",
      "1610 tensor(60524.8398, grad_fn=<MseLossBackward0>)\n",
      "1611 tensor(60502.0195, grad_fn=<MseLossBackward0>)\n",
      "1612 tensor(60479.1953, grad_fn=<MseLossBackward0>)\n",
      "1613 tensor(60456.3672, grad_fn=<MseLossBackward0>)\n",
      "1614 tensor(60433.5430, grad_fn=<MseLossBackward0>)\n",
      "1615 tensor(60410.7109, grad_fn=<MseLossBackward0>)\n",
      "1616 tensor(60387.8750, grad_fn=<MseLossBackward0>)\n",
      "1617 tensor(60365.0430, grad_fn=<MseLossBackward0>)\n",
      "1618 tensor(60342.2070, grad_fn=<MseLossBackward0>)\n",
      "1619 tensor(60319.3711, grad_fn=<MseLossBackward0>)\n",
      "1620 tensor(60296.5352, grad_fn=<MseLossBackward0>)\n",
      "1621 tensor(60273.6914, grad_fn=<MseLossBackward0>)\n",
      "1622 tensor(60250.8516, grad_fn=<MseLossBackward0>)\n",
      "1623 tensor(60228.0039, grad_fn=<MseLossBackward0>)\n",
      "1624 tensor(60205.1602, grad_fn=<MseLossBackward0>)\n",
      "1625 tensor(60182.3164, grad_fn=<MseLossBackward0>)\n",
      "1626 tensor(60159.4609, grad_fn=<MseLossBackward0>)\n",
      "1627 tensor(60136.6211, grad_fn=<MseLossBackward0>)\n",
      "1628 tensor(60113.7656, grad_fn=<MseLossBackward0>)\n",
      "1629 tensor(60090.9062, grad_fn=<MseLossBackward0>)\n",
      "1630 tensor(60068.0586, grad_fn=<MseLossBackward0>)\n",
      "1631 tensor(60045.1953, grad_fn=<MseLossBackward0>)\n",
      "1632 tensor(60022.3398, grad_fn=<MseLossBackward0>)\n",
      "1633 tensor(59999.4844, grad_fn=<MseLossBackward0>)\n",
      "1634 tensor(59976.6250, grad_fn=<MseLossBackward0>)\n",
      "1635 tensor(59953.7578, grad_fn=<MseLossBackward0>)\n",
      "1636 tensor(59930.8867, grad_fn=<MseLossBackward0>)\n",
      "1637 tensor(59908.0273, grad_fn=<MseLossBackward0>)\n",
      "1638 tensor(59885.1562, grad_fn=<MseLossBackward0>)\n",
      "1639 tensor(59862.2891, grad_fn=<MseLossBackward0>)\n",
      "1640 tensor(59839.4141, grad_fn=<MseLossBackward0>)\n",
      "1641 tensor(59816.5469, grad_fn=<MseLossBackward0>)\n",
      "1642 tensor(59793.6719, grad_fn=<MseLossBackward0>)\n",
      "1643 tensor(59770.7930, grad_fn=<MseLossBackward0>)\n",
      "1644 tensor(59747.9180, grad_fn=<MseLossBackward0>)\n",
      "1645 tensor(59725.0430, grad_fn=<MseLossBackward0>)\n",
      "1646 tensor(59702.1680, grad_fn=<MseLossBackward0>)\n",
      "1647 tensor(59679.2812, grad_fn=<MseLossBackward0>)\n",
      "1648 tensor(59656.4023, grad_fn=<MseLossBackward0>)\n",
      "1649 tensor(59633.5156, grad_fn=<MseLossBackward0>)\n",
      "1650 tensor(59610.6328, grad_fn=<MseLossBackward0>)\n",
      "1651 tensor(59587.7461, grad_fn=<MseLossBackward0>)\n",
      "1652 tensor(59564.8594, grad_fn=<MseLossBackward0>)\n",
      "1653 tensor(59541.9688, grad_fn=<MseLossBackward0>)\n",
      "1654 tensor(59519.0820, grad_fn=<MseLossBackward0>)\n",
      "1655 tensor(59496.1875, grad_fn=<MseLossBackward0>)\n",
      "1656 tensor(59473.2930, grad_fn=<MseLossBackward0>)\n",
      "1657 tensor(59450.4023, grad_fn=<MseLossBackward0>)\n",
      "1658 tensor(59427.5039, grad_fn=<MseLossBackward0>)\n",
      "1659 tensor(59404.6094, grad_fn=<MseLossBackward0>)\n",
      "1660 tensor(59381.7109, grad_fn=<MseLossBackward0>)\n",
      "1661 tensor(59358.8125, grad_fn=<MseLossBackward0>)\n",
      "1662 tensor(59335.9102, grad_fn=<MseLossBackward0>)\n",
      "1663 tensor(59313.0039, grad_fn=<MseLossBackward0>)\n",
      "1664 tensor(59290.1055, grad_fn=<MseLossBackward0>)\n",
      "1665 tensor(59267.1992, grad_fn=<MseLossBackward0>)\n",
      "1666 tensor(59244.2969, grad_fn=<MseLossBackward0>)\n",
      "1667 tensor(59221.3906, grad_fn=<MseLossBackward0>)\n",
      "1668 tensor(59198.4844, grad_fn=<MseLossBackward0>)\n",
      "1669 tensor(59175.5703, grad_fn=<MseLossBackward0>)\n",
      "1670 tensor(59152.6680, grad_fn=<MseLossBackward0>)\n",
      "1671 tensor(59129.7539, grad_fn=<MseLossBackward0>)\n",
      "1672 tensor(59106.8359, grad_fn=<MseLossBackward0>)\n",
      "1673 tensor(59083.9258, grad_fn=<MseLossBackward0>)\n",
      "1674 tensor(59061.0117, grad_fn=<MseLossBackward0>)\n",
      "1675 tensor(59038.0938, grad_fn=<MseLossBackward0>)\n",
      "1676 tensor(59015.1836, grad_fn=<MseLossBackward0>)\n",
      "1677 tensor(58992.2656, grad_fn=<MseLossBackward0>)\n",
      "1678 tensor(58969.3398, grad_fn=<MseLossBackward0>)\n",
      "1679 tensor(58946.4219, grad_fn=<MseLossBackward0>)\n",
      "1680 tensor(58923.5039, grad_fn=<MseLossBackward0>)\n",
      "1681 tensor(58900.5742, grad_fn=<MseLossBackward0>)\n",
      "1682 tensor(58877.6562, grad_fn=<MseLossBackward0>)\n",
      "1683 tensor(58854.7305, grad_fn=<MseLossBackward0>)\n",
      "1684 tensor(58831.8086, grad_fn=<MseLossBackward0>)\n",
      "1685 tensor(58808.8828, grad_fn=<MseLossBackward0>)\n",
      "1686 tensor(58785.9531, grad_fn=<MseLossBackward0>)\n",
      "1687 tensor(58763.0195, grad_fn=<MseLossBackward0>)\n",
      "1688 tensor(58740.0977, grad_fn=<MseLossBackward0>)\n",
      "1689 tensor(58717.1602, grad_fn=<MseLossBackward0>)\n",
      "1690 tensor(58694.2305, grad_fn=<MseLossBackward0>)\n",
      "1691 tensor(58671.3008, grad_fn=<MseLossBackward0>)\n",
      "1692 tensor(58648.3672, grad_fn=<MseLossBackward0>)\n",
      "1693 tensor(58625.4375, grad_fn=<MseLossBackward0>)\n",
      "1694 tensor(58602.4961, grad_fn=<MseLossBackward0>)\n",
      "1695 tensor(58579.5703, grad_fn=<MseLossBackward0>)\n",
      "1696 tensor(58556.6328, grad_fn=<MseLossBackward0>)\n",
      "1697 tensor(58533.6953, grad_fn=<MseLossBackward0>)\n",
      "1698 tensor(58510.7539, grad_fn=<MseLossBackward0>)\n",
      "1699 tensor(58487.8164, grad_fn=<MseLossBackward0>)\n",
      "1700 tensor(58464.8789, grad_fn=<MseLossBackward0>)\n",
      "1701 tensor(58441.9375, grad_fn=<MseLossBackward0>)\n",
      "1702 tensor(58418.9922, grad_fn=<MseLossBackward0>)\n",
      "1703 tensor(58396.0586, grad_fn=<MseLossBackward0>)\n",
      "1704 tensor(58373.1133, grad_fn=<MseLossBackward0>)\n",
      "1705 tensor(58350.1719, grad_fn=<MseLossBackward0>)\n",
      "1706 tensor(58327.2227, grad_fn=<MseLossBackward0>)\n",
      "1707 tensor(58304.2773, grad_fn=<MseLossBackward0>)\n",
      "1708 tensor(58281.3281, grad_fn=<MseLossBackward0>)\n",
      "1709 tensor(58258.3867, grad_fn=<MseLossBackward0>)\n",
      "1710 tensor(58235.4414, grad_fn=<MseLossBackward0>)\n",
      "1711 tensor(58212.4922, grad_fn=<MseLossBackward0>)\n",
      "1712 tensor(58189.5430, grad_fn=<MseLossBackward0>)\n",
      "1713 tensor(58166.5938, grad_fn=<MseLossBackward0>)\n",
      "1714 tensor(58143.6406, grad_fn=<MseLossBackward0>)\n",
      "1715 tensor(58120.6953, grad_fn=<MseLossBackward0>)\n",
      "1716 tensor(58097.7461, grad_fn=<MseLossBackward0>)\n",
      "1717 tensor(58074.7891, grad_fn=<MseLossBackward0>)\n",
      "1718 tensor(58051.8359, grad_fn=<MseLossBackward0>)\n",
      "1719 tensor(58028.8867, grad_fn=<MseLossBackward0>)\n",
      "1720 tensor(58005.9297, grad_fn=<MseLossBackward0>)\n",
      "1721 tensor(57982.9766, grad_fn=<MseLossBackward0>)\n",
      "1722 tensor(57960.0195, grad_fn=<MseLossBackward0>)\n",
      "1723 tensor(57937.0703, grad_fn=<MseLossBackward0>)\n",
      "1724 tensor(57914.1094, grad_fn=<MseLossBackward0>)\n",
      "1725 tensor(57891.1523, grad_fn=<MseLossBackward0>)\n",
      "1726 tensor(57868.1953, grad_fn=<MseLossBackward0>)\n",
      "1727 tensor(57845.2344, grad_fn=<MseLossBackward0>)\n",
      "1728 tensor(57822.2852, grad_fn=<MseLossBackward0>)\n",
      "1729 tensor(57799.3242, grad_fn=<MseLossBackward0>)\n",
      "1730 tensor(57776.3672, grad_fn=<MseLossBackward0>)\n",
      "1731 tensor(57753.4062, grad_fn=<MseLossBackward0>)\n",
      "1732 tensor(57730.4453, grad_fn=<MseLossBackward0>)\n",
      "1733 tensor(57707.4844, grad_fn=<MseLossBackward0>)\n",
      "1734 tensor(57684.5273, grad_fn=<MseLossBackward0>)\n",
      "1735 tensor(57661.5664, grad_fn=<MseLossBackward0>)\n",
      "1736 tensor(57638.6016, grad_fn=<MseLossBackward0>)\n",
      "1737 tensor(57615.6406, grad_fn=<MseLossBackward0>)\n",
      "1738 tensor(57592.6797, grad_fn=<MseLossBackward0>)\n",
      "1739 tensor(57569.7109, grad_fn=<MseLossBackward0>)\n",
      "1740 tensor(57546.7500, grad_fn=<MseLossBackward0>)\n",
      "1741 tensor(57523.7891, grad_fn=<MseLossBackward0>)\n",
      "1742 tensor(57500.8242, grad_fn=<MseLossBackward0>)\n",
      "1743 tensor(57477.8594, grad_fn=<MseLossBackward0>)\n",
      "1744 tensor(57454.8945, grad_fn=<MseLossBackward0>)\n",
      "1745 tensor(57431.9258, grad_fn=<MseLossBackward0>)\n",
      "1746 tensor(57408.9648, grad_fn=<MseLossBackward0>)\n",
      "1747 tensor(57385.9961, grad_fn=<MseLossBackward0>)\n",
      "1748 tensor(57363.0312, grad_fn=<MseLossBackward0>)\n",
      "1749 tensor(57340.0664, grad_fn=<MseLossBackward0>)\n",
      "1750 tensor(57317.1016, grad_fn=<MseLossBackward0>)\n",
      "1751 tensor(57294.1328, grad_fn=<MseLossBackward0>)\n",
      "1752 tensor(57271.1680, grad_fn=<MseLossBackward0>)\n",
      "1753 tensor(57248.2031, grad_fn=<MseLossBackward0>)\n",
      "1754 tensor(57225.2344, grad_fn=<MseLossBackward0>)\n",
      "1755 tensor(57202.2656, grad_fn=<MseLossBackward0>)\n",
      "1756 tensor(57179.2969, grad_fn=<MseLossBackward0>)\n",
      "1757 tensor(57156.3281, grad_fn=<MseLossBackward0>)\n",
      "1758 tensor(57133.3672, grad_fn=<MseLossBackward0>)\n",
      "1759 tensor(57110.3945, grad_fn=<MseLossBackward0>)\n",
      "1760 tensor(57087.4297, grad_fn=<MseLossBackward0>)\n",
      "1761 tensor(57064.4648, grad_fn=<MseLossBackward0>)\n",
      "1762 tensor(57041.4883, grad_fn=<MseLossBackward0>)\n",
      "1763 tensor(57018.5195, grad_fn=<MseLossBackward0>)\n",
      "1764 tensor(56995.5508, grad_fn=<MseLossBackward0>)\n",
      "1765 tensor(56972.5859, grad_fn=<MseLossBackward0>)\n",
      "1766 tensor(56949.6172, grad_fn=<MseLossBackward0>)\n",
      "1767 tensor(56926.6484, grad_fn=<MseLossBackward0>)\n",
      "1768 tensor(56903.6797, grad_fn=<MseLossBackward0>)\n",
      "1769 tensor(56880.7070, grad_fn=<MseLossBackward0>)\n",
      "1770 tensor(56857.7383, grad_fn=<MseLossBackward0>)\n",
      "1771 tensor(56834.7695, grad_fn=<MseLossBackward0>)\n",
      "1772 tensor(56811.7969, grad_fn=<MseLossBackward0>)\n",
      "1773 tensor(56788.8320, grad_fn=<MseLossBackward0>)\n",
      "1774 tensor(56765.8594, grad_fn=<MseLossBackward0>)\n",
      "1775 tensor(56742.8906, grad_fn=<MseLossBackward0>)\n",
      "1776 tensor(56719.9219, grad_fn=<MseLossBackward0>)\n",
      "1777 tensor(56696.9531, grad_fn=<MseLossBackward0>)\n",
      "1778 tensor(56673.9883, grad_fn=<MseLossBackward0>)\n",
      "1779 tensor(56651.0195, grad_fn=<MseLossBackward0>)\n",
      "1780 tensor(56628.0508, grad_fn=<MseLossBackward0>)\n",
      "1781 tensor(56605.0820, grad_fn=<MseLossBackward0>)\n",
      "1782 tensor(56582.1133, grad_fn=<MseLossBackward0>)\n",
      "1783 tensor(56559.1406, grad_fn=<MseLossBackward0>)\n",
      "1784 tensor(56536.1719, grad_fn=<MseLossBackward0>)\n",
      "1785 tensor(56513.2109, grad_fn=<MseLossBackward0>)\n",
      "1786 tensor(56490.2422, grad_fn=<MseLossBackward0>)\n",
      "1787 tensor(56467.2656, grad_fn=<MseLossBackward0>)\n",
      "1788 tensor(56444.3008, grad_fn=<MseLossBackward0>)\n",
      "1789 tensor(56421.3359, grad_fn=<MseLossBackward0>)\n",
      "1790 tensor(56398.3672, grad_fn=<MseLossBackward0>)\n",
      "1791 tensor(56375.4023, grad_fn=<MseLossBackward0>)\n",
      "1792 tensor(56352.4336, grad_fn=<MseLossBackward0>)\n",
      "1793 tensor(56329.4648, grad_fn=<MseLossBackward0>)\n",
      "1794 tensor(56306.4961, grad_fn=<MseLossBackward0>)\n",
      "1795 tensor(56283.5352, grad_fn=<MseLossBackward0>)\n",
      "1796 tensor(56260.5664, grad_fn=<MseLossBackward0>)\n",
      "1797 tensor(56237.6055, grad_fn=<MseLossBackward0>)\n",
      "1798 tensor(56214.6367, grad_fn=<MseLossBackward0>)\n",
      "1799 tensor(56191.6680, grad_fn=<MseLossBackward0>)\n",
      "1800 tensor(56168.7070, grad_fn=<MseLossBackward0>)\n",
      "1801 tensor(56145.7383, grad_fn=<MseLossBackward0>)\n",
      "1802 tensor(56122.7773, grad_fn=<MseLossBackward0>)\n",
      "1803 tensor(56099.8164, grad_fn=<MseLossBackward0>)\n",
      "1804 tensor(56076.8477, grad_fn=<MseLossBackward0>)\n",
      "1805 tensor(56053.8867, grad_fn=<MseLossBackward0>)\n",
      "1806 tensor(56030.9297, grad_fn=<MseLossBackward0>)\n",
      "1807 tensor(56007.9609, grad_fn=<MseLossBackward0>)\n",
      "1808 tensor(55984.9922, grad_fn=<MseLossBackward0>)\n",
      "1809 tensor(55962.0430, grad_fn=<MseLossBackward0>)\n",
      "1810 tensor(55939.0820, grad_fn=<MseLossBackward0>)\n",
      "1811 tensor(55916.1133, grad_fn=<MseLossBackward0>)\n",
      "1812 tensor(55893.1523, grad_fn=<MseLossBackward0>)\n",
      "1813 tensor(55870.1953, grad_fn=<MseLossBackward0>)\n",
      "1814 tensor(55847.2344, grad_fn=<MseLossBackward0>)\n",
      "1815 tensor(55824.2773, grad_fn=<MseLossBackward0>)\n",
      "1816 tensor(55801.3164, grad_fn=<MseLossBackward0>)\n",
      "1817 tensor(55778.3633, grad_fn=<MseLossBackward0>)\n",
      "1818 tensor(55755.4062, grad_fn=<MseLossBackward0>)\n",
      "1819 tensor(55732.4453, grad_fn=<MseLossBackward0>)\n",
      "1820 tensor(55709.4805, grad_fn=<MseLossBackward0>)\n",
      "1821 tensor(55686.5312, grad_fn=<MseLossBackward0>)\n",
      "1822 tensor(55663.5781, grad_fn=<MseLossBackward0>)\n",
      "1823 tensor(55640.6211, grad_fn=<MseLossBackward0>)\n",
      "1824 tensor(55617.6719, grad_fn=<MseLossBackward0>)\n",
      "1825 tensor(55594.7148, grad_fn=<MseLossBackward0>)\n",
      "1826 tensor(55571.7617, grad_fn=<MseLossBackward0>)\n",
      "1827 tensor(55548.8047, grad_fn=<MseLossBackward0>)\n",
      "1828 tensor(55525.8555, grad_fn=<MseLossBackward0>)\n",
      "1829 tensor(55502.9062, grad_fn=<MseLossBackward0>)\n",
      "1830 tensor(55479.9570, grad_fn=<MseLossBackward0>)\n",
      "1831 tensor(55457., grad_fn=<MseLossBackward0>)\n",
      "1832 tensor(55434.0508, grad_fn=<MseLossBackward0>)\n",
      "1833 tensor(55411.1094, grad_fn=<MseLossBackward0>)\n",
      "1834 tensor(55388.1602, grad_fn=<MseLossBackward0>)\n",
      "1835 tensor(55365.2109, grad_fn=<MseLossBackward0>)\n",
      "1836 tensor(55342.2617, grad_fn=<MseLossBackward0>)\n",
      "1837 tensor(55319.3164, grad_fn=<MseLossBackward0>)\n",
      "1838 tensor(55296.3672, grad_fn=<MseLossBackward0>)\n",
      "1839 tensor(55273.4336, grad_fn=<MseLossBackward0>)\n",
      "1840 tensor(55250.4844, grad_fn=<MseLossBackward0>)\n",
      "1841 tensor(55227.5391, grad_fn=<MseLossBackward0>)\n",
      "1842 tensor(55204.6016, grad_fn=<MseLossBackward0>)\n",
      "1843 tensor(55181.6602, grad_fn=<MseLossBackward0>)\n",
      "1844 tensor(55158.7227, grad_fn=<MseLossBackward0>)\n",
      "1845 tensor(55135.7812, grad_fn=<MseLossBackward0>)\n",
      "1846 tensor(55112.8359, grad_fn=<MseLossBackward0>)\n",
      "1847 tensor(55089.9062, grad_fn=<MseLossBackward0>)\n",
      "1848 tensor(55066.9609, grad_fn=<MseLossBackward0>)\n",
      "1849 tensor(55044.0273, grad_fn=<MseLossBackward0>)\n",
      "1850 tensor(55021.0898, grad_fn=<MseLossBackward0>)\n",
      "1851 tensor(54998.1602, grad_fn=<MseLossBackward0>)\n",
      "1852 tensor(54975.2227, grad_fn=<MseLossBackward0>)\n",
      "1853 tensor(54952.2930, grad_fn=<MseLossBackward0>)\n",
      "1854 tensor(54929.3594, grad_fn=<MseLossBackward0>)\n",
      "1855 tensor(54906.4219, grad_fn=<MseLossBackward0>)\n",
      "1856 tensor(54883.5000, grad_fn=<MseLossBackward0>)\n",
      "1857 tensor(54860.5703, grad_fn=<MseLossBackward0>)\n",
      "1858 tensor(54837.6367, grad_fn=<MseLossBackward0>)\n",
      "1859 tensor(54814.7148, grad_fn=<MseLossBackward0>)\n",
      "1860 tensor(54791.7812, grad_fn=<MseLossBackward0>)\n",
      "1861 tensor(54768.8594, grad_fn=<MseLossBackward0>)\n",
      "1862 tensor(54745.9336, grad_fn=<MseLossBackward0>)\n",
      "1863 tensor(54723.0117, grad_fn=<MseLossBackward0>)\n",
      "1864 tensor(54700.0859, grad_fn=<MseLossBackward0>)\n",
      "1865 tensor(54677.1602, grad_fn=<MseLossBackward0>)\n",
      "1866 tensor(54654.2422, grad_fn=<MseLossBackward0>)\n",
      "1867 tensor(54631.3242, grad_fn=<MseLossBackward0>)\n",
      "1868 tensor(54608.4141, grad_fn=<MseLossBackward0>)\n",
      "1869 tensor(54585.4883, grad_fn=<MseLossBackward0>)\n",
      "1870 tensor(54562.5781, grad_fn=<MseLossBackward0>)\n",
      "1871 tensor(54539.6641, grad_fn=<MseLossBackward0>)\n",
      "1872 tensor(54516.7461, grad_fn=<MseLossBackward0>)\n",
      "1873 tensor(54493.8359, grad_fn=<MseLossBackward0>)\n",
      "1874 tensor(54470.9219, grad_fn=<MseLossBackward0>)\n",
      "1875 tensor(54448.0117, grad_fn=<MseLossBackward0>)\n",
      "1876 tensor(54425.1016, grad_fn=<MseLossBackward0>)\n",
      "1877 tensor(54402.1953, grad_fn=<MseLossBackward0>)\n",
      "1878 tensor(54379.2891, grad_fn=<MseLossBackward0>)\n",
      "1879 tensor(54356.3828, grad_fn=<MseLossBackward0>)\n",
      "1880 tensor(54333.4727, grad_fn=<MseLossBackward0>)\n",
      "1881 tensor(54310.5781, grad_fn=<MseLossBackward0>)\n",
      "1882 tensor(54287.6719, grad_fn=<MseLossBackward0>)\n",
      "1883 tensor(54264.7734, grad_fn=<MseLossBackward0>)\n",
      "1884 tensor(54241.8672, grad_fn=<MseLossBackward0>)\n",
      "1885 tensor(54218.9688, grad_fn=<MseLossBackward0>)\n",
      "1886 tensor(54196.0742, grad_fn=<MseLossBackward0>)\n",
      "1887 tensor(54173.1758, grad_fn=<MseLossBackward0>)\n",
      "1888 tensor(54150.2812, grad_fn=<MseLossBackward0>)\n",
      "1889 tensor(54127.3906, grad_fn=<MseLossBackward0>)\n",
      "1890 tensor(54104.4961, grad_fn=<MseLossBackward0>)\n",
      "1891 tensor(54081.6055, grad_fn=<MseLossBackward0>)\n",
      "1892 tensor(54058.7109, grad_fn=<MseLossBackward0>)\n",
      "1893 tensor(54035.8281, grad_fn=<MseLossBackward0>)\n",
      "1894 tensor(54012.9375, grad_fn=<MseLossBackward0>)\n",
      "1895 tensor(53990.0547, grad_fn=<MseLossBackward0>)\n",
      "1896 tensor(53967.1758, grad_fn=<MseLossBackward0>)\n",
      "1897 tensor(53944.2812, grad_fn=<MseLossBackward0>)\n",
      "1898 tensor(53921.4062, grad_fn=<MseLossBackward0>)\n",
      "1899 tensor(53898.5273, grad_fn=<MseLossBackward0>)\n",
      "1900 tensor(53875.6484, grad_fn=<MseLossBackward0>)\n",
      "1901 tensor(53852.7734, grad_fn=<MseLossBackward0>)\n",
      "1902 tensor(53829.8984, grad_fn=<MseLossBackward0>)\n",
      "1903 tensor(53807.0234, grad_fn=<MseLossBackward0>)\n",
      "1904 tensor(53784.1484, grad_fn=<MseLossBackward0>)\n",
      "1905 tensor(53761.2734, grad_fn=<MseLossBackward0>)\n",
      "1906 tensor(53738.4062, grad_fn=<MseLossBackward0>)\n",
      "1907 tensor(53715.5391, grad_fn=<MseLossBackward0>)\n",
      "1908 tensor(53692.6719, grad_fn=<MseLossBackward0>)\n",
      "1909 tensor(53669.8047, grad_fn=<MseLossBackward0>)\n",
      "1910 tensor(53646.9414, grad_fn=<MseLossBackward0>)\n",
      "1911 tensor(53624.0781, grad_fn=<MseLossBackward0>)\n",
      "1912 tensor(53601.2188, grad_fn=<MseLossBackward0>)\n",
      "1913 tensor(53578.3555, grad_fn=<MseLossBackward0>)\n",
      "1914 tensor(53555.5000, grad_fn=<MseLossBackward0>)\n",
      "1915 tensor(53532.6445, grad_fn=<MseLossBackward0>)\n",
      "1916 tensor(53509.7891, grad_fn=<MseLossBackward0>)\n",
      "1917 tensor(53486.9414, grad_fn=<MseLossBackward0>)\n",
      "1918 tensor(53464.0898, grad_fn=<MseLossBackward0>)\n",
      "1919 tensor(53441.2344, grad_fn=<MseLossBackward0>)\n",
      "1920 tensor(53418.3867, grad_fn=<MseLossBackward0>)\n",
      "1921 tensor(53395.5430, grad_fn=<MseLossBackward0>)\n",
      "1922 tensor(53372.6992, grad_fn=<MseLossBackward0>)\n",
      "1923 tensor(53349.8555, grad_fn=<MseLossBackward0>)\n",
      "1924 tensor(53327.0117, grad_fn=<MseLossBackward0>)\n",
      "1925 tensor(53304.1719, grad_fn=<MseLossBackward0>)\n",
      "1926 tensor(53281.3320, grad_fn=<MseLossBackward0>)\n",
      "1927 tensor(53258.5039, grad_fn=<MseLossBackward0>)\n",
      "1928 tensor(53235.6641, grad_fn=<MseLossBackward0>)\n",
      "1929 tensor(53212.8281, grad_fn=<MseLossBackward0>)\n",
      "1930 tensor(53190.0039, grad_fn=<MseLossBackward0>)\n",
      "1931 tensor(53167.1758, grad_fn=<MseLossBackward0>)\n",
      "1932 tensor(53144.3438, grad_fn=<MseLossBackward0>)\n",
      "1933 tensor(53121.5195, grad_fn=<MseLossBackward0>)\n",
      "1934 tensor(53098.6875, grad_fn=<MseLossBackward0>)\n",
      "1935 tensor(53075.8711, grad_fn=<MseLossBackward0>)\n",
      "1936 tensor(53053.0508, grad_fn=<MseLossBackward0>)\n",
      "1937 tensor(53030.2344, grad_fn=<MseLossBackward0>)\n",
      "1938 tensor(53007.4102, grad_fn=<MseLossBackward0>)\n",
      "1939 tensor(52984.5977, grad_fn=<MseLossBackward0>)\n",
      "1940 tensor(52961.7852, grad_fn=<MseLossBackward0>)\n",
      "1941 tensor(52938.9727, grad_fn=<MseLossBackward0>)\n",
      "1942 tensor(52916.1680, grad_fn=<MseLossBackward0>)\n",
      "1943 tensor(52893.3555, grad_fn=<MseLossBackward0>)\n",
      "1944 tensor(52870.5586, grad_fn=<MseLossBackward0>)\n",
      "1945 tensor(52847.7461, grad_fn=<MseLossBackward0>)\n",
      "1946 tensor(52824.9453, grad_fn=<MseLossBackward0>)\n",
      "1947 tensor(52802.1406, grad_fn=<MseLossBackward0>)\n",
      "1948 tensor(52779.3477, grad_fn=<MseLossBackward0>)\n",
      "1949 tensor(52756.5469, grad_fn=<MseLossBackward0>)\n",
      "1950 tensor(52733.7617, grad_fn=<MseLossBackward0>)\n",
      "1951 tensor(52710.9688, grad_fn=<MseLossBackward0>)\n",
      "1952 tensor(52688.1680, grad_fn=<MseLossBackward0>)\n",
      "1953 tensor(52665.3828, grad_fn=<MseLossBackward0>)\n",
      "1954 tensor(52642.6016, grad_fn=<MseLossBackward0>)\n",
      "1955 tensor(52619.8164, grad_fn=<MseLossBackward0>)\n",
      "1956 tensor(52597.0352, grad_fn=<MseLossBackward0>)\n",
      "1957 tensor(52574.2539, grad_fn=<MseLossBackward0>)\n",
      "1958 tensor(52551.4805, grad_fn=<MseLossBackward0>)\n",
      "1959 tensor(52528.6992, grad_fn=<MseLossBackward0>)\n",
      "1960 tensor(52505.9258, grad_fn=<MseLossBackward0>)\n",
      "1961 tensor(52483.1523, grad_fn=<MseLossBackward0>)\n",
      "1962 tensor(52460.3906, grad_fn=<MseLossBackward0>)\n",
      "1963 tensor(52437.6133, grad_fn=<MseLossBackward0>)\n",
      "1964 tensor(52414.8516, grad_fn=<MseLossBackward0>)\n",
      "1965 tensor(52392.0898, grad_fn=<MseLossBackward0>)\n",
      "1966 tensor(52369.3281, grad_fn=<MseLossBackward0>)\n",
      "1967 tensor(52346.5664, grad_fn=<MseLossBackward0>)\n",
      "1968 tensor(52323.8164, grad_fn=<MseLossBackward0>)\n",
      "1969 tensor(52301.0625, grad_fn=<MseLossBackward0>)\n",
      "1970 tensor(52278.3086, grad_fn=<MseLossBackward0>)\n",
      "1971 tensor(52255.5508, grad_fn=<MseLossBackward0>)\n",
      "1972 tensor(52232.8086, grad_fn=<MseLossBackward0>)\n",
      "1973 tensor(52210.0586, grad_fn=<MseLossBackward0>)\n",
      "1974 tensor(52187.3164, grad_fn=<MseLossBackward0>)\n",
      "1975 tensor(52164.5742, grad_fn=<MseLossBackward0>)\n",
      "1976 tensor(52141.8398, grad_fn=<MseLossBackward0>)\n",
      "1977 tensor(52119.1055, grad_fn=<MseLossBackward0>)\n",
      "1978 tensor(52096.3672, grad_fn=<MseLossBackward0>)\n",
      "1979 tensor(52073.6328, grad_fn=<MseLossBackward0>)\n",
      "1980 tensor(52050.9023, grad_fn=<MseLossBackward0>)\n",
      "1981 tensor(52028.1758, grad_fn=<MseLossBackward0>)\n",
      "1982 tensor(52005.4492, grad_fn=<MseLossBackward0>)\n",
      "1983 tensor(51982.7305, grad_fn=<MseLossBackward0>)\n",
      "1984 tensor(51960.0078, grad_fn=<MseLossBackward0>)\n",
      "1985 tensor(51937.2891, grad_fn=<MseLossBackward0>)\n",
      "1986 tensor(51914.5664, grad_fn=<MseLossBackward0>)\n",
      "1987 tensor(51891.8555, grad_fn=<MseLossBackward0>)\n",
      "1988 tensor(51869.1445, grad_fn=<MseLossBackward0>)\n",
      "1989 tensor(51846.4375, grad_fn=<MseLossBackward0>)\n",
      "1990 tensor(51823.7305, grad_fn=<MseLossBackward0>)\n",
      "1991 tensor(51801.0273, grad_fn=<MseLossBackward0>)\n",
      "1992 tensor(51778.3242, grad_fn=<MseLossBackward0>)\n",
      "1993 tensor(51755.6250, grad_fn=<MseLossBackward0>)\n",
      "1994 tensor(51732.9297, grad_fn=<MseLossBackward0>)\n",
      "1995 tensor(51710.2344, grad_fn=<MseLossBackward0>)\n",
      "1996 tensor(51687.5430, grad_fn=<MseLossBackward0>)\n",
      "1997 tensor(51664.8516, grad_fn=<MseLossBackward0>)\n",
      "1998 tensor(51642.1680, grad_fn=<MseLossBackward0>)\n",
      "1999 tensor(51619.4844, grad_fn=<MseLossBackward0>)\n",
      "2000 tensor(51596.7969, grad_fn=<MseLossBackward0>)\n",
      "2001 tensor(51574.1172, grad_fn=<MseLossBackward0>)\n",
      "2002 tensor(51551.4414, grad_fn=<MseLossBackward0>)\n",
      "2003 tensor(51528.7695, grad_fn=<MseLossBackward0>)\n",
      "2004 tensor(51506.1016, grad_fn=<MseLossBackward0>)\n",
      "2005 tensor(51483.4258, grad_fn=<MseLossBackward0>)\n",
      "2006 tensor(51460.7617, grad_fn=<MseLossBackward0>)\n",
      "2007 tensor(51438.0938, grad_fn=<MseLossBackward0>)\n",
      "2008 tensor(51415.4336, grad_fn=<MseLossBackward0>)\n",
      "2009 tensor(51392.7734, grad_fn=<MseLossBackward0>)\n",
      "2010 tensor(51370.1211, grad_fn=<MseLossBackward0>)\n",
      "2011 tensor(51347.4609, grad_fn=<MseLossBackward0>)\n",
      "2012 tensor(51324.8125, grad_fn=<MseLossBackward0>)\n",
      "2013 tensor(51302.1641, grad_fn=<MseLossBackward0>)\n",
      "2014 tensor(51279.5195, grad_fn=<MseLossBackward0>)\n",
      "2015 tensor(51256.8750, grad_fn=<MseLossBackward0>)\n",
      "2016 tensor(51234.2383, grad_fn=<MseLossBackward0>)\n",
      "2017 tensor(51211.5977, grad_fn=<MseLossBackward0>)\n",
      "2018 tensor(51188.9609, grad_fn=<MseLossBackward0>)\n",
      "2019 tensor(51166.3242, grad_fn=<MseLossBackward0>)\n",
      "2020 tensor(51143.6992, grad_fn=<MseLossBackward0>)\n",
      "2021 tensor(51121.0742, grad_fn=<MseLossBackward0>)\n",
      "2022 tensor(51098.4453, grad_fn=<MseLossBackward0>)\n",
      "2023 tensor(51075.8281, grad_fn=<MseLossBackward0>)\n",
      "2024 tensor(51053.2070, grad_fn=<MseLossBackward0>)\n",
      "2025 tensor(51030.5977, grad_fn=<MseLossBackward0>)\n",
      "2026 tensor(51007.9805, grad_fn=<MseLossBackward0>)\n",
      "2027 tensor(50985.3711, grad_fn=<MseLossBackward0>)\n",
      "2028 tensor(50962.7656, grad_fn=<MseLossBackward0>)\n",
      "2029 tensor(50940.1562, grad_fn=<MseLossBackward0>)\n",
      "2030 tensor(50917.5547, grad_fn=<MseLossBackward0>)\n",
      "2031 tensor(50894.9492, grad_fn=<MseLossBackward0>)\n",
      "2032 tensor(50872.3594, grad_fn=<MseLossBackward0>)\n",
      "2033 tensor(50849.7656, grad_fn=<MseLossBackward0>)\n",
      "2034 tensor(50827.1758, grad_fn=<MseLossBackward0>)\n",
      "2035 tensor(50804.5859, grad_fn=<MseLossBackward0>)\n",
      "2036 tensor(50782., grad_fn=<MseLossBackward0>)\n",
      "2037 tensor(50759.4219, grad_fn=<MseLossBackward0>)\n",
      "2038 tensor(50736.8438, grad_fn=<MseLossBackward0>)\n",
      "2039 tensor(50714.2695, grad_fn=<MseLossBackward0>)\n",
      "2040 tensor(50691.6992, grad_fn=<MseLossBackward0>)\n",
      "2041 tensor(50669.1250, grad_fn=<MseLossBackward0>)\n",
      "2042 tensor(50646.5586, grad_fn=<MseLossBackward0>)\n",
      "2043 tensor(50623.9961, grad_fn=<MseLossBackward0>)\n",
      "2044 tensor(50601.4336, grad_fn=<MseLossBackward0>)\n",
      "2045 tensor(50578.8750, grad_fn=<MseLossBackward0>)\n",
      "2046 tensor(50556.3203, grad_fn=<MseLossBackward0>)\n",
      "2047 tensor(50533.7656, grad_fn=<MseLossBackward0>)\n",
      "2048 tensor(50511.2188, grad_fn=<MseLossBackward0>)\n",
      "2049 tensor(50488.6719, grad_fn=<MseLossBackward0>)\n",
      "2050 tensor(50466.1289, grad_fn=<MseLossBackward0>)\n",
      "2051 tensor(50443.5898, grad_fn=<MseLossBackward0>)\n",
      "2052 tensor(50421.0508, grad_fn=<MseLossBackward0>)\n",
      "2053 tensor(50398.5234, grad_fn=<MseLossBackward0>)\n",
      "2054 tensor(50375.9883, grad_fn=<MseLossBackward0>)\n",
      "2055 tensor(50353.4570, grad_fn=<MseLossBackward0>)\n",
      "2056 tensor(50330.9297, grad_fn=<MseLossBackward0>)\n",
      "2057 tensor(50308.4141, grad_fn=<MseLossBackward0>)\n",
      "2058 tensor(50285.8984, grad_fn=<MseLossBackward0>)\n",
      "2059 tensor(50263.3828, grad_fn=<MseLossBackward0>)\n",
      "2060 tensor(50240.8672, grad_fn=<MseLossBackward0>)\n",
      "2061 tensor(50218.3594, grad_fn=<MseLossBackward0>)\n",
      "2062 tensor(50195.8555, grad_fn=<MseLossBackward0>)\n",
      "2063 tensor(50173.3555, grad_fn=<MseLossBackward0>)\n",
      "2064 tensor(50150.8516, grad_fn=<MseLossBackward0>)\n",
      "2065 tensor(50128.3555, grad_fn=<MseLossBackward0>)\n",
      "2066 tensor(50105.8633, grad_fn=<MseLossBackward0>)\n",
      "2067 tensor(50083.3750, grad_fn=<MseLossBackward0>)\n",
      "2068 tensor(50060.8906, grad_fn=<MseLossBackward0>)\n",
      "2069 tensor(50038.4062, grad_fn=<MseLossBackward0>)\n",
      "2070 tensor(50015.9219, grad_fn=<MseLossBackward0>)\n",
      "2071 tensor(49993.4492, grad_fn=<MseLossBackward0>)\n",
      "2072 tensor(49970.9766, grad_fn=<MseLossBackward0>)\n",
      "2073 tensor(49948.5078, grad_fn=<MseLossBackward0>)\n",
      "2074 tensor(49926.0352, grad_fn=<MseLossBackward0>)\n",
      "2075 tensor(49903.5742, grad_fn=<MseLossBackward0>)\n",
      "2076 tensor(49881.1094, grad_fn=<MseLossBackward0>)\n",
      "2077 tensor(49858.6602, grad_fn=<MseLossBackward0>)\n",
      "2078 tensor(49836.2031, grad_fn=<MseLossBackward0>)\n",
      "2079 tensor(49813.7500, grad_fn=<MseLossBackward0>)\n",
      "2080 tensor(49791.3047, grad_fn=<MseLossBackward0>)\n",
      "2081 tensor(49768.8633, grad_fn=<MseLossBackward0>)\n",
      "2082 tensor(49746.4219, grad_fn=<MseLossBackward0>)\n",
      "2083 tensor(49723.9844, grad_fn=<MseLossBackward0>)\n",
      "2084 tensor(49701.5586, grad_fn=<MseLossBackward0>)\n",
      "2085 tensor(49679.1211, grad_fn=<MseLossBackward0>)\n",
      "2086 tensor(49656.6992, grad_fn=<MseLossBackward0>)\n",
      "2087 tensor(49634.2695, grad_fn=<MseLossBackward0>)\n",
      "2088 tensor(49611.8555, grad_fn=<MseLossBackward0>)\n",
      "2089 tensor(49589.4414, grad_fn=<MseLossBackward0>)\n",
      "2090 tensor(49567.0312, grad_fn=<MseLossBackward0>)\n",
      "2091 tensor(49544.6172, grad_fn=<MseLossBackward0>)\n",
      "2092 tensor(49522.2109, grad_fn=<MseLossBackward0>)\n",
      "2093 tensor(49499.8086, grad_fn=<MseLossBackward0>)\n",
      "2094 tensor(49477.4102, grad_fn=<MseLossBackward0>)\n",
      "2095 tensor(49455.0156, grad_fn=<MseLossBackward0>)\n",
      "2096 tensor(49432.6250, grad_fn=<MseLossBackward0>)\n",
      "2097 tensor(49410.2344, grad_fn=<MseLossBackward0>)\n",
      "2098 tensor(49387.8516, grad_fn=<MseLossBackward0>)\n",
      "2099 tensor(49365.4727, grad_fn=<MseLossBackward0>)\n",
      "2100 tensor(49343.0977, grad_fn=<MseLossBackward0>)\n",
      "2101 tensor(49320.7188, grad_fn=<MseLossBackward0>)\n",
      "2102 tensor(49298.3477, grad_fn=<MseLossBackward0>)\n",
      "2103 tensor(49275.9844, grad_fn=<MseLossBackward0>)\n",
      "2104 tensor(49253.6250, grad_fn=<MseLossBackward0>)\n",
      "2105 tensor(49231.2617, grad_fn=<MseLossBackward0>)\n",
      "2106 tensor(49208.9062, grad_fn=<MseLossBackward0>)\n",
      "2107 tensor(49186.5508, grad_fn=<MseLossBackward0>)\n",
      "2108 tensor(49164.2109, grad_fn=<MseLossBackward0>)\n",
      "2109 tensor(49141.8633, grad_fn=<MseLossBackward0>)\n",
      "2110 tensor(49119.5234, grad_fn=<MseLossBackward0>)\n",
      "2111 tensor(49097.1836, grad_fn=<MseLossBackward0>)\n",
      "2112 tensor(49074.8516, grad_fn=<MseLossBackward0>)\n",
      "2113 tensor(49052.5195, grad_fn=<MseLossBackward0>)\n",
      "2114 tensor(49030.1953, grad_fn=<MseLossBackward0>)\n",
      "2115 tensor(49007.8711, grad_fn=<MseLossBackward0>)\n",
      "2116 tensor(48985.5547, grad_fn=<MseLossBackward0>)\n",
      "2117 tensor(48963.2344, grad_fn=<MseLossBackward0>)\n",
      "2118 tensor(48940.9258, grad_fn=<MseLossBackward0>)\n",
      "2119 tensor(48918.6172, grad_fn=<MseLossBackward0>)\n",
      "2120 tensor(48896.3203, grad_fn=<MseLossBackward0>)\n",
      "2121 tensor(48874.0117, grad_fn=<MseLossBackward0>)\n",
      "2122 tensor(48851.7188, grad_fn=<MseLossBackward0>)\n",
      "2123 tensor(48829.4258, grad_fn=<MseLossBackward0>)\n",
      "2124 tensor(48807.1406, grad_fn=<MseLossBackward0>)\n",
      "2125 tensor(48784.8555, grad_fn=<MseLossBackward0>)\n",
      "2126 tensor(48762.5742, grad_fn=<MseLossBackward0>)\n",
      "2127 tensor(48740.3008, grad_fn=<MseLossBackward0>)\n",
      "2128 tensor(48718.0273, grad_fn=<MseLossBackward0>)\n",
      "2129 tensor(48695.7539, grad_fn=<MseLossBackward0>)\n",
      "2130 tensor(48673.4883, grad_fn=<MseLossBackward0>)\n",
      "2131 tensor(48651.2266, grad_fn=<MseLossBackward0>)\n",
      "2132 tensor(48628.9727, grad_fn=<MseLossBackward0>)\n",
      "2133 tensor(48606.7188, grad_fn=<MseLossBackward0>)\n",
      "2134 tensor(48584.4688, grad_fn=<MseLossBackward0>)\n",
      "2135 tensor(48562.2266, grad_fn=<MseLossBackward0>)\n",
      "2136 tensor(48539.9766, grad_fn=<MseLossBackward0>)\n",
      "2137 tensor(48517.7461, grad_fn=<MseLossBackward0>)\n",
      "2138 tensor(48495.5078, grad_fn=<MseLossBackward0>)\n",
      "2139 tensor(48473.2773, grad_fn=<MseLossBackward0>)\n",
      "2140 tensor(48451.0586, grad_fn=<MseLossBackward0>)\n",
      "2141 tensor(48428.8359, grad_fn=<MseLossBackward0>)\n",
      "2142 tensor(48406.6133, grad_fn=<MseLossBackward0>)\n",
      "2143 tensor(48384.3984, grad_fn=<MseLossBackward0>)\n",
      "2144 tensor(48362.1914, grad_fn=<MseLossBackward0>)\n",
      "2145 tensor(48339.9844, grad_fn=<MseLossBackward0>)\n",
      "2146 tensor(48317.7852, grad_fn=<MseLossBackward0>)\n",
      "2147 tensor(48295.5859, grad_fn=<MseLossBackward0>)\n",
      "2148 tensor(48273.3945, grad_fn=<MseLossBackward0>)\n",
      "2149 tensor(48251.2070, grad_fn=<MseLossBackward0>)\n",
      "2150 tensor(48229.0195, grad_fn=<MseLossBackward0>)\n",
      "2151 tensor(48206.8398, grad_fn=<MseLossBackward0>)\n",
      "2152 tensor(48184.6562, grad_fn=<MseLossBackward0>)\n",
      "2153 tensor(48162.4883, grad_fn=<MseLossBackward0>)\n",
      "2154 tensor(48140.3203, grad_fn=<MseLossBackward0>)\n",
      "2155 tensor(48118.1562, grad_fn=<MseLossBackward0>)\n",
      "2156 tensor(48096., grad_fn=<MseLossBackward0>)\n",
      "2157 tensor(48073.8359, grad_fn=<MseLossBackward0>)\n",
      "2158 tensor(48051.6836, grad_fn=<MseLossBackward0>)\n",
      "2159 tensor(48029.5391, grad_fn=<MseLossBackward0>)\n",
      "2160 tensor(48007.3945, grad_fn=<MseLossBackward0>)\n",
      "2161 tensor(47985.2539, grad_fn=<MseLossBackward0>)\n",
      "2162 tensor(47963.1172, grad_fn=<MseLossBackward0>)\n",
      "2163 tensor(47940.9883, grad_fn=<MseLossBackward0>)\n",
      "2164 tensor(47918.8633, grad_fn=<MseLossBackward0>)\n",
      "2165 tensor(47896.7383, grad_fn=<MseLossBackward0>)\n",
      "2166 tensor(47874.6211, grad_fn=<MseLossBackward0>)\n",
      "2167 tensor(47852.5078, grad_fn=<MseLossBackward0>)\n",
      "2168 tensor(47830.3945, grad_fn=<MseLossBackward0>)\n",
      "2169 tensor(47808.2969, grad_fn=<MseLossBackward0>)\n",
      "2170 tensor(47786.1914, grad_fn=<MseLossBackward0>)\n",
      "2171 tensor(47764.0938, grad_fn=<MseLossBackward0>)\n",
      "2172 tensor(47742.0039, grad_fn=<MseLossBackward0>)\n",
      "2173 tensor(47719.9141, grad_fn=<MseLossBackward0>)\n",
      "2174 tensor(47697.8281, grad_fn=<MseLossBackward0>)\n",
      "2175 tensor(47675.7500, grad_fn=<MseLossBackward0>)\n",
      "2176 tensor(47653.6719, grad_fn=<MseLossBackward0>)\n",
      "2177 tensor(47631.6055, grad_fn=<MseLossBackward0>)\n",
      "2178 tensor(47609.5391, grad_fn=<MseLossBackward0>)\n",
      "2179 tensor(47587.4727, grad_fn=<MseLossBackward0>)\n",
      "2180 tensor(47565.4180, grad_fn=<MseLossBackward0>)\n",
      "2181 tensor(47543.3711, grad_fn=<MseLossBackward0>)\n",
      "2182 tensor(47521.3203, grad_fn=<MseLossBackward0>)\n",
      "2183 tensor(47499.2734, grad_fn=<MseLossBackward0>)\n",
      "2184 tensor(47477.2305, grad_fn=<MseLossBackward0>)\n",
      "2185 tensor(47455.1953, grad_fn=<MseLossBackward0>)\n",
      "2186 tensor(47433.1680, grad_fn=<MseLossBackward0>)\n",
      "2187 tensor(47411.1406, grad_fn=<MseLossBackward0>)\n",
      "2188 tensor(47389.1211, grad_fn=<MseLossBackward0>)\n",
      "2189 tensor(47367.1016, grad_fn=<MseLossBackward0>)\n",
      "2190 tensor(47345.0898, grad_fn=<MseLossBackward0>)\n",
      "2191 tensor(47323.0781, grad_fn=<MseLossBackward0>)\n",
      "2192 tensor(47301.0781, grad_fn=<MseLossBackward0>)\n",
      "2193 tensor(47279.0781, grad_fn=<MseLossBackward0>)\n",
      "2194 tensor(47257.0820, grad_fn=<MseLossBackward0>)\n",
      "2195 tensor(47235.0938, grad_fn=<MseLossBackward0>)\n",
      "2196 tensor(47213.1094, grad_fn=<MseLossBackward0>)\n",
      "2197 tensor(47191.1211, grad_fn=<MseLossBackward0>)\n",
      "2198 tensor(47169.1523, grad_fn=<MseLossBackward0>)\n",
      "2199 tensor(47147.1797, grad_fn=<MseLossBackward0>)\n",
      "2200 tensor(47125.2109, grad_fn=<MseLossBackward0>)\n",
      "2201 tensor(47103.2500, grad_fn=<MseLossBackward0>)\n",
      "2202 tensor(47081.2891, grad_fn=<MseLossBackward0>)\n",
      "2203 tensor(47059.3320, grad_fn=<MseLossBackward0>)\n",
      "2204 tensor(47037.3906, grad_fn=<MseLossBackward0>)\n",
      "2205 tensor(47015.4453, grad_fn=<MseLossBackward0>)\n",
      "2206 tensor(46993.5000, grad_fn=<MseLossBackward0>)\n",
      "2207 tensor(46971.5703, grad_fn=<MseLossBackward0>)\n",
      "2208 tensor(46949.6445, grad_fn=<MseLossBackward0>)\n",
      "2209 tensor(46927.7148, grad_fn=<MseLossBackward0>)\n",
      "2210 tensor(46905.7969, grad_fn=<MseLossBackward0>)\n",
      "2211 tensor(46883.8789, grad_fn=<MseLossBackward0>)\n",
      "2212 tensor(46861.9648, grad_fn=<MseLossBackward0>)\n",
      "2213 tensor(46840.0586, grad_fn=<MseLossBackward0>)\n",
      "2214 tensor(46818.1602, grad_fn=<MseLossBackward0>)\n",
      "2215 tensor(46796.2617, grad_fn=<MseLossBackward0>)\n",
      "2216 tensor(46774.3711, grad_fn=<MseLossBackward0>)\n",
      "2217 tensor(46752.4844, grad_fn=<MseLossBackward0>)\n",
      "2218 tensor(46730.5977, grad_fn=<MseLossBackward0>)\n",
      "2219 tensor(46708.7227, grad_fn=<MseLossBackward0>)\n",
      "2220 tensor(46686.8477, grad_fn=<MseLossBackward0>)\n",
      "2221 tensor(46664.9805, grad_fn=<MseLossBackward0>)\n",
      "2222 tensor(46643.1172, grad_fn=<MseLossBackward0>)\n",
      "2223 tensor(46621.2578, grad_fn=<MseLossBackward0>)\n",
      "2224 tensor(46599.4023, grad_fn=<MseLossBackward0>)\n",
      "2225 tensor(46577.5508, grad_fn=<MseLossBackward0>)\n",
      "2226 tensor(46555.7109, grad_fn=<MseLossBackward0>)\n",
      "2227 tensor(46533.8672, grad_fn=<MseLossBackward0>)\n",
      "2228 tensor(46512.0352, grad_fn=<MseLossBackward0>)\n",
      "2229 tensor(46490.2070, grad_fn=<MseLossBackward0>)\n",
      "2230 tensor(46468.3867, grad_fn=<MseLossBackward0>)\n",
      "2231 tensor(46446.5664, grad_fn=<MseLossBackward0>)\n",
      "2232 tensor(46424.7461, grad_fn=<MseLossBackward0>)\n",
      "2233 tensor(46402.9375, grad_fn=<MseLossBackward0>)\n",
      "2234 tensor(46381.1367, grad_fn=<MseLossBackward0>)\n",
      "2235 tensor(46359.3359, grad_fn=<MseLossBackward0>)\n",
      "2236 tensor(46337.5391, grad_fn=<MseLossBackward0>)\n",
      "2237 tensor(46315.7500, grad_fn=<MseLossBackward0>)\n",
      "2238 tensor(46293.9648, grad_fn=<MseLossBackward0>)\n",
      "2239 tensor(46272.1836, grad_fn=<MseLossBackward0>)\n",
      "2240 tensor(46250.4102, grad_fn=<MseLossBackward0>)\n",
      "2241 tensor(46228.6406, grad_fn=<MseLossBackward0>)\n",
      "2242 tensor(46206.8828, grad_fn=<MseLossBackward0>)\n",
      "2243 tensor(46185.1172, grad_fn=<MseLossBackward0>)\n",
      "2244 tensor(46163.3633, grad_fn=<MseLossBackward0>)\n",
      "2245 tensor(46141.6133, grad_fn=<MseLossBackward0>)\n",
      "2246 tensor(46119.8672, grad_fn=<MseLossBackward0>)\n",
      "2247 tensor(46098.1367, grad_fn=<MseLossBackward0>)\n",
      "2248 tensor(46076.3945, grad_fn=<MseLossBackward0>)\n",
      "2249 tensor(46054.6641, grad_fn=<MseLossBackward0>)\n",
      "2250 tensor(46032.9492, grad_fn=<MseLossBackward0>)\n",
      "2251 tensor(46011.2227, grad_fn=<MseLossBackward0>)\n",
      "2252 tensor(45989.5117, grad_fn=<MseLossBackward0>)\n",
      "2253 tensor(45967.8047, grad_fn=<MseLossBackward0>)\n",
      "2254 tensor(45946.0938, grad_fn=<MseLossBackward0>)\n",
      "2255 tensor(45924.3984, grad_fn=<MseLossBackward0>)\n",
      "2256 tensor(45902.7070, grad_fn=<MseLossBackward0>)\n",
      "2257 tensor(45881.0195, grad_fn=<MseLossBackward0>)\n",
      "2258 tensor(45859.3359, grad_fn=<MseLossBackward0>)\n",
      "2259 tensor(45837.6602, grad_fn=<MseLossBackward0>)\n",
      "2260 tensor(45815.9844, grad_fn=<MseLossBackward0>)\n",
      "2261 tensor(45794.3164, grad_fn=<MseLossBackward0>)\n",
      "2262 tensor(45772.6523, grad_fn=<MseLossBackward0>)\n",
      "2263 tensor(45750.9961, grad_fn=<MseLossBackward0>)\n",
      "2264 tensor(45729.3477, grad_fn=<MseLossBackward0>)\n",
      "2265 tensor(45707.6992, grad_fn=<MseLossBackward0>)\n",
      "2266 tensor(45686.0586, grad_fn=<MseLossBackward0>)\n",
      "2267 tensor(45664.4219, grad_fn=<MseLossBackward0>)\n",
      "2268 tensor(45642.7930, grad_fn=<MseLossBackward0>)\n",
      "2269 tensor(45621.1680, grad_fn=<MseLossBackward0>)\n",
      "2270 tensor(45599.5469, grad_fn=<MseLossBackward0>)\n",
      "2271 tensor(45577.9297, grad_fn=<MseLossBackward0>)\n",
      "2272 tensor(45556.3242, grad_fn=<MseLossBackward0>)\n",
      "2273 tensor(45534.7188, grad_fn=<MseLossBackward0>)\n",
      "2274 tensor(45513.1250, grad_fn=<MseLossBackward0>)\n",
      "2275 tensor(45491.5312, grad_fn=<MseLossBackward0>)\n",
      "2276 tensor(45469.9414, grad_fn=<MseLossBackward0>)\n",
      "2277 tensor(45448.3633, grad_fn=<MseLossBackward0>)\n",
      "2278 tensor(45426.7812, grad_fn=<MseLossBackward0>)\n",
      "2279 tensor(45405.2109, grad_fn=<MseLossBackward0>)\n",
      "2280 tensor(45383.6406, grad_fn=<MseLossBackward0>)\n",
      "2281 tensor(45362.0820, grad_fn=<MseLossBackward0>)\n",
      "2282 tensor(45340.5273, grad_fn=<MseLossBackward0>)\n",
      "2283 tensor(45318.9805, grad_fn=<MseLossBackward0>)\n",
      "2284 tensor(45297.4297, grad_fn=<MseLossBackward0>)\n",
      "2285 tensor(45275.8945, grad_fn=<MseLossBackward0>)\n",
      "2286 tensor(45254.3633, grad_fn=<MseLossBackward0>)\n",
      "2287 tensor(45232.8320, grad_fn=<MseLossBackward0>)\n",
      "2288 tensor(45211.3086, grad_fn=<MseLossBackward0>)\n",
      "2289 tensor(45189.7930, grad_fn=<MseLossBackward0>)\n",
      "2290 tensor(45168.2773, grad_fn=<MseLossBackward0>)\n",
      "2291 tensor(45146.7734, grad_fn=<MseLossBackward0>)\n",
      "2292 tensor(45125.2734, grad_fn=<MseLossBackward0>)\n",
      "2293 tensor(45103.7812, grad_fn=<MseLossBackward0>)\n",
      "2294 tensor(45082.2891, grad_fn=<MseLossBackward0>)\n",
      "2295 tensor(45060.8086, grad_fn=<MseLossBackward0>)\n",
      "2296 tensor(45039.3242, grad_fn=<MseLossBackward0>)\n",
      "2297 tensor(45017.8516, grad_fn=<MseLossBackward0>)\n",
      "2298 tensor(44996.3867, grad_fn=<MseLossBackward0>)\n",
      "2299 tensor(44974.9258, grad_fn=<MseLossBackward0>)\n",
      "2300 tensor(44953.4727, grad_fn=<MseLossBackward0>)\n",
      "2301 tensor(44932.0195, grad_fn=<MseLossBackward0>)\n",
      "2302 tensor(44910.5703, grad_fn=<MseLossBackward0>)\n",
      "2303 tensor(44889.1328, grad_fn=<MseLossBackward0>)\n",
      "2304 tensor(44867.6992, grad_fn=<MseLossBackward0>)\n",
      "2305 tensor(44846.2695, grad_fn=<MseLossBackward0>)\n",
      "2306 tensor(44824.8516, grad_fn=<MseLossBackward0>)\n",
      "2307 tensor(44803.4336, grad_fn=<MseLossBackward0>)\n",
      "2308 tensor(44782.0195, grad_fn=<MseLossBackward0>)\n",
      "2309 tensor(44760.6172, grad_fn=<MseLossBackward0>)\n",
      "2310 tensor(44739.2148, grad_fn=<MseLossBackward0>)\n",
      "2311 tensor(44717.8203, grad_fn=<MseLossBackward0>)\n",
      "2312 tensor(44696.4297, grad_fn=<MseLossBackward0>)\n",
      "2313 tensor(44675.0508, grad_fn=<MseLossBackward0>)\n",
      "2314 tensor(44653.6758, grad_fn=<MseLossBackward0>)\n",
      "2315 tensor(44632.3047, grad_fn=<MseLossBackward0>)\n",
      "2316 tensor(44610.9375, grad_fn=<MseLossBackward0>)\n",
      "2317 tensor(44589.5781, grad_fn=<MseLossBackward0>)\n",
      "2318 tensor(44568.2266, grad_fn=<MseLossBackward0>)\n",
      "2319 tensor(44546.8789, grad_fn=<MseLossBackward0>)\n",
      "2320 tensor(44525.5352, grad_fn=<MseLossBackward0>)\n",
      "2321 tensor(44504.1953, grad_fn=<MseLossBackward0>)\n",
      "2322 tensor(44482.8672, grad_fn=<MseLossBackward0>)\n",
      "2323 tensor(44461.5430, grad_fn=<MseLossBackward0>)\n",
      "2324 tensor(44440.2266, grad_fn=<MseLossBackward0>)\n",
      "2325 tensor(44418.9102, grad_fn=<MseLossBackward0>)\n",
      "2326 tensor(44397.6016, grad_fn=<MseLossBackward0>)\n",
      "2327 tensor(44376.3047, grad_fn=<MseLossBackward0>)\n",
      "2328 tensor(44355.0078, grad_fn=<MseLossBackward0>)\n",
      "2329 tensor(44333.7188, grad_fn=<MseLossBackward0>)\n",
      "2330 tensor(44312.4375, grad_fn=<MseLossBackward0>)\n",
      "2331 tensor(44291.1562, grad_fn=<MseLossBackward0>)\n",
      "2332 tensor(44269.8867, grad_fn=<MseLossBackward0>)\n",
      "2333 tensor(44248.6172, grad_fn=<MseLossBackward0>)\n",
      "2334 tensor(44227.3555, grad_fn=<MseLossBackward0>)\n",
      "2335 tensor(44206.1016, grad_fn=<MseLossBackward0>)\n",
      "2336 tensor(44184.8555, grad_fn=<MseLossBackward0>)\n",
      "2337 tensor(44163.6094, grad_fn=<MseLossBackward0>)\n",
      "2338 tensor(44142.3750, grad_fn=<MseLossBackward0>)\n",
      "2339 tensor(44121.1445, grad_fn=<MseLossBackward0>)\n",
      "2340 tensor(44099.9180, grad_fn=<MseLossBackward0>)\n",
      "2341 tensor(44078.7070, grad_fn=<MseLossBackward0>)\n",
      "2342 tensor(44057.4922, grad_fn=<MseLossBackward0>)\n",
      "2343 tensor(44036.2812, grad_fn=<MseLossBackward0>)\n",
      "2344 tensor(44015.0820, grad_fn=<MseLossBackward0>)\n",
      "2345 tensor(43993.8906, grad_fn=<MseLossBackward0>)\n",
      "2346 tensor(43972.7031, grad_fn=<MseLossBackward0>)\n",
      "2347 tensor(43951.5195, grad_fn=<MseLossBackward0>)\n",
      "2348 tensor(43930.3359, grad_fn=<MseLossBackward0>)\n",
      "2349 tensor(43909.1719, grad_fn=<MseLossBackward0>)\n",
      "2350 tensor(43888.0039, grad_fn=<MseLossBackward0>)\n",
      "2351 tensor(43866.8477, grad_fn=<MseLossBackward0>)\n",
      "2352 tensor(43845.6953, grad_fn=<MseLossBackward0>)\n",
      "2353 tensor(43824.5508, grad_fn=<MseLossBackward0>)\n",
      "2354 tensor(43803.4102, grad_fn=<MseLossBackward0>)\n",
      "2355 tensor(43782.2812, grad_fn=<MseLossBackward0>)\n",
      "2356 tensor(43761.1523, grad_fn=<MseLossBackward0>)\n",
      "2357 tensor(43740.0273, grad_fn=<MseLossBackward0>)\n",
      "2358 tensor(43718.9141, grad_fn=<MseLossBackward0>)\n",
      "2359 tensor(43697.8008, grad_fn=<MseLossBackward0>)\n",
      "2360 tensor(43676.6992, grad_fn=<MseLossBackward0>)\n",
      "2361 tensor(43655.6016, grad_fn=<MseLossBackward0>)\n",
      "2362 tensor(43634.5156, grad_fn=<MseLossBackward0>)\n",
      "2363 tensor(43613.4297, grad_fn=<MseLossBackward0>)\n",
      "2364 tensor(43592.3477, grad_fn=<MseLossBackward0>)\n",
      "2365 tensor(43571.2734, grad_fn=<MseLossBackward0>)\n",
      "2366 tensor(43550.2109, grad_fn=<MseLossBackward0>)\n",
      "2367 tensor(43529.1523, grad_fn=<MseLossBackward0>)\n",
      "2368 tensor(43508.0977, grad_fn=<MseLossBackward0>)\n",
      "2369 tensor(43487.0508, grad_fn=<MseLossBackward0>)\n",
      "2370 tensor(43466.0117, grad_fn=<MseLossBackward0>)\n",
      "2371 tensor(43444.9766, grad_fn=<MseLossBackward0>)\n",
      "2372 tensor(43423.9531, grad_fn=<MseLossBackward0>)\n",
      "2373 tensor(43402.9258, grad_fn=<MseLossBackward0>)\n",
      "2374 tensor(43381.9102, grad_fn=<MseLossBackward0>)\n",
      "2375 tensor(43360.9023, grad_fn=<MseLossBackward0>)\n",
      "2376 tensor(43339.8984, grad_fn=<MseLossBackward0>)\n",
      "2377 tensor(43318.9023, grad_fn=<MseLossBackward0>)\n",
      "2378 tensor(43297.9102, grad_fn=<MseLossBackward0>)\n",
      "2379 tensor(43276.9297, grad_fn=<MseLossBackward0>)\n",
      "2380 tensor(43255.9492, grad_fn=<MseLossBackward0>)\n",
      "2381 tensor(43234.9766, grad_fn=<MseLossBackward0>)\n",
      "2382 tensor(43214.0117, grad_fn=<MseLossBackward0>)\n",
      "2383 tensor(43193.0547, grad_fn=<MseLossBackward0>)\n",
      "2384 tensor(43172.1016, grad_fn=<MseLossBackward0>)\n",
      "2385 tensor(43151.1562, grad_fn=<MseLossBackward0>)\n",
      "2386 tensor(43130.2148, grad_fn=<MseLossBackward0>)\n",
      "2387 tensor(43109.2812, grad_fn=<MseLossBackward0>)\n",
      "2388 tensor(43088.3555, grad_fn=<MseLossBackward0>)\n",
      "2389 tensor(43067.4336, grad_fn=<MseLossBackward0>)\n",
      "2390 tensor(43046.5234, grad_fn=<MseLossBackward0>)\n",
      "2391 tensor(43025.6172, grad_fn=<MseLossBackward0>)\n",
      "2392 tensor(43004.7109, grad_fn=<MseLossBackward0>)\n",
      "2393 tensor(42983.8203, grad_fn=<MseLossBackward0>)\n",
      "2394 tensor(42962.9297, grad_fn=<MseLossBackward0>)\n",
      "2395 tensor(42942.0469, grad_fn=<MseLossBackward0>)\n",
      "2396 tensor(42921.1758, grad_fn=<MseLossBackward0>)\n",
      "2397 tensor(42900.3047, grad_fn=<MseLossBackward0>)\n",
      "2398 tensor(42879.4453, grad_fn=<MseLossBackward0>)\n",
      "2399 tensor(42858.5898, grad_fn=<MseLossBackward0>)\n",
      "2400 tensor(42837.7383, grad_fn=<MseLossBackward0>)\n",
      "2401 tensor(42816.8984, grad_fn=<MseLossBackward0>)\n",
      "2402 tensor(42796.0625, grad_fn=<MseLossBackward0>)\n",
      "2403 tensor(42775.2344, grad_fn=<MseLossBackward0>)\n",
      "2404 tensor(42754.4102, grad_fn=<MseLossBackward0>)\n",
      "2405 tensor(42733.5938, grad_fn=<MseLossBackward0>)\n",
      "2406 tensor(42712.7852, grad_fn=<MseLossBackward0>)\n",
      "2407 tensor(42691.9805, grad_fn=<MseLossBackward0>)\n",
      "2408 tensor(42671.1875, grad_fn=<MseLossBackward0>)\n",
      "2409 tensor(42650.3945, grad_fn=<MseLossBackward0>)\n",
      "2410 tensor(42629.6094, grad_fn=<MseLossBackward0>)\n",
      "2411 tensor(42608.8320, grad_fn=<MseLossBackward0>)\n",
      "2412 tensor(42588.0664, grad_fn=<MseLossBackward0>)\n",
      "2413 tensor(42567.2969, grad_fn=<MseLossBackward0>)\n",
      "2414 tensor(42546.5469, grad_fn=<MseLossBackward0>)\n",
      "2415 tensor(42525.7930, grad_fn=<MseLossBackward0>)\n",
      "2416 tensor(42505.0508, grad_fn=<MseLossBackward0>)\n",
      "2417 tensor(42484.3164, grad_fn=<MseLossBackward0>)\n",
      "2418 tensor(42463.5859, grad_fn=<MseLossBackward0>)\n",
      "2419 tensor(42442.8633, grad_fn=<MseLossBackward0>)\n",
      "2420 tensor(42422.1445, grad_fn=<MseLossBackward0>)\n",
      "2421 tensor(42401.4375, grad_fn=<MseLossBackward0>)\n",
      "2422 tensor(42380.7305, grad_fn=<MseLossBackward0>)\n",
      "2423 tensor(42360.0352, grad_fn=<MseLossBackward0>)\n",
      "2424 tensor(42339.3477, grad_fn=<MseLossBackward0>)\n",
      "2425 tensor(42318.6641, grad_fn=<MseLossBackward0>)\n",
      "2426 tensor(42297.9883, grad_fn=<MseLossBackward0>)\n",
      "2427 tensor(42277.3203, grad_fn=<MseLossBackward0>)\n",
      "2428 tensor(42256.6562, grad_fn=<MseLossBackward0>)\n",
      "2429 tensor(42236.0039, grad_fn=<MseLossBackward0>)\n",
      "2430 tensor(42215.3516, grad_fn=<MseLossBackward0>)\n",
      "2431 tensor(42194.7070, grad_fn=<MseLossBackward0>)\n",
      "2432 tensor(42174.0742, grad_fn=<MseLossBackward0>)\n",
      "2433 tensor(42153.4453, grad_fn=<MseLossBackward0>)\n",
      "2434 tensor(42132.8242, grad_fn=<MseLossBackward0>)\n",
      "2435 tensor(42112.2109, grad_fn=<MseLossBackward0>)\n",
      "2436 tensor(42091.6016, grad_fn=<MseLossBackward0>)\n",
      "2437 tensor(42071., grad_fn=<MseLossBackward0>)\n",
      "2438 tensor(42050.4023, grad_fn=<MseLossBackward0>)\n",
      "2439 tensor(42029.8164, grad_fn=<MseLossBackward0>)\n",
      "2440 tensor(42009.2383, grad_fn=<MseLossBackward0>)\n",
      "2441 tensor(41988.6602, grad_fn=<MseLossBackward0>)\n",
      "2442 tensor(41968.0977, grad_fn=<MseLossBackward0>)\n",
      "2443 tensor(41947.5391, grad_fn=<MseLossBackward0>)\n",
      "2444 tensor(41926.9844, grad_fn=<MseLossBackward0>)\n",
      "2445 tensor(41906.4375, grad_fn=<MseLossBackward0>)\n",
      "2446 tensor(41885.9023, grad_fn=<MseLossBackward0>)\n",
      "2447 tensor(41865.3672, grad_fn=<MseLossBackward0>)\n",
      "2448 tensor(41844.8438, grad_fn=<MseLossBackward0>)\n",
      "2449 tensor(41824.3242, grad_fn=<MseLossBackward0>)\n",
      "2450 tensor(41803.8125, grad_fn=<MseLossBackward0>)\n",
      "2451 tensor(41783.3086, grad_fn=<MseLossBackward0>)\n",
      "2452 tensor(41762.8086, grad_fn=<MseLossBackward0>)\n",
      "2453 tensor(41742.3203, grad_fn=<MseLossBackward0>)\n",
      "2454 tensor(41721.8398, grad_fn=<MseLossBackward0>)\n",
      "2455 tensor(41701.3633, grad_fn=<MseLossBackward0>)\n",
      "2456 tensor(41680.8945, grad_fn=<MseLossBackward0>)\n",
      "2457 tensor(41660.4297, grad_fn=<MseLossBackward0>)\n",
      "2458 tensor(41639.9766, grad_fn=<MseLossBackward0>)\n",
      "2459 tensor(41619.5234, grad_fn=<MseLossBackward0>)\n",
      "2460 tensor(41599.0859, grad_fn=<MseLossBackward0>)\n",
      "2461 tensor(41578.6523, grad_fn=<MseLossBackward0>)\n",
      "2462 tensor(41558.2227, grad_fn=<MseLossBackward0>)\n",
      "2463 tensor(41537.8047, grad_fn=<MseLossBackward0>)\n",
      "2464 tensor(41517.3906, grad_fn=<MseLossBackward0>)\n",
      "2465 tensor(41496.9883, grad_fn=<MseLossBackward0>)\n",
      "2466 tensor(41476.5898, grad_fn=<MseLossBackward0>)\n",
      "2467 tensor(41456.1953, grad_fn=<MseLossBackward0>)\n",
      "2468 tensor(41435.8125, grad_fn=<MseLossBackward0>)\n",
      "2469 tensor(41415.4375, grad_fn=<MseLossBackward0>)\n",
      "2470 tensor(41395.0664, grad_fn=<MseLossBackward0>)\n",
      "2471 tensor(41374.7031, grad_fn=<MseLossBackward0>)\n",
      "2472 tensor(41354.3477, grad_fn=<MseLossBackward0>)\n",
      "2473 tensor(41333.9961, grad_fn=<MseLossBackward0>)\n",
      "2474 tensor(41313.6602, grad_fn=<MseLossBackward0>)\n",
      "2475 tensor(41293.3242, grad_fn=<MseLossBackward0>)\n",
      "2476 tensor(41273., grad_fn=<MseLossBackward0>)\n",
      "2477 tensor(41252.6758, grad_fn=<MseLossBackward0>)\n",
      "2478 tensor(41232.3672, grad_fn=<MseLossBackward0>)\n",
      "2479 tensor(41212.0586, grad_fn=<MseLossBackward0>)\n",
      "2480 tensor(41191.7617, grad_fn=<MseLossBackward0>)\n",
      "2481 tensor(41171.4688, grad_fn=<MseLossBackward0>)\n",
      "2482 tensor(41151.1875, grad_fn=<MseLossBackward0>)\n",
      "2483 tensor(41130.9102, grad_fn=<MseLossBackward0>)\n",
      "2484 tensor(41110.6367, grad_fn=<MseLossBackward0>)\n",
      "2485 tensor(41090.3750, grad_fn=<MseLossBackward0>)\n",
      "2486 tensor(41070.1250, grad_fn=<MseLossBackward0>)\n",
      "2487 tensor(41049.8789, grad_fn=<MseLossBackward0>)\n",
      "2488 tensor(41029.6406, grad_fn=<MseLossBackward0>)\n",
      "2489 tensor(41009.4023, grad_fn=<MseLossBackward0>)\n",
      "2490 tensor(40989.1797, grad_fn=<MseLossBackward0>)\n",
      "2491 tensor(40968.9648, grad_fn=<MseLossBackward0>)\n",
      "2492 tensor(40948.7539, grad_fn=<MseLossBackward0>)\n",
      "2493 tensor(40928.5469, grad_fn=<MseLossBackward0>)\n",
      "2494 tensor(40908.3516, grad_fn=<MseLossBackward0>)\n",
      "2495 tensor(40888.1641, grad_fn=<MseLossBackward0>)\n",
      "2496 tensor(40867.9805, grad_fn=<MseLossBackward0>)\n",
      "2497 tensor(40847.8086, grad_fn=<MseLossBackward0>)\n",
      "2498 tensor(40827.6445, grad_fn=<MseLossBackward0>)\n",
      "2499 tensor(40807.4805, grad_fn=<MseLossBackward0>)\n",
      "2500 tensor(40787.3281, grad_fn=<MseLossBackward0>)\n",
      "2501 tensor(40767.1836, grad_fn=<MseLossBackward0>)\n",
      "2502 tensor(40747.0469, grad_fn=<MseLossBackward0>)\n",
      "2503 tensor(40726.9180, grad_fn=<MseLossBackward0>)\n",
      "2504 tensor(40706.7930, grad_fn=<MseLossBackward0>)\n",
      "2505 tensor(40686.6836, grad_fn=<MseLossBackward0>)\n",
      "2506 tensor(40666.5703, grad_fn=<MseLossBackward0>)\n",
      "2507 tensor(40646.4727, grad_fn=<MseLossBackward0>)\n",
      "2508 tensor(40626.3828, grad_fn=<MseLossBackward0>)\n",
      "2509 tensor(40606.2969, grad_fn=<MseLossBackward0>)\n",
      "2510 tensor(40586.2188, grad_fn=<MseLossBackward0>)\n",
      "2511 tensor(40566.1523, grad_fn=<MseLossBackward0>)\n",
      "2512 tensor(40546.0898, grad_fn=<MseLossBackward0>)\n",
      "2513 tensor(40526.0312, grad_fn=<MseLossBackward0>)\n",
      "2514 tensor(40505.9844, grad_fn=<MseLossBackward0>)\n",
      "2515 tensor(40485.9453, grad_fn=<MseLossBackward0>)\n",
      "2516 tensor(40465.9102, grad_fn=<MseLossBackward0>)\n",
      "2517 tensor(40445.8906, grad_fn=<MseLossBackward0>)\n",
      "2518 tensor(40425.8711, grad_fn=<MseLossBackward0>)\n",
      "2519 tensor(40405.8633, grad_fn=<MseLossBackward0>)\n",
      "2520 tensor(40385.8594, grad_fn=<MseLossBackward0>)\n",
      "2521 tensor(40365.8594, grad_fn=<MseLossBackward0>)\n",
      "2522 tensor(40345.8789, grad_fn=<MseLossBackward0>)\n",
      "2523 tensor(40325.8945, grad_fn=<MseLossBackward0>)\n",
      "2524 tensor(40305.9258, grad_fn=<MseLossBackward0>)\n",
      "2525 tensor(40285.9609, grad_fn=<MseLossBackward0>)\n",
      "2526 tensor(40266.0039, grad_fn=<MseLossBackward0>)\n",
      "2527 tensor(40246.0547, grad_fn=<MseLossBackward0>)\n",
      "2528 tensor(40226.1133, grad_fn=<MseLossBackward0>)\n",
      "2529 tensor(40206.1797, grad_fn=<MseLossBackward0>)\n",
      "2530 tensor(40186.2539, grad_fn=<MseLossBackward0>)\n",
      "2531 tensor(40166.3359, grad_fn=<MseLossBackward0>)\n",
      "2532 tensor(40146.4219, grad_fn=<MseLossBackward0>)\n",
      "2533 tensor(40126.5234, grad_fn=<MseLossBackward0>)\n",
      "2534 tensor(40106.6250, grad_fn=<MseLossBackward0>)\n",
      "2535 tensor(40086.7383, grad_fn=<MseLossBackward0>)\n",
      "2536 tensor(40066.8555, grad_fn=<MseLossBackward0>)\n",
      "2537 tensor(40046.9883, grad_fn=<MseLossBackward0>)\n",
      "2538 tensor(40027.1172, grad_fn=<MseLossBackward0>)\n",
      "2539 tensor(40007.2656, grad_fn=<MseLossBackward0>)\n",
      "2540 tensor(39987.4141, grad_fn=<MseLossBackward0>)\n",
      "2541 tensor(39967.5742, grad_fn=<MseLossBackward0>)\n",
      "2542 tensor(39947.7383, grad_fn=<MseLossBackward0>)\n",
      "2543 tensor(39927.9102, grad_fn=<MseLossBackward0>)\n",
      "2544 tensor(39908.0938, grad_fn=<MseLossBackward0>)\n",
      "2545 tensor(39888.2852, grad_fn=<MseLossBackward0>)\n",
      "2546 tensor(39868.4805, grad_fn=<MseLossBackward0>)\n",
      "2547 tensor(39848.6836, grad_fn=<MseLossBackward0>)\n",
      "2548 tensor(39828.8984, grad_fn=<MseLossBackward0>)\n",
      "2549 tensor(39809.1172, grad_fn=<MseLossBackward0>)\n",
      "2550 tensor(39789.3438, grad_fn=<MseLossBackward0>)\n",
      "2551 tensor(39769.5859, grad_fn=<MseLossBackward0>)\n",
      "2552 tensor(39749.8242, grad_fn=<MseLossBackward0>)\n",
      "2553 tensor(39730.0781, grad_fn=<MseLossBackward0>)\n",
      "2554 tensor(39710.3359, grad_fn=<MseLossBackward0>)\n",
      "2555 tensor(39690.6055, grad_fn=<MseLossBackward0>)\n",
      "2556 tensor(39670.8789, grad_fn=<MseLossBackward0>)\n",
      "2557 tensor(39651.1602, grad_fn=<MseLossBackward0>)\n",
      "2558 tensor(39631.4531, grad_fn=<MseLossBackward0>)\n",
      "2559 tensor(39611.7539, grad_fn=<MseLossBackward0>)\n",
      "2560 tensor(39592.0586, grad_fn=<MseLossBackward0>)\n",
      "2561 tensor(39572.3750, grad_fn=<MseLossBackward0>)\n",
      "2562 tensor(39552.6953, grad_fn=<MseLossBackward0>)\n",
      "2563 tensor(39533.0273, grad_fn=<MseLossBackward0>)\n",
      "2564 tensor(39513.3633, grad_fn=<MseLossBackward0>)\n",
      "2565 tensor(39493.7109, grad_fn=<MseLossBackward0>)\n",
      "2566 tensor(39474.0664, grad_fn=<MseLossBackward0>)\n",
      "2567 tensor(39454.4258, grad_fn=<MseLossBackward0>)\n",
      "2568 tensor(39434.7969, grad_fn=<MseLossBackward0>)\n",
      "2569 tensor(39415.1797, grad_fn=<MseLossBackward0>)\n",
      "2570 tensor(39395.5625, grad_fn=<MseLossBackward0>)\n",
      "2571 tensor(39375.9570, grad_fn=<MseLossBackward0>)\n",
      "2572 tensor(39356.3594, grad_fn=<MseLossBackward0>)\n",
      "2573 tensor(39336.7695, grad_fn=<MseLossBackward0>)\n",
      "2574 tensor(39317.1875, grad_fn=<MseLossBackward0>)\n",
      "2575 tensor(39297.6133, grad_fn=<MseLossBackward0>)\n",
      "2576 tensor(39278.0469, grad_fn=<MseLossBackward0>)\n",
      "2577 tensor(39258.4844, grad_fn=<MseLossBackward0>)\n",
      "2578 tensor(39238.9375, grad_fn=<MseLossBackward0>)\n",
      "2579 tensor(39219.3945, grad_fn=<MseLossBackward0>)\n",
      "2580 tensor(39199.8594, grad_fn=<MseLossBackward0>)\n",
      "2581 tensor(39180.3359, grad_fn=<MseLossBackward0>)\n",
      "2582 tensor(39160.8164, grad_fn=<MseLossBackward0>)\n",
      "2583 tensor(39141.3047, grad_fn=<MseLossBackward0>)\n",
      "2584 tensor(39121.8008, grad_fn=<MseLossBackward0>)\n",
      "2585 tensor(39102.3125, grad_fn=<MseLossBackward0>)\n",
      "2586 tensor(39082.8242, grad_fn=<MseLossBackward0>)\n",
      "2587 tensor(39063.3477, grad_fn=<MseLossBackward0>)\n",
      "2588 tensor(39043.8750, grad_fn=<MseLossBackward0>)\n",
      "2589 tensor(39024.4141, grad_fn=<MseLossBackward0>)\n",
      "2590 tensor(39004.9609, grad_fn=<MseLossBackward0>)\n",
      "2591 tensor(38985.5156, grad_fn=<MseLossBackward0>)\n",
      "2592 tensor(38966.0742, grad_fn=<MseLossBackward0>)\n",
      "2593 tensor(38946.6523, grad_fn=<MseLossBackward0>)\n",
      "2594 tensor(38927.2305, grad_fn=<MseLossBackward0>)\n",
      "2595 tensor(38907.8164, grad_fn=<MseLossBackward0>)\n",
      "2596 tensor(38888.4141, grad_fn=<MseLossBackward0>)\n",
      "2597 tensor(38869.0156, grad_fn=<MseLossBackward0>)\n",
      "2598 tensor(38849.6289, grad_fn=<MseLossBackward0>)\n",
      "2599 tensor(38830.2500, grad_fn=<MseLossBackward0>)\n",
      "2600 tensor(38810.8750, grad_fn=<MseLossBackward0>)\n",
      "2601 tensor(38791.5117, grad_fn=<MseLossBackward0>)\n",
      "2602 tensor(38772.1562, grad_fn=<MseLossBackward0>)\n",
      "2603 tensor(38752.8125, grad_fn=<MseLossBackward0>)\n",
      "2604 tensor(38733.4727, grad_fn=<MseLossBackward0>)\n",
      "2605 tensor(38714.1406, grad_fn=<MseLossBackward0>)\n",
      "2606 tensor(38694.8164, grad_fn=<MseLossBackward0>)\n",
      "2607 tensor(38675.5000, grad_fn=<MseLossBackward0>)\n",
      "2608 tensor(38656.1953, grad_fn=<MseLossBackward0>)\n",
      "2609 tensor(38636.8945, grad_fn=<MseLossBackward0>)\n",
      "2610 tensor(38617.6055, grad_fn=<MseLossBackward0>)\n",
      "2611 tensor(38598.3281, grad_fn=<MseLossBackward0>)\n",
      "2612 tensor(38579.0547, grad_fn=<MseLossBackward0>)\n",
      "2613 tensor(38559.7852, grad_fn=<MseLossBackward0>)\n",
      "2614 tensor(38540.5312, grad_fn=<MseLossBackward0>)\n",
      "2615 tensor(38521.2812, grad_fn=<MseLossBackward0>)\n",
      "2616 tensor(38502.0391, grad_fn=<MseLossBackward0>)\n",
      "2617 tensor(38482.8125, grad_fn=<MseLossBackward0>)\n",
      "2618 tensor(38463.5898, grad_fn=<MseLossBackward0>)\n",
      "2619 tensor(38444.3750, grad_fn=<MseLossBackward0>)\n",
      "2620 tensor(38425.1641, grad_fn=<MseLossBackward0>)\n",
      "2621 tensor(38405.9688, grad_fn=<MseLossBackward0>)\n",
      "2622 tensor(38386.7773, grad_fn=<MseLossBackward0>)\n",
      "2623 tensor(38367.5938, grad_fn=<MseLossBackward0>)\n",
      "2624 tensor(38348.4219, grad_fn=<MseLossBackward0>)\n",
      "2625 tensor(38329.2578, grad_fn=<MseLossBackward0>)\n",
      "2626 tensor(38310.1016, grad_fn=<MseLossBackward0>)\n",
      "2627 tensor(38290.9531, grad_fn=<MseLossBackward0>)\n",
      "2628 tensor(38271.8086, grad_fn=<MseLossBackward0>)\n",
      "2629 tensor(38252.6836, grad_fn=<MseLossBackward0>)\n",
      "2630 tensor(38233.5547, grad_fn=<MseLossBackward0>)\n",
      "2631 tensor(38214.4414, grad_fn=<MseLossBackward0>)\n",
      "2632 tensor(38195.3359, grad_fn=<MseLossBackward0>)\n",
      "2633 tensor(38176.2383, grad_fn=<MseLossBackward0>)\n",
      "2634 tensor(38157.1484, grad_fn=<MseLossBackward0>)\n",
      "2635 tensor(38138.0664, grad_fn=<MseLossBackward0>)\n",
      "2636 tensor(38119., grad_fn=<MseLossBackward0>)\n",
      "2637 tensor(38099.9336, grad_fn=<MseLossBackward0>)\n",
      "2638 tensor(38080.8750, grad_fn=<MseLossBackward0>)\n",
      "2639 tensor(38061.8281, grad_fn=<MseLossBackward0>)\n",
      "2640 tensor(38042.7930, grad_fn=<MseLossBackward0>)\n",
      "2641 tensor(38023.7656, grad_fn=<MseLossBackward0>)\n",
      "2642 tensor(38004.7422, grad_fn=<MseLossBackward0>)\n",
      "2643 tensor(37985.7266, grad_fn=<MseLossBackward0>)\n",
      "2644 tensor(37966.7188, grad_fn=<MseLossBackward0>)\n",
      "2645 tensor(37947.7227, grad_fn=<MseLossBackward0>)\n",
      "2646 tensor(37928.7383, grad_fn=<MseLossBackward0>)\n",
      "2647 tensor(37909.7617, grad_fn=<MseLossBackward0>)\n",
      "2648 tensor(37890.7891, grad_fn=<MseLossBackward0>)\n",
      "2649 tensor(37871.8242, grad_fn=<MseLossBackward0>)\n",
      "2650 tensor(37852.8711, grad_fn=<MseLossBackward0>)\n",
      "2651 tensor(37833.9258, grad_fn=<MseLossBackward0>)\n",
      "2652 tensor(37814.9922, grad_fn=<MseLossBackward0>)\n",
      "2653 tensor(37796.0625, grad_fn=<MseLossBackward0>)\n",
      "2654 tensor(37777.1406, grad_fn=<MseLossBackward0>)\n",
      "2655 tensor(37758.2344, grad_fn=<MseLossBackward0>)\n",
      "2656 tensor(37739.3281, grad_fn=<MseLossBackward0>)\n",
      "2657 tensor(37720.4375, grad_fn=<MseLossBackward0>)\n",
      "2658 tensor(37701.5547, grad_fn=<MseLossBackward0>)\n",
      "2659 tensor(37682.6719, grad_fn=<MseLossBackward0>)\n",
      "2660 tensor(37663.8086, grad_fn=<MseLossBackward0>)\n",
      "2661 tensor(37644.9492, grad_fn=<MseLossBackward0>)\n",
      "2662 tensor(37626.0977, grad_fn=<MseLossBackward0>)\n",
      "2663 tensor(37607.2578, grad_fn=<MseLossBackward0>)\n",
      "2664 tensor(37588.4258, grad_fn=<MseLossBackward0>)\n",
      "2665 tensor(37569.5977, grad_fn=<MseLossBackward0>)\n",
      "2666 tensor(37550.7852, grad_fn=<MseLossBackward0>)\n",
      "2667 tensor(37531.9766, grad_fn=<MseLossBackward0>)\n",
      "2668 tensor(37513.1758, grad_fn=<MseLossBackward0>)\n",
      "2669 tensor(37494.3867, grad_fn=<MseLossBackward0>)\n",
      "2670 tensor(37475.6094, grad_fn=<MseLossBackward0>)\n",
      "2671 tensor(37456.8359, grad_fn=<MseLossBackward0>)\n",
      "2672 tensor(37438.0703, grad_fn=<MseLossBackward0>)\n",
      "2673 tensor(37419.3164, grad_fn=<MseLossBackward0>)\n",
      "2674 tensor(37400.5703, grad_fn=<MseLossBackward0>)\n",
      "2675 tensor(37381.8320, grad_fn=<MseLossBackward0>)\n",
      "2676 tensor(37363.1016, grad_fn=<MseLossBackward0>)\n",
      "2677 tensor(37344.3828, grad_fn=<MseLossBackward0>)\n",
      "2678 tensor(37325.6680, grad_fn=<MseLossBackward0>)\n",
      "2679 tensor(37306.9688, grad_fn=<MseLossBackward0>)\n",
      "2680 tensor(37288.2773, grad_fn=<MseLossBackward0>)\n",
      "2681 tensor(37269.5898, grad_fn=<MseLossBackward0>)\n",
      "2682 tensor(37250.9141, grad_fn=<MseLossBackward0>)\n",
      "2683 tensor(37232.2461, grad_fn=<MseLossBackward0>)\n",
      "2684 tensor(37213.5859, grad_fn=<MseLossBackward0>)\n",
      "2685 tensor(37194.9375, grad_fn=<MseLossBackward0>)\n",
      "2686 tensor(37176.2969, grad_fn=<MseLossBackward0>)\n",
      "2687 tensor(37157.6641, grad_fn=<MseLossBackward0>)\n",
      "2688 tensor(37139.0391, grad_fn=<MseLossBackward0>)\n",
      "2689 tensor(37120.4258, grad_fn=<MseLossBackward0>)\n",
      "2690 tensor(37101.8164, grad_fn=<MseLossBackward0>)\n",
      "2691 tensor(37083.2227, grad_fn=<MseLossBackward0>)\n",
      "2692 tensor(37064.6328, grad_fn=<MseLossBackward0>)\n",
      "2693 tensor(37046.0508, grad_fn=<MseLossBackward0>)\n",
      "2694 tensor(37027.4805, grad_fn=<MseLossBackward0>)\n",
      "2695 tensor(37008.9180, grad_fn=<MseLossBackward0>)\n",
      "2696 tensor(36990.3672, grad_fn=<MseLossBackward0>)\n",
      "2697 tensor(36971.8242, grad_fn=<MseLossBackward0>)\n",
      "2698 tensor(36953.2852, grad_fn=<MseLossBackward0>)\n",
      "2699 tensor(36934.7617, grad_fn=<MseLossBackward0>)\n",
      "2700 tensor(36916.2422, grad_fn=<MseLossBackward0>)\n",
      "2701 tensor(36897.7344, grad_fn=<MseLossBackward0>)\n",
      "2702 tensor(36879.2344, grad_fn=<MseLossBackward0>)\n",
      "2703 tensor(36860.7422, grad_fn=<MseLossBackward0>)\n",
      "2704 tensor(36842.2617, grad_fn=<MseLossBackward0>)\n",
      "2705 tensor(36823.7852, grad_fn=<MseLossBackward0>)\n",
      "2706 tensor(36805.3242, grad_fn=<MseLossBackward0>)\n",
      "2707 tensor(36786.8672, grad_fn=<MseLossBackward0>)\n",
      "2708 tensor(36768.4219, grad_fn=<MseLossBackward0>)\n",
      "2709 tensor(36749.9844, grad_fn=<MseLossBackward0>)\n",
      "2710 tensor(36731.5547, grad_fn=<MseLossBackward0>)\n",
      "2711 tensor(36713.1367, grad_fn=<MseLossBackward0>)\n",
      "2712 tensor(36694.7266, grad_fn=<MseLossBackward0>)\n",
      "2713 tensor(36676.3242, grad_fn=<MseLossBackward0>)\n",
      "2714 tensor(36657.9336, grad_fn=<MseLossBackward0>)\n",
      "2715 tensor(36639.5469, grad_fn=<MseLossBackward0>)\n",
      "2716 tensor(36621.1719, grad_fn=<MseLossBackward0>)\n",
      "2717 tensor(36602.8086, grad_fn=<MseLossBackward0>)\n",
      "2718 tensor(36584.4492, grad_fn=<MseLossBackward0>)\n",
      "2719 tensor(36566.1055, grad_fn=<MseLossBackward0>)\n",
      "2720 tensor(36547.7656, grad_fn=<MseLossBackward0>)\n",
      "2721 tensor(36529.4375, grad_fn=<MseLossBackward0>)\n",
      "2722 tensor(36511.1133, grad_fn=<MseLossBackward0>)\n",
      "2723 tensor(36492.8047, grad_fn=<MseLossBackward0>)\n",
      "2724 tensor(36474.5000, grad_fn=<MseLossBackward0>)\n",
      "2725 tensor(36456.2070, grad_fn=<MseLossBackward0>)\n",
      "2726 tensor(36437.9258, grad_fn=<MseLossBackward0>)\n",
      "2727 tensor(36419.6484, grad_fn=<MseLossBackward0>)\n",
      "2728 tensor(36401.3789, grad_fn=<MseLossBackward0>)\n",
      "2729 tensor(36383.1250, grad_fn=<MseLossBackward0>)\n",
      "2730 tensor(36364.8789, grad_fn=<MseLossBackward0>)\n",
      "2731 tensor(36346.6367, grad_fn=<MseLossBackward0>)\n",
      "2732 tensor(36328.4023, grad_fn=<MseLossBackward0>)\n",
      "2733 tensor(36310.1836, grad_fn=<MseLossBackward0>)\n",
      "2734 tensor(36291.9688, grad_fn=<MseLossBackward0>)\n",
      "2735 tensor(36273.7695, grad_fn=<MseLossBackward0>)\n",
      "2736 tensor(36255.5781, grad_fn=<MseLossBackward0>)\n",
      "2737 tensor(36237.3945, grad_fn=<MseLossBackward0>)\n",
      "2738 tensor(36219.2188, grad_fn=<MseLossBackward0>)\n",
      "2739 tensor(36201.0508, grad_fn=<MseLossBackward0>)\n",
      "2740 tensor(36182.8945, grad_fn=<MseLossBackward0>)\n",
      "2741 tensor(36164.7461, grad_fn=<MseLossBackward0>)\n",
      "2742 tensor(36146.6094, grad_fn=<MseLossBackward0>)\n",
      "2743 tensor(36128.4766, grad_fn=<MseLossBackward0>)\n",
      "2744 tensor(36110.3516, grad_fn=<MseLossBackward0>)\n",
      "2745 tensor(36092.2461, grad_fn=<MseLossBackward0>)\n",
      "2746 tensor(36074.1445, grad_fn=<MseLossBackward0>)\n",
      "2747 tensor(36056.0508, grad_fn=<MseLossBackward0>)\n",
      "2748 tensor(36037.9648, grad_fn=<MseLossBackward0>)\n",
      "2749 tensor(36019.8945, grad_fn=<MseLossBackward0>)\n",
      "2750 tensor(36001.8281, grad_fn=<MseLossBackward0>)\n",
      "2751 tensor(35983.7695, grad_fn=<MseLossBackward0>)\n",
      "2752 tensor(35965.7227, grad_fn=<MseLossBackward0>)\n",
      "2753 tensor(35947.6875, grad_fn=<MseLossBackward0>)\n",
      "2754 tensor(35929.6602, grad_fn=<MseLossBackward0>)\n",
      "2755 tensor(35911.6406, grad_fn=<MseLossBackward0>)\n",
      "2756 tensor(35893.6289, grad_fn=<MseLossBackward0>)\n",
      "2757 tensor(35875.6289, grad_fn=<MseLossBackward0>)\n",
      "2758 tensor(35857.6406, grad_fn=<MseLossBackward0>)\n",
      "2759 tensor(35839.6523, grad_fn=<MseLossBackward0>)\n",
      "2760 tensor(35821.6836, grad_fn=<MseLossBackward0>)\n",
      "2761 tensor(35803.7148, grad_fn=<MseLossBackward0>)\n",
      "2762 tensor(35785.7656, grad_fn=<MseLossBackward0>)\n",
      "2763 tensor(35767.8164, grad_fn=<MseLossBackward0>)\n",
      "2764 tensor(35749.8789, grad_fn=<MseLossBackward0>)\n",
      "2765 tensor(35731.9531, grad_fn=<MseLossBackward0>)\n",
      "2766 tensor(35714.0391, grad_fn=<MseLossBackward0>)\n",
      "2767 tensor(35696.1289, grad_fn=<MseLossBackward0>)\n",
      "2768 tensor(35678.2305, grad_fn=<MseLossBackward0>)\n",
      "2769 tensor(35660.3359, grad_fn=<MseLossBackward0>)\n",
      "2770 tensor(35642.4609, grad_fn=<MseLossBackward0>)\n",
      "2771 tensor(35624.5898, grad_fn=<MseLossBackward0>)\n",
      "2772 tensor(35606.7305, grad_fn=<MseLossBackward0>)\n",
      "2773 tensor(35588.8750, grad_fn=<MseLossBackward0>)\n",
      "2774 tensor(35571.0352, grad_fn=<MseLossBackward0>)\n",
      "2775 tensor(35553.1992, grad_fn=<MseLossBackward0>)\n",
      "2776 tensor(35535.3711, grad_fn=<MseLossBackward0>)\n",
      "2777 tensor(35517.5625, grad_fn=<MseLossBackward0>)\n",
      "2778 tensor(35499.7539, grad_fn=<MseLossBackward0>)\n",
      "2779 tensor(35481.9570, grad_fn=<MseLossBackward0>)\n",
      "2780 tensor(35464.1719, grad_fn=<MseLossBackward0>)\n",
      "2781 tensor(35446.3984, grad_fn=<MseLossBackward0>)\n",
      "2782 tensor(35428.6250, grad_fn=<MseLossBackward0>)\n",
      "2783 tensor(35410.8711, grad_fn=<MseLossBackward0>)\n",
      "2784 tensor(35393.1172, grad_fn=<MseLossBackward0>)\n",
      "2785 tensor(35375.3789, grad_fn=<MseLossBackward0>)\n",
      "2786 tensor(35357.6484, grad_fn=<MseLossBackward0>)\n",
      "2787 tensor(35339.9297, grad_fn=<MseLossBackward0>)\n",
      "2788 tensor(35322.2188, grad_fn=<MseLossBackward0>)\n",
      "2789 tensor(35304.5117, grad_fn=<MseLossBackward0>)\n",
      "2790 tensor(35286.8203, grad_fn=<MseLossBackward0>)\n",
      "2791 tensor(35269.1367, grad_fn=<MseLossBackward0>)\n",
      "2792 tensor(35251.4648, grad_fn=<MseLossBackward0>)\n",
      "2793 tensor(35233.8008, grad_fn=<MseLossBackward0>)\n",
      "2794 tensor(35216.1445, grad_fn=<MseLossBackward0>)\n",
      "2795 tensor(35198.5000, grad_fn=<MseLossBackward0>)\n",
      "2796 tensor(35180.8633, grad_fn=<MseLossBackward0>)\n",
      "2797 tensor(35163.2344, grad_fn=<MseLossBackward0>)\n",
      "2798 tensor(35145.6211, grad_fn=<MseLossBackward0>)\n",
      "2799 tensor(35128.0156, grad_fn=<MseLossBackward0>)\n",
      "2800 tensor(35110.4141, grad_fn=<MseLossBackward0>)\n",
      "2801 tensor(35092.8281, grad_fn=<MseLossBackward0>)\n",
      "2802 tensor(35075.2500, grad_fn=<MseLossBackward0>)\n",
      "2803 tensor(35057.6797, grad_fn=<MseLossBackward0>)\n",
      "2804 tensor(35040.1172, grad_fn=<MseLossBackward0>)\n",
      "2805 tensor(35022.5664, grad_fn=<MseLossBackward0>)\n",
      "2806 tensor(35005.0273, grad_fn=<MseLossBackward0>)\n",
      "2807 tensor(34987.5000, grad_fn=<MseLossBackward0>)\n",
      "2808 tensor(34969.9727, grad_fn=<MseLossBackward0>)\n",
      "2809 tensor(34952.4609, grad_fn=<MseLossBackward0>)\n",
      "2810 tensor(34934.9531, grad_fn=<MseLossBackward0>)\n",
      "2811 tensor(34917.4648, grad_fn=<MseLossBackward0>)\n",
      "2812 tensor(34899.9766, grad_fn=<MseLossBackward0>)\n",
      "2813 tensor(34882.5078, grad_fn=<MseLossBackward0>)\n",
      "2814 tensor(34865.0391, grad_fn=<MseLossBackward0>)\n",
      "2815 tensor(34847.5859, grad_fn=<MseLossBackward0>)\n",
      "2816 tensor(34830.1367, grad_fn=<MseLossBackward0>)\n",
      "2817 tensor(34812.7070, grad_fn=<MseLossBackward0>)\n",
      "2818 tensor(34795.2773, grad_fn=<MseLossBackward0>)\n",
      "2819 tensor(34777.8633, grad_fn=<MseLossBackward0>)\n",
      "2820 tensor(34760.4570, grad_fn=<MseLossBackward0>)\n",
      "2821 tensor(34743.0586, grad_fn=<MseLossBackward0>)\n",
      "2822 tensor(34725.6680, grad_fn=<MseLossBackward0>)\n",
      "2823 tensor(34708.2891, grad_fn=<MseLossBackward0>)\n",
      "2824 tensor(34690.9219, grad_fn=<MseLossBackward0>)\n",
      "2825 tensor(34673.5586, grad_fn=<MseLossBackward0>)\n",
      "2826 tensor(34656.2109, grad_fn=<MseLossBackward0>)\n",
      "2827 tensor(34638.8711, grad_fn=<MseLossBackward0>)\n",
      "2828 tensor(34621.5469, grad_fn=<MseLossBackward0>)\n",
      "2829 tensor(34604.2227, grad_fn=<MseLossBackward0>)\n",
      "2830 tensor(34586.9141, grad_fn=<MseLossBackward0>)\n",
      "2831 tensor(34569.6094, grad_fn=<MseLossBackward0>)\n",
      "2832 tensor(34552.3164, grad_fn=<MseLossBackward0>)\n",
      "2833 tensor(34535.0352, grad_fn=<MseLossBackward0>)\n",
      "2834 tensor(34517.7695, grad_fn=<MseLossBackward0>)\n",
      "2835 tensor(34500.5000, grad_fn=<MseLossBackward0>)\n",
      "2836 tensor(34483.2461, grad_fn=<MseLossBackward0>)\n",
      "2837 tensor(34466.0039, grad_fn=<MseLossBackward0>)\n",
      "2838 tensor(34448.7734, grad_fn=<MseLossBackward0>)\n",
      "2839 tensor(34431.5469, grad_fn=<MseLossBackward0>)\n",
      "2840 tensor(34414.3320, grad_fn=<MseLossBackward0>)\n",
      "2841 tensor(34397.1289, grad_fn=<MseLossBackward0>)\n",
      "2842 tensor(34379.9297, grad_fn=<MseLossBackward0>)\n",
      "2843 tensor(34362.7500, grad_fn=<MseLossBackward0>)\n",
      "2844 tensor(34345.5742, grad_fn=<MseLossBackward0>)\n",
      "2845 tensor(34328.4102, grad_fn=<MseLossBackward0>)\n",
      "2846 tensor(34311.2500, grad_fn=<MseLossBackward0>)\n",
      "2847 tensor(34294.1055, grad_fn=<MseLossBackward0>)\n",
      "2848 tensor(34276.9648, grad_fn=<MseLossBackward0>)\n",
      "2849 tensor(34259.8438, grad_fn=<MseLossBackward0>)\n",
      "2850 tensor(34242.7266, grad_fn=<MseLossBackward0>)\n",
      "2851 tensor(34225.6172, grad_fn=<MseLossBackward0>)\n",
      "2852 tensor(34208.5195, grad_fn=<MseLossBackward0>)\n",
      "2853 tensor(34191.4336, grad_fn=<MseLossBackward0>)\n",
      "2854 tensor(34174.3516, grad_fn=<MseLossBackward0>)\n",
      "2855 tensor(34157.2891, grad_fn=<MseLossBackward0>)\n",
      "2856 tensor(34140.2305, grad_fn=<MseLossBackward0>)\n",
      "2857 tensor(34123.1797, grad_fn=<MseLossBackward0>)\n",
      "2858 tensor(34106.1406, grad_fn=<MseLossBackward0>)\n",
      "2859 tensor(34089.1094, grad_fn=<MseLossBackward0>)\n",
      "2860 tensor(34072.0938, grad_fn=<MseLossBackward0>)\n",
      "2861 tensor(34055.0898, grad_fn=<MseLossBackward0>)\n",
      "2862 tensor(34038.0859, grad_fn=<MseLossBackward0>)\n",
      "2863 tensor(34021.0977, grad_fn=<MseLossBackward0>)\n",
      "2864 tensor(34004.1133, grad_fn=<MseLossBackward0>)\n",
      "2865 tensor(33987.1406, grad_fn=<MseLossBackward0>)\n",
      "2866 tensor(33970.1836, grad_fn=<MseLossBackward0>)\n",
      "2867 tensor(33953.2344, grad_fn=<MseLossBackward0>)\n",
      "2868 tensor(33936.2930, grad_fn=<MseLossBackward0>)\n",
      "2869 tensor(33919.3633, grad_fn=<MseLossBackward0>)\n",
      "2870 tensor(33902.4414, grad_fn=<MseLossBackward0>)\n",
      "2871 tensor(33885.5273, grad_fn=<MseLossBackward0>)\n",
      "2872 tensor(33868.6328, grad_fn=<MseLossBackward0>)\n",
      "2873 tensor(33851.7383, grad_fn=<MseLossBackward0>)\n",
      "2874 tensor(33834.8594, grad_fn=<MseLossBackward0>)\n",
      "2875 tensor(33817.9805, grad_fn=<MseLossBackward0>)\n",
      "2876 tensor(33801.1211, grad_fn=<MseLossBackward0>)\n",
      "2877 tensor(33784.2695, grad_fn=<MseLossBackward0>)\n",
      "2878 tensor(33767.4297, grad_fn=<MseLossBackward0>)\n",
      "2879 tensor(33750.5977, grad_fn=<MseLossBackward0>)\n",
      "2880 tensor(33733.7734, grad_fn=<MseLossBackward0>)\n",
      "2881 tensor(33716.9648, grad_fn=<MseLossBackward0>)\n",
      "2882 tensor(33700.1602, grad_fn=<MseLossBackward0>)\n",
      "2883 tensor(33683.3672, grad_fn=<MseLossBackward0>)\n",
      "2884 tensor(33666.5820, grad_fn=<MseLossBackward0>)\n",
      "2885 tensor(33649.8125, grad_fn=<MseLossBackward0>)\n",
      "2886 tensor(33633.0469, grad_fn=<MseLossBackward0>)\n",
      "2887 tensor(33616.2930, grad_fn=<MseLossBackward0>)\n",
      "2888 tensor(33599.5547, grad_fn=<MseLossBackward0>)\n",
      "2889 tensor(33582.8203, grad_fn=<MseLossBackward0>)\n",
      "2890 tensor(33566.0938, grad_fn=<MseLossBackward0>)\n",
      "2891 tensor(33549.3828, grad_fn=<MseLossBackward0>)\n",
      "2892 tensor(33532.6797, grad_fn=<MseLossBackward0>)\n",
      "2893 tensor(33515.9883, grad_fn=<MseLossBackward0>)\n",
      "2894 tensor(33499.3086, grad_fn=<MseLossBackward0>)\n",
      "2895 tensor(33482.6328, grad_fn=<MseLossBackward0>)\n",
      "2896 tensor(33465.9688, grad_fn=<MseLossBackward0>)\n",
      "2897 tensor(33449.3164, grad_fn=<MseLossBackward0>)\n",
      "2898 tensor(33432.6680, grad_fn=<MseLossBackward0>)\n",
      "2899 tensor(33416.0352, grad_fn=<MseLossBackward0>)\n",
      "2900 tensor(33399.4141, grad_fn=<MseLossBackward0>)\n",
      "2901 tensor(33382.8047, grad_fn=<MseLossBackward0>)\n",
      "2902 tensor(33366.1992, grad_fn=<MseLossBackward0>)\n",
      "2903 tensor(33349.6055, grad_fn=<MseLossBackward0>)\n",
      "2904 tensor(33333.0195, grad_fn=<MseLossBackward0>)\n",
      "2905 tensor(33316.4453, grad_fn=<MseLossBackward0>)\n",
      "2906 tensor(33299.8828, grad_fn=<MseLossBackward0>)\n",
      "2907 tensor(33283.3320, grad_fn=<MseLossBackward0>)\n",
      "2908 tensor(33266.7891, grad_fn=<MseLossBackward0>)\n",
      "2909 tensor(33250.2539, grad_fn=<MseLossBackward0>)\n",
      "2910 tensor(33233.7344, grad_fn=<MseLossBackward0>)\n",
      "2911 tensor(33217.2148, grad_fn=<MseLossBackward0>)\n",
      "2912 tensor(33200.7109, grad_fn=<MseLossBackward0>)\n",
      "2913 tensor(33184.2188, grad_fn=<MseLossBackward0>)\n",
      "2914 tensor(33167.7344, grad_fn=<MseLossBackward0>)\n",
      "2915 tensor(33151.2617, grad_fn=<MseLossBackward0>)\n",
      "2916 tensor(33134.8008, grad_fn=<MseLossBackward0>)\n",
      "2917 tensor(33118.3438, grad_fn=<MseLossBackward0>)\n",
      "2918 tensor(33101.9062, grad_fn=<MseLossBackward0>)\n",
      "2919 tensor(33085.4727, grad_fn=<MseLossBackward0>)\n",
      "2920 tensor(33069.0508, grad_fn=<MseLossBackward0>)\n",
      "2921 tensor(33052.6367, grad_fn=<MseLossBackward0>)\n",
      "2922 tensor(33036.2305, grad_fn=<MseLossBackward0>)\n",
      "2923 tensor(33019.8398, grad_fn=<MseLossBackward0>)\n",
      "2924 tensor(33003.4570, grad_fn=<MseLossBackward0>)\n",
      "2925 tensor(32987.0859, grad_fn=<MseLossBackward0>)\n",
      "2926 tensor(32970.7227, grad_fn=<MseLossBackward0>)\n",
      "2927 tensor(32954.3750, grad_fn=<MseLossBackward0>)\n",
      "2928 tensor(32938.0273, grad_fn=<MseLossBackward0>)\n",
      "2929 tensor(32921.6953, grad_fn=<MseLossBackward0>)\n",
      "2930 tensor(32905.3789, grad_fn=<MseLossBackward0>)\n",
      "2931 tensor(32889.0664, grad_fn=<MseLossBackward0>)\n",
      "2932 tensor(32872.7617, grad_fn=<MseLossBackward0>)\n",
      "2933 tensor(32856.4727, grad_fn=<MseLossBackward0>)\n",
      "2934 tensor(32840.1914, grad_fn=<MseLossBackward0>)\n",
      "2935 tensor(32823.9180, grad_fn=<MseLossBackward0>)\n",
      "2936 tensor(32807.6562, grad_fn=<MseLossBackward0>)\n",
      "2937 tensor(32791.4102, grad_fn=<MseLossBackward0>)\n",
      "2938 tensor(32775.1680, grad_fn=<MseLossBackward0>)\n",
      "2939 tensor(32758.9355, grad_fn=<MseLossBackward0>)\n",
      "2940 tensor(32742.7148, grad_fn=<MseLossBackward0>)\n",
      "2941 tensor(32726.5020, grad_fn=<MseLossBackward0>)\n",
      "2942 tensor(32710.3047, grad_fn=<MseLossBackward0>)\n",
      "2943 tensor(32694.1133, grad_fn=<MseLossBackward0>)\n",
      "2944 tensor(32677.9336, grad_fn=<MseLossBackward0>)\n",
      "2945 tensor(32661.7676, grad_fn=<MseLossBackward0>)\n",
      "2946 tensor(32645.6055, grad_fn=<MseLossBackward0>)\n",
      "2947 tensor(32629.4570, grad_fn=<MseLossBackward0>)\n",
      "2948 tensor(32613.3184, grad_fn=<MseLossBackward0>)\n",
      "2949 tensor(32597.1895, grad_fn=<MseLossBackward0>)\n",
      "2950 tensor(32581.0703, grad_fn=<MseLossBackward0>)\n",
      "2951 tensor(32564.9590, grad_fn=<MseLossBackward0>)\n",
      "2952 tensor(32548.8613, grad_fn=<MseLossBackward0>)\n",
      "2953 tensor(32532.7734, grad_fn=<MseLossBackward0>)\n",
      "2954 tensor(32516.6953, grad_fn=<MseLossBackward0>)\n",
      "2955 tensor(32500.6289, grad_fn=<MseLossBackward0>)\n",
      "2956 tensor(32484.5723, grad_fn=<MseLossBackward0>)\n",
      "2957 tensor(32468.5234, grad_fn=<MseLossBackward0>)\n",
      "2958 tensor(32452.4863, grad_fn=<MseLossBackward0>)\n",
      "2959 tensor(32436.4590, grad_fn=<MseLossBackward0>)\n",
      "2960 tensor(32420.4434, grad_fn=<MseLossBackward0>)\n",
      "2961 tensor(32404.4355, grad_fn=<MseLossBackward0>)\n",
      "2962 tensor(32388.4395, grad_fn=<MseLossBackward0>)\n",
      "2963 tensor(32372.4531, grad_fn=<MseLossBackward0>)\n",
      "2964 tensor(32356.4785, grad_fn=<MseLossBackward0>)\n",
      "2965 tensor(32340.5137, grad_fn=<MseLossBackward0>)\n",
      "2966 tensor(32324.5605, grad_fn=<MseLossBackward0>)\n",
      "2967 tensor(32308.6133, grad_fn=<MseLossBackward0>)\n",
      "2968 tensor(32292.6777, grad_fn=<MseLossBackward0>)\n",
      "2969 tensor(32276.7559, grad_fn=<MseLossBackward0>)\n",
      "2970 tensor(32260.8379, grad_fn=<MseLossBackward0>)\n",
      "2971 tensor(32244.9355, grad_fn=<MseLossBackward0>)\n",
      "2972 tensor(32229.0430, grad_fn=<MseLossBackward0>)\n",
      "2973 tensor(32213.1582, grad_fn=<MseLossBackward0>)\n",
      "2974 tensor(32197.2852, grad_fn=<MseLossBackward0>)\n",
      "2975 tensor(32181.4219, grad_fn=<MseLossBackward0>)\n",
      "2976 tensor(32165.5723, grad_fn=<MseLossBackward0>)\n",
      "2977 tensor(32149.7266, grad_fn=<MseLossBackward0>)\n",
      "2978 tensor(32133.8945, grad_fn=<MseLossBackward0>)\n",
      "2979 tensor(32118.0762, grad_fn=<MseLossBackward0>)\n",
      "2980 tensor(32102.2617, grad_fn=<MseLossBackward0>)\n",
      "2981 tensor(32086.4648, grad_fn=<MseLossBackward0>)\n",
      "2982 tensor(32070.6738, grad_fn=<MseLossBackward0>)\n",
      "2983 tensor(32054.8945, grad_fn=<MseLossBackward0>)\n",
      "2984 tensor(32039.1211, grad_fn=<MseLossBackward0>)\n",
      "2985 tensor(32023.3652, grad_fn=<MseLossBackward0>)\n",
      "2986 tensor(32007.6152, grad_fn=<MseLossBackward0>)\n",
      "2987 tensor(31991.8730, grad_fn=<MseLossBackward0>)\n",
      "2988 tensor(31976.1445, grad_fn=<MseLossBackward0>)\n",
      "2989 tensor(31960.4277, grad_fn=<MseLossBackward0>)\n",
      "2990 tensor(31944.7168, grad_fn=<MseLossBackward0>)\n",
      "2991 tensor(31929.0195, grad_fn=<MseLossBackward0>)\n",
      "2992 tensor(31913.3359, grad_fn=<MseLossBackward0>)\n",
      "2993 tensor(31897.6562, grad_fn=<MseLossBackward0>)\n",
      "2994 tensor(31881.9902, grad_fn=<MseLossBackward0>)\n",
      "2995 tensor(31866.3359, grad_fn=<MseLossBackward0>)\n",
      "2996 tensor(31850.6895, grad_fn=<MseLossBackward0>)\n",
      "2997 tensor(31835.0527, grad_fn=<MseLossBackward0>)\n",
      "2998 tensor(31819.4316, grad_fn=<MseLossBackward0>)\n",
      "2999 tensor(31803.8145, grad_fn=<MseLossBackward0>)\n",
      "3000 tensor(31788.2109, grad_fn=<MseLossBackward0>)\n",
      "3001 tensor(31772.6133, grad_fn=<MseLossBackward0>)\n",
      "3002 tensor(31757.0312, grad_fn=<MseLossBackward0>)\n",
      "3003 tensor(31741.4551, grad_fn=<MseLossBackward0>)\n",
      "3004 tensor(31725.8926, grad_fn=<MseLossBackward0>)\n",
      "3005 tensor(31710.3418, grad_fn=<MseLossBackward0>)\n",
      "3006 tensor(31694.8008, grad_fn=<MseLossBackward0>)\n",
      "3007 tensor(31679.2695, grad_fn=<MseLossBackward0>)\n",
      "3008 tensor(31663.7500, grad_fn=<MseLossBackward0>)\n",
      "3009 tensor(31648.2363, grad_fn=<MseLossBackward0>)\n",
      "3010 tensor(31632.7344, grad_fn=<MseLossBackward0>)\n",
      "3011 tensor(31617.2441, grad_fn=<MseLossBackward0>)\n",
      "3012 tensor(31601.7656, grad_fn=<MseLossBackward0>)\n",
      "3013 tensor(31586.2949, grad_fn=<MseLossBackward0>)\n",
      "3014 tensor(31570.8359, grad_fn=<MseLossBackward0>)\n",
      "3015 tensor(31555.3887, grad_fn=<MseLossBackward0>)\n",
      "3016 tensor(31539.9531, grad_fn=<MseLossBackward0>)\n",
      "3017 tensor(31524.5215, grad_fn=<MseLossBackward0>)\n",
      "3018 tensor(31509.1074, grad_fn=<MseLossBackward0>)\n",
      "3019 tensor(31493.7012, grad_fn=<MseLossBackward0>)\n",
      "3020 tensor(31478.3047, grad_fn=<MseLossBackward0>)\n",
      "3021 tensor(31462.9160, grad_fn=<MseLossBackward0>)\n",
      "3022 tensor(31447.5410, grad_fn=<MseLossBackward0>)\n",
      "3023 tensor(31432.1777, grad_fn=<MseLossBackward0>)\n",
      "3024 tensor(31416.8242, grad_fn=<MseLossBackward0>)\n",
      "3025 tensor(31401.4785, grad_fn=<MseLossBackward0>)\n",
      "3026 tensor(31386.1445, grad_fn=<MseLossBackward0>)\n",
      "3027 tensor(31370.8223, grad_fn=<MseLossBackward0>)\n",
      "3028 tensor(31355.5078, grad_fn=<MseLossBackward0>)\n",
      "3029 tensor(31340.2051, grad_fn=<MseLossBackward0>)\n",
      "3030 tensor(31324.9141, grad_fn=<MseLossBackward0>)\n",
      "3031 tensor(31309.6309, grad_fn=<MseLossBackward0>)\n",
      "3032 tensor(31294.3613, grad_fn=<MseLossBackward0>)\n",
      "3033 tensor(31279.1016, grad_fn=<MseLossBackward0>)\n",
      "3034 tensor(31263.8516, grad_fn=<MseLossBackward0>)\n",
      "3035 tensor(31248.6133, grad_fn=<MseLossBackward0>)\n",
      "3036 tensor(31233.3848, grad_fn=<MseLossBackward0>)\n",
      "3037 tensor(31218.1660, grad_fn=<MseLossBackward0>)\n",
      "3038 tensor(31202.9551, grad_fn=<MseLossBackward0>)\n",
      "3039 tensor(31187.7617, grad_fn=<MseLossBackward0>)\n",
      "3040 tensor(31172.5723, grad_fn=<MseLossBackward0>)\n",
      "3041 tensor(31157.3945, grad_fn=<MseLossBackward0>)\n",
      "3042 tensor(31142.2285, grad_fn=<MseLossBackward0>)\n",
      "3043 tensor(31127.0742, grad_fn=<MseLossBackward0>)\n",
      "3044 tensor(31111.9297, grad_fn=<MseLossBackward0>)\n",
      "3045 tensor(31096.7949, grad_fn=<MseLossBackward0>)\n",
      "3046 tensor(31081.6699, grad_fn=<MseLossBackward0>)\n",
      "3047 tensor(31066.5605, grad_fn=<MseLossBackward0>)\n",
      "3048 tensor(31051.4551, grad_fn=<MseLossBackward0>)\n",
      "3049 tensor(31036.3613, grad_fn=<MseLossBackward0>)\n",
      "3050 tensor(31021.2793, grad_fn=<MseLossBackward0>)\n",
      "3051 tensor(31006.2070, grad_fn=<MseLossBackward0>)\n",
      "3052 tensor(30991.1465, grad_fn=<MseLossBackward0>)\n",
      "3053 tensor(30976.0977, grad_fn=<MseLossBackward0>)\n",
      "3054 tensor(30961.0566, grad_fn=<MseLossBackward0>)\n",
      "3055 tensor(30946.0273, grad_fn=<MseLossBackward0>)\n",
      "3056 tensor(30931.0078, grad_fn=<MseLossBackward0>)\n",
      "3057 tensor(30916.0020, grad_fn=<MseLossBackward0>)\n",
      "3058 tensor(30901.0039, grad_fn=<MseLossBackward0>)\n",
      "3059 tensor(30886.0156, grad_fn=<MseLossBackward0>)\n",
      "3060 tensor(30871.0371, grad_fn=<MseLossBackward0>)\n",
      "3061 tensor(30856.0742, grad_fn=<MseLossBackward0>)\n",
      "3062 tensor(30841.1172, grad_fn=<MseLossBackward0>)\n",
      "3063 tensor(30826.1699, grad_fn=<MseLossBackward0>)\n",
      "3064 tensor(30811.2383, grad_fn=<MseLossBackward0>)\n",
      "3065 tensor(30796.3164, grad_fn=<MseLossBackward0>)\n",
      "3066 tensor(30781.3984, grad_fn=<MseLossBackward0>)\n",
      "3067 tensor(30766.4980, grad_fn=<MseLossBackward0>)\n",
      "3068 tensor(30751.6074, grad_fn=<MseLossBackward0>)\n",
      "3069 tensor(30736.7227, grad_fn=<MseLossBackward0>)\n",
      "3070 tensor(30721.8535, grad_fn=<MseLossBackward0>)\n",
      "3071 tensor(30706.9902, grad_fn=<MseLossBackward0>)\n",
      "3072 tensor(30692.1426, grad_fn=<MseLossBackward0>)\n",
      "3073 tensor(30677.3027, grad_fn=<MseLossBackward0>)\n",
      "3074 tensor(30662.4727, grad_fn=<MseLossBackward0>)\n",
      "3075 tensor(30647.6582, grad_fn=<MseLossBackward0>)\n",
      "3076 tensor(30632.8516, grad_fn=<MseLossBackward0>)\n",
      "3077 tensor(30618.0508, grad_fn=<MseLossBackward0>)\n",
      "3078 tensor(30603.2656, grad_fn=<MseLossBackward0>)\n",
      "3079 tensor(30588.4902, grad_fn=<MseLossBackward0>)\n",
      "3080 tensor(30573.7246, grad_fn=<MseLossBackward0>)\n",
      "3081 tensor(30558.9707, grad_fn=<MseLossBackward0>)\n",
      "3082 tensor(30544.2266, grad_fn=<MseLossBackward0>)\n",
      "3083 tensor(30529.4941, grad_fn=<MseLossBackward0>)\n",
      "3084 tensor(30514.7715, grad_fn=<MseLossBackward0>)\n",
      "3085 tensor(30500.0586, grad_fn=<MseLossBackward0>)\n",
      "3086 tensor(30485.3555, grad_fn=<MseLossBackward0>)\n",
      "3087 tensor(30470.6641, grad_fn=<MseLossBackward0>)\n",
      "3088 tensor(30455.9863, grad_fn=<MseLossBackward0>)\n",
      "3089 tensor(30441.3164, grad_fn=<MseLossBackward0>)\n",
      "3090 tensor(30426.6562, grad_fn=<MseLossBackward0>)\n",
      "3091 tensor(30412.0098, grad_fn=<MseLossBackward0>)\n",
      "3092 tensor(30397.3711, grad_fn=<MseLossBackward0>)\n",
      "3093 tensor(30382.7422, grad_fn=<MseLossBackward0>)\n",
      "3094 tensor(30368.1270, grad_fn=<MseLossBackward0>)\n",
      "3095 tensor(30353.5195, grad_fn=<MseLossBackward0>)\n",
      "3096 tensor(30338.9258, grad_fn=<MseLossBackward0>)\n",
      "3097 tensor(30324.3418, grad_fn=<MseLossBackward0>)\n",
      "3098 tensor(30309.7695, grad_fn=<MseLossBackward0>)\n",
      "3099 tensor(30295.2051, grad_fn=<MseLossBackward0>)\n",
      "3100 tensor(30280.6523, grad_fn=<MseLossBackward0>)\n",
      "3101 tensor(30266.1074, grad_fn=<MseLossBackward0>)\n",
      "3102 tensor(30251.5801, grad_fn=<MseLossBackward0>)\n",
      "3103 tensor(30237.0566, grad_fn=<MseLossBackward0>)\n",
      "3104 tensor(30222.5469, grad_fn=<MseLossBackward0>)\n",
      "3105 tensor(30208.0508, grad_fn=<MseLossBackward0>)\n",
      "3106 tensor(30193.5586, grad_fn=<MseLossBackward0>)\n",
      "3107 tensor(30179.0801, grad_fn=<MseLossBackward0>)\n",
      "3108 tensor(30164.6152, grad_fn=<MseLossBackward0>)\n",
      "3109 tensor(30150.1582, grad_fn=<MseLossBackward0>)\n",
      "3110 tensor(30135.7109, grad_fn=<MseLossBackward0>)\n",
      "3111 tensor(30121.2754, grad_fn=<MseLossBackward0>)\n",
      "3112 tensor(30106.8496, grad_fn=<MseLossBackward0>)\n",
      "3113 tensor(30092.4336, grad_fn=<MseLossBackward0>)\n",
      "3114 tensor(30078.0332, grad_fn=<MseLossBackward0>)\n",
      "3115 tensor(30063.6406, grad_fn=<MseLossBackward0>)\n",
      "3116 tensor(30049.2559, grad_fn=<MseLossBackward0>)\n",
      "3117 tensor(30034.8867, grad_fn=<MseLossBackward0>)\n",
      "3118 tensor(30020.5254, grad_fn=<MseLossBackward0>)\n",
      "3119 tensor(30006.1758, grad_fn=<MseLossBackward0>)\n",
      "3120 tensor(29991.8340, grad_fn=<MseLossBackward0>)\n",
      "3121 tensor(29977.5059, grad_fn=<MseLossBackward0>)\n",
      "3122 tensor(29963.1875, grad_fn=<MseLossBackward0>)\n",
      "3123 tensor(29948.8809, grad_fn=<MseLossBackward0>)\n",
      "3124 tensor(29934.5840, grad_fn=<MseLossBackward0>)\n",
      "3125 tensor(29920.2988, grad_fn=<MseLossBackward0>)\n",
      "3126 tensor(29906.0234, grad_fn=<MseLossBackward0>)\n",
      "3127 tensor(29891.7598, grad_fn=<MseLossBackward0>)\n",
      "3128 tensor(29877.5039, grad_fn=<MseLossBackward0>)\n",
      "3129 tensor(29863.2637, grad_fn=<MseLossBackward0>)\n",
      "3130 tensor(29849.0273, grad_fn=<MseLossBackward0>)\n",
      "3131 tensor(29834.8027, grad_fn=<MseLossBackward0>)\n",
      "3132 tensor(29820.5938, grad_fn=<MseLossBackward0>)\n",
      "3133 tensor(29806.3926, grad_fn=<MseLossBackward0>)\n",
      "3134 tensor(29792.2051, grad_fn=<MseLossBackward0>)\n",
      "3135 tensor(29778.0254, grad_fn=<MseLossBackward0>)\n",
      "3136 tensor(29763.8574, grad_fn=<MseLossBackward0>)\n",
      "3137 tensor(29749.7012, grad_fn=<MseLossBackward0>)\n",
      "3138 tensor(29735.5527, grad_fn=<MseLossBackward0>)\n",
      "3139 tensor(29721.4180, grad_fn=<MseLossBackward0>)\n",
      "3140 tensor(29707.2910, grad_fn=<MseLossBackward0>)\n",
      "3141 tensor(29693.1738, grad_fn=<MseLossBackward0>)\n",
      "3142 tensor(29679.0703, grad_fn=<MseLossBackward0>)\n",
      "3143 tensor(29664.9805, grad_fn=<MseLossBackward0>)\n",
      "3144 tensor(29650.8945, grad_fn=<MseLossBackward0>)\n",
      "3145 tensor(29636.8262, grad_fn=<MseLossBackward0>)\n",
      "3146 tensor(29622.7637, grad_fn=<MseLossBackward0>)\n",
      "3147 tensor(29608.7129, grad_fn=<MseLossBackward0>)\n",
      "3148 tensor(29594.6719, grad_fn=<MseLossBackward0>)\n",
      "3149 tensor(29580.6445, grad_fn=<MseLossBackward0>)\n",
      "3150 tensor(29566.6270, grad_fn=<MseLossBackward0>)\n",
      "3151 tensor(29552.6172, grad_fn=<MseLossBackward0>)\n",
      "3152 tensor(29538.6211, grad_fn=<MseLossBackward0>)\n",
      "3153 tensor(29524.6367, grad_fn=<MseLossBackward0>)\n",
      "3154 tensor(29510.6602, grad_fn=<MseLossBackward0>)\n",
      "3155 tensor(29496.6953, grad_fn=<MseLossBackward0>)\n",
      "3156 tensor(29482.7422, grad_fn=<MseLossBackward0>)\n",
      "3157 tensor(29468.7969, grad_fn=<MseLossBackward0>)\n",
      "3158 tensor(29454.8652, grad_fn=<MseLossBackward0>)\n",
      "3159 tensor(29440.9434, grad_fn=<MseLossBackward0>)\n",
      "3160 tensor(29427.0352, grad_fn=<MseLossBackward0>)\n",
      "3161 tensor(29413.1309, grad_fn=<MseLossBackward0>)\n",
      "3162 tensor(29399.2441, grad_fn=<MseLossBackward0>)\n",
      "3163 tensor(29385.3652, grad_fn=<MseLossBackward0>)\n",
      "3164 tensor(29371.4980, grad_fn=<MseLossBackward0>)\n",
      "3165 tensor(29357.6426, grad_fn=<MseLossBackward0>)\n",
      "3166 tensor(29343.7949, grad_fn=<MseLossBackward0>)\n",
      "3167 tensor(29329.9570, grad_fn=<MseLossBackward0>)\n",
      "3168 tensor(29316.1348, grad_fn=<MseLossBackward0>)\n",
      "3169 tensor(29302.3203, grad_fn=<MseLossBackward0>)\n",
      "3170 tensor(29288.5156, grad_fn=<MseLossBackward0>)\n",
      "3171 tensor(29274.7246, grad_fn=<MseLossBackward0>)\n",
      "3172 tensor(29260.9434, grad_fn=<MseLossBackward0>)\n",
      "3173 tensor(29247.1719, grad_fn=<MseLossBackward0>)\n",
      "3174 tensor(29233.4121, grad_fn=<MseLossBackward0>)\n",
      "3175 tensor(29219.6621, grad_fn=<MseLossBackward0>)\n",
      "3176 tensor(29205.9219, grad_fn=<MseLossBackward0>)\n",
      "3177 tensor(29192.1934, grad_fn=<MseLossBackward0>)\n",
      "3178 tensor(29178.4766, grad_fn=<MseLossBackward0>)\n",
      "3179 tensor(29164.7695, grad_fn=<MseLossBackward0>)\n",
      "3180 tensor(29151.0762, grad_fn=<MseLossBackward0>)\n",
      "3181 tensor(29137.3887, grad_fn=<MseLossBackward0>)\n",
      "3182 tensor(29123.7168, grad_fn=<MseLossBackward0>)\n",
      "3183 tensor(29110.0508, grad_fn=<MseLossBackward0>)\n",
      "3184 tensor(29096.4004, grad_fn=<MseLossBackward0>)\n",
      "3185 tensor(29082.7559, grad_fn=<MseLossBackward0>)\n",
      "3186 tensor(29069.1250, grad_fn=<MseLossBackward0>)\n",
      "3187 tensor(29055.5059, grad_fn=<MseLossBackward0>)\n",
      "3188 tensor(29041.8965, grad_fn=<MseLossBackward0>)\n",
      "3189 tensor(29028.2988, grad_fn=<MseLossBackward0>)\n",
      "3190 tensor(29014.7109, grad_fn=<MseLossBackward0>)\n",
      "3191 tensor(29001.1367, grad_fn=<MseLossBackward0>)\n",
      "3192 tensor(28987.5684, grad_fn=<MseLossBackward0>)\n",
      "3193 tensor(28974.0117, grad_fn=<MseLossBackward0>)\n",
      "3194 tensor(28960.4668, grad_fn=<MseLossBackward0>)\n",
      "3195 tensor(28946.9336, grad_fn=<MseLossBackward0>)\n",
      "3196 tensor(28933.4102, grad_fn=<MseLossBackward0>)\n",
      "3197 tensor(28919.8984, grad_fn=<MseLossBackward0>)\n",
      "3198 tensor(28906.3984, grad_fn=<MseLossBackward0>)\n",
      "3199 tensor(28892.9043, grad_fn=<MseLossBackward0>)\n",
      "3200 tensor(28879.4258, grad_fn=<MseLossBackward0>)\n",
      "3201 tensor(28865.9531, grad_fn=<MseLossBackward0>)\n",
      "3202 tensor(28852.4980, grad_fn=<MseLossBackward0>)\n",
      "3203 tensor(28839.0469, grad_fn=<MseLossBackward0>)\n",
      "3204 tensor(28825.6133, grad_fn=<MseLossBackward0>)\n",
      "3205 tensor(28812.1875, grad_fn=<MseLossBackward0>)\n",
      "3206 tensor(28798.7715, grad_fn=<MseLossBackward0>)\n",
      "3207 tensor(28785.3691, grad_fn=<MseLossBackward0>)\n",
      "3208 tensor(28771.9746, grad_fn=<MseLossBackward0>)\n",
      "3209 tensor(28758.5898, grad_fn=<MseLossBackward0>)\n",
      "3210 tensor(28745.2188, grad_fn=<MseLossBackward0>)\n",
      "3211 tensor(28731.8555, grad_fn=<MseLossBackward0>)\n",
      "3212 tensor(28718.5059, grad_fn=<MseLossBackward0>)\n",
      "3213 tensor(28705.1680, grad_fn=<MseLossBackward0>)\n",
      "3214 tensor(28691.8398, grad_fn=<MseLossBackward0>)\n",
      "3215 tensor(28678.5195, grad_fn=<MseLossBackward0>)\n",
      "3216 tensor(28665.2148, grad_fn=<MseLossBackward0>)\n",
      "3217 tensor(28651.9160, grad_fn=<MseLossBackward0>)\n",
      "3218 tensor(28638.6289, grad_fn=<MseLossBackward0>)\n",
      "3219 tensor(28625.3574, grad_fn=<MseLossBackward0>)\n",
      "3220 tensor(28612.0957, grad_fn=<MseLossBackward0>)\n",
      "3221 tensor(28598.8418, grad_fn=<MseLossBackward0>)\n",
      "3222 tensor(28585.5977, grad_fn=<MseLossBackward0>)\n",
      "3223 tensor(28572.3633, grad_fn=<MseLossBackward0>)\n",
      "3224 tensor(28559.1406, grad_fn=<MseLossBackward0>)\n",
      "3225 tensor(28545.9355, grad_fn=<MseLossBackward0>)\n",
      "3226 tensor(28532.7344, grad_fn=<MseLossBackward0>)\n",
      "3227 tensor(28519.5449, grad_fn=<MseLossBackward0>)\n",
      "3228 tensor(28506.3691, grad_fn=<MseLossBackward0>)\n",
      "3229 tensor(28493.2031, grad_fn=<MseLossBackward0>)\n",
      "3230 tensor(28480.0469, grad_fn=<MseLossBackward0>)\n",
      "3231 tensor(28466.9004, grad_fn=<MseLossBackward0>)\n",
      "3232 tensor(28453.7656, grad_fn=<MseLossBackward0>)\n",
      "3233 tensor(28440.6426, grad_fn=<MseLossBackward0>)\n",
      "3234 tensor(28427.5312, grad_fn=<MseLossBackward0>)\n",
      "3235 tensor(28414.4316, grad_fn=<MseLossBackward0>)\n",
      "3236 tensor(28401.3398, grad_fn=<MseLossBackward0>)\n",
      "3237 tensor(28388.2559, grad_fn=<MseLossBackward0>)\n",
      "3238 tensor(28375.1895, grad_fn=<MseLossBackward0>)\n",
      "3239 tensor(28362.1270, grad_fn=<MseLossBackward0>)\n",
      "3240 tensor(28349.0801, grad_fn=<MseLossBackward0>)\n",
      "3241 tensor(28336.0469, grad_fn=<MseLossBackward0>)\n",
      "3242 tensor(28323.0176, grad_fn=<MseLossBackward0>)\n",
      "3243 tensor(28310.0059, grad_fn=<MseLossBackward0>)\n",
      "3244 tensor(28296.9961, grad_fn=<MseLossBackward0>)\n",
      "3245 tensor(28284.0059, grad_fn=<MseLossBackward0>)\n",
      "3246 tensor(28271.0234, grad_fn=<MseLossBackward0>)\n",
      "3247 tensor(28258.0488, grad_fn=<MseLossBackward0>)\n",
      "3248 tensor(28245.0898, grad_fn=<MseLossBackward0>)\n",
      "3249 tensor(28232.1387, grad_fn=<MseLossBackward0>)\n",
      "3250 tensor(28219.2012, grad_fn=<MseLossBackward0>)\n",
      "3251 tensor(28206.2715, grad_fn=<MseLossBackward0>)\n",
      "3252 tensor(28193.3516, grad_fn=<MseLossBackward0>)\n",
      "3253 tensor(28180.4453, grad_fn=<MseLossBackward0>)\n",
      "3254 tensor(28167.5488, grad_fn=<MseLossBackward0>)\n",
      "3255 tensor(28154.6641, grad_fn=<MseLossBackward0>)\n",
      "3256 tensor(28141.7891, grad_fn=<MseLossBackward0>)\n",
      "3257 tensor(28128.9238, grad_fn=<MseLossBackward0>)\n",
      "3258 tensor(28116.0703, grad_fn=<MseLossBackward0>)\n",
      "3259 tensor(28103.2305, grad_fn=<MseLossBackward0>)\n",
      "3260 tensor(28090.3984, grad_fn=<MseLossBackward0>)\n",
      "3261 tensor(28077.5762, grad_fn=<MseLossBackward0>)\n",
      "3262 tensor(28064.7695, grad_fn=<MseLossBackward0>)\n",
      "3263 tensor(28051.9707, grad_fn=<MseLossBackward0>)\n",
      "3264 tensor(28039.1836, grad_fn=<MseLossBackward0>)\n",
      "3265 tensor(28026.4043, grad_fn=<MseLossBackward0>)\n",
      "3266 tensor(28013.6367, grad_fn=<MseLossBackward0>)\n",
      "3267 tensor(28000.8809, grad_fn=<MseLossBackward0>)\n",
      "3268 tensor(27988.1367, grad_fn=<MseLossBackward0>)\n",
      "3269 tensor(27975.4023, grad_fn=<MseLossBackward0>)\n",
      "3270 tensor(27962.6797, grad_fn=<MseLossBackward0>)\n",
      "3271 tensor(27949.9668, grad_fn=<MseLossBackward0>)\n",
      "3272 tensor(27937.2637, grad_fn=<MseLossBackward0>)\n",
      "3273 tensor(27924.5742, grad_fn=<MseLossBackward0>)\n",
      "3274 tensor(27911.8965, grad_fn=<MseLossBackward0>)\n",
      "3275 tensor(27899.2266, grad_fn=<MseLossBackward0>)\n",
      "3276 tensor(27886.5703, grad_fn=<MseLossBackward0>)\n",
      "3277 tensor(27873.9199, grad_fn=<MseLossBackward0>)\n",
      "3278 tensor(27861.2871, grad_fn=<MseLossBackward0>)\n",
      "3279 tensor(27848.6582, grad_fn=<MseLossBackward0>)\n",
      "3280 tensor(27836.0469, grad_fn=<MseLossBackward0>)\n",
      "3281 tensor(27823.4395, grad_fn=<MseLossBackward0>)\n",
      "3282 tensor(27810.8496, grad_fn=<MseLossBackward0>)\n",
      "3283 tensor(27798.2656, grad_fn=<MseLossBackward0>)\n",
      "3284 tensor(27785.6934, grad_fn=<MseLossBackward0>)\n",
      "3285 tensor(27773.1309, grad_fn=<MseLossBackward0>)\n",
      "3286 tensor(27760.5840, grad_fn=<MseLossBackward0>)\n",
      "3287 tensor(27748.0430, grad_fn=<MseLossBackward0>)\n",
      "3288 tensor(27735.5156, grad_fn=<MseLossBackward0>)\n",
      "3289 tensor(27722.9961, grad_fn=<MseLossBackward0>)\n",
      "3290 tensor(27710.4902, grad_fn=<MseLossBackward0>)\n",
      "3291 tensor(27697.9941, grad_fn=<MseLossBackward0>)\n",
      "3292 tensor(27685.5098, grad_fn=<MseLossBackward0>)\n",
      "3293 tensor(27673.0371, grad_fn=<MseLossBackward0>)\n",
      "3294 tensor(27660.5723, grad_fn=<MseLossBackward0>)\n",
      "3295 tensor(27648.1211, grad_fn=<MseLossBackward0>)\n",
      "3296 tensor(27635.6777, grad_fn=<MseLossBackward0>)\n",
      "3297 tensor(27623.2480, grad_fn=<MseLossBackward0>)\n",
      "3298 tensor(27610.8281, grad_fn=<MseLossBackward0>)\n",
      "3299 tensor(27598.4199, grad_fn=<MseLossBackward0>)\n",
      "3300 tensor(27586.0195, grad_fn=<MseLossBackward0>)\n",
      "3301 tensor(27573.6328, grad_fn=<MseLossBackward0>)\n",
      "3302 tensor(27561.2539, grad_fn=<MseLossBackward0>)\n",
      "3303 tensor(27548.8887, grad_fn=<MseLossBackward0>)\n",
      "3304 tensor(27536.5352, grad_fn=<MseLossBackward0>)\n",
      "3305 tensor(27524.1914, grad_fn=<MseLossBackward0>)\n",
      "3306 tensor(27511.8574, grad_fn=<MseLossBackward0>)\n",
      "3307 tensor(27499.5332, grad_fn=<MseLossBackward0>)\n",
      "3308 tensor(27487.2227, grad_fn=<MseLossBackward0>)\n",
      "3309 tensor(27474.9238, grad_fn=<MseLossBackward0>)\n",
      "3310 tensor(27462.6289, grad_fn=<MseLossBackward0>)\n",
      "3311 tensor(27450.3516, grad_fn=<MseLossBackward0>)\n",
      "3312 tensor(27438.0840, grad_fn=<MseLossBackward0>)\n",
      "3313 tensor(27425.8262, grad_fn=<MseLossBackward0>)\n",
      "3314 tensor(27413.5762, grad_fn=<MseLossBackward0>)\n",
      "3315 tensor(27401.3398, grad_fn=<MseLossBackward0>)\n",
      "3316 tensor(27389.1172, grad_fn=<MseLossBackward0>)\n",
      "3317 tensor(27376.9023, grad_fn=<MseLossBackward0>)\n",
      "3318 tensor(27364.6992, grad_fn=<MseLossBackward0>)\n",
      "3319 tensor(27352.5059, grad_fn=<MseLossBackward0>)\n",
      "3320 tensor(27340.3242, grad_fn=<MseLossBackward0>)\n",
      "3321 tensor(27328.1523, grad_fn=<MseLossBackward0>)\n",
      "3322 tensor(27315.9941, grad_fn=<MseLossBackward0>)\n",
      "3323 tensor(27303.8418, grad_fn=<MseLossBackward0>)\n",
      "3324 tensor(27291.7031, grad_fn=<MseLossBackward0>)\n",
      "3325 tensor(27279.5762, grad_fn=<MseLossBackward0>)\n",
      "3326 tensor(27267.4570, grad_fn=<MseLossBackward0>)\n",
      "3327 tensor(27255.3516, grad_fn=<MseLossBackward0>)\n",
      "3328 tensor(27243.2578, grad_fn=<MseLossBackward0>)\n",
      "3329 tensor(27231.1738, grad_fn=<MseLossBackward0>)\n",
      "3330 tensor(27219.0977, grad_fn=<MseLossBackward0>)\n",
      "3331 tensor(27207.0352, grad_fn=<MseLossBackward0>)\n",
      "3332 tensor(27194.9824, grad_fn=<MseLossBackward0>)\n",
      "3333 tensor(27182.9395, grad_fn=<MseLossBackward0>)\n",
      "3334 tensor(27170.9082, grad_fn=<MseLossBackward0>)\n",
      "3335 tensor(27158.8867, grad_fn=<MseLossBackward0>)\n",
      "3336 tensor(27146.8809, grad_fn=<MseLossBackward0>)\n",
      "3337 tensor(27134.8809, grad_fn=<MseLossBackward0>)\n",
      "3338 tensor(27122.8945, grad_fn=<MseLossBackward0>)\n",
      "3339 tensor(27110.9180, grad_fn=<MseLossBackward0>)\n",
      "3340 tensor(27098.9512, grad_fn=<MseLossBackward0>)\n",
      "3341 tensor(27086.9980, grad_fn=<MseLossBackward0>)\n",
      "3342 tensor(27075.0547, grad_fn=<MseLossBackward0>)\n",
      "3343 tensor(27063.1191, grad_fn=<MseLossBackward0>)\n",
      "3344 tensor(27051.1953, grad_fn=<MseLossBackward0>)\n",
      "3345 tensor(27039.2832, grad_fn=<MseLossBackward0>)\n",
      "3346 tensor(27027.3867, grad_fn=<MseLossBackward0>)\n",
      "3347 tensor(27015.4922, grad_fn=<MseLossBackward0>)\n",
      "3348 tensor(27003.6113, grad_fn=<MseLossBackward0>)\n",
      "3349 tensor(26991.7422, grad_fn=<MseLossBackward0>)\n",
      "3350 tensor(26979.8867, grad_fn=<MseLossBackward0>)\n",
      "3351 tensor(26968.0410, grad_fn=<MseLossBackward0>)\n",
      "3352 tensor(26956.2031, grad_fn=<MseLossBackward0>)\n",
      "3353 tensor(26944.3750, grad_fn=<MseLossBackward0>)\n",
      "3354 tensor(26932.5625, grad_fn=<MseLossBackward0>)\n",
      "3355 tensor(26920.7559, grad_fn=<MseLossBackward0>)\n",
      "3356 tensor(26908.9629, grad_fn=<MseLossBackward0>)\n",
      "3357 tensor(26897.1816, grad_fn=<MseLossBackward0>)\n",
      "3358 tensor(26885.4082, grad_fn=<MseLossBackward0>)\n",
      "3359 tensor(26873.6484, grad_fn=<MseLossBackward0>)\n",
      "3360 tensor(26861.8984, grad_fn=<MseLossBackward0>)\n",
      "3361 tensor(26850.1621, grad_fn=<MseLossBackward0>)\n",
      "3362 tensor(26838.4297, grad_fn=<MseLossBackward0>)\n",
      "3363 tensor(26826.7129, grad_fn=<MseLossBackward0>)\n",
      "3364 tensor(26815.0059, grad_fn=<MseLossBackward0>)\n",
      "3365 tensor(26803.3086, grad_fn=<MseLossBackward0>)\n",
      "3366 tensor(26791.6250, grad_fn=<MseLossBackward0>)\n",
      "3367 tensor(26779.9512, grad_fn=<MseLossBackward0>)\n",
      "3368 tensor(26768.2852, grad_fn=<MseLossBackward0>)\n",
      "3369 tensor(26756.6328, grad_fn=<MseLossBackward0>)\n",
      "3370 tensor(26744.9883, grad_fn=<MseLossBackward0>)\n",
      "3371 tensor(26733.3574, grad_fn=<MseLossBackward0>)\n",
      "3372 tensor(26721.7363, grad_fn=<MseLossBackward0>)\n",
      "3373 tensor(26710.1270, grad_fn=<MseLossBackward0>)\n",
      "3374 tensor(26698.5273, grad_fn=<MseLossBackward0>)\n",
      "3375 tensor(26686.9395, grad_fn=<MseLossBackward0>)\n",
      "3376 tensor(26675.3633, grad_fn=<MseLossBackward0>)\n",
      "3377 tensor(26663.7949, grad_fn=<MseLossBackward0>)\n",
      "3378 tensor(26652.2344, grad_fn=<MseLossBackward0>)\n",
      "3379 tensor(26640.6914, grad_fn=<MseLossBackward0>)\n",
      "3380 tensor(26629.1582, grad_fn=<MseLossBackward0>)\n",
      "3381 tensor(26617.6328, grad_fn=<MseLossBackward0>)\n",
      "3382 tensor(26606.1211, grad_fn=<MseLossBackward0>)\n",
      "3383 tensor(26594.6211, grad_fn=<MseLossBackward0>)\n",
      "3384 tensor(26583.1270, grad_fn=<MseLossBackward0>)\n",
      "3385 tensor(26571.6465, grad_fn=<MseLossBackward0>)\n",
      "3386 tensor(26560.1777, grad_fn=<MseLossBackward0>)\n",
      "3387 tensor(26548.7168, grad_fn=<MseLossBackward0>)\n",
      "3388 tensor(26537.2656, grad_fn=<MseLossBackward0>)\n",
      "3389 tensor(26525.8301, grad_fn=<MseLossBackward0>)\n",
      "3390 tensor(26514.4023, grad_fn=<MseLossBackward0>)\n",
      "3391 tensor(26502.9863, grad_fn=<MseLossBackward0>)\n",
      "3392 tensor(26491.5801, grad_fn=<MseLossBackward0>)\n",
      "3393 tensor(26480.1836, grad_fn=<MseLossBackward0>)\n",
      "3394 tensor(26468.7988, grad_fn=<MseLossBackward0>)\n",
      "3395 tensor(26457.4277, grad_fn=<MseLossBackward0>)\n",
      "3396 tensor(26446.0625, grad_fn=<MseLossBackward0>)\n",
      "3397 tensor(26434.7109, grad_fn=<MseLossBackward0>)\n",
      "3398 tensor(26423.3691, grad_fn=<MseLossBackward0>)\n",
      "3399 tensor(26412.0391, grad_fn=<MseLossBackward0>)\n",
      "3400 tensor(26400.7207, grad_fn=<MseLossBackward0>)\n",
      "3401 tensor(26389.4121, grad_fn=<MseLossBackward0>)\n",
      "3402 tensor(26378.1094, grad_fn=<MseLossBackward0>)\n",
      "3403 tensor(26366.8242, grad_fn=<MseLossBackward0>)\n",
      "3404 tensor(26355.5469, grad_fn=<MseLossBackward0>)\n",
      "3405 tensor(26344.2812, grad_fn=<MseLossBackward0>)\n",
      "3406 tensor(26333.0254, grad_fn=<MseLossBackward0>)\n",
      "3407 tensor(26321.7832, grad_fn=<MseLossBackward0>)\n",
      "3408 tensor(26310.5449, grad_fn=<MseLossBackward0>)\n",
      "3409 tensor(26299.3242, grad_fn=<MseLossBackward0>)\n",
      "3410 tensor(26288.1094, grad_fn=<MseLossBackward0>)\n",
      "3411 tensor(26276.9121, grad_fn=<MseLossBackward0>)\n",
      "3412 tensor(26265.7148, grad_fn=<MseLossBackward0>)\n",
      "3413 tensor(26254.5371, grad_fn=<MseLossBackward0>)\n",
      "3414 tensor(26243.3672, grad_fn=<MseLossBackward0>)\n",
      "3415 tensor(26232.2070, grad_fn=<MseLossBackward0>)\n",
      "3416 tensor(26221.0586, grad_fn=<MseLossBackward0>)\n",
      "3417 tensor(26209.9199, grad_fn=<MseLossBackward0>)\n",
      "3418 tensor(26198.7949, grad_fn=<MseLossBackward0>)\n",
      "3419 tensor(26187.6816, grad_fn=<MseLossBackward0>)\n",
      "3420 tensor(26176.5742, grad_fn=<MseLossBackward0>)\n",
      "3421 tensor(26165.4785, grad_fn=<MseLossBackward0>)\n",
      "3422 tensor(26154.3945, grad_fn=<MseLossBackward0>)\n",
      "3423 tensor(26143.3203, grad_fn=<MseLossBackward0>)\n",
      "3424 tensor(26132.2559, grad_fn=<MseLossBackward0>)\n",
      "3425 tensor(26121.2051, grad_fn=<MseLossBackward0>)\n",
      "3426 tensor(26110.1660, grad_fn=<MseLossBackward0>)\n",
      "3427 tensor(26099.1348, grad_fn=<MseLossBackward0>)\n",
      "3428 tensor(26088.1133, grad_fn=<MseLossBackward0>)\n",
      "3429 tensor(26077.1035, grad_fn=<MseLossBackward0>)\n",
      "3430 tensor(26066.1035, grad_fn=<MseLossBackward0>)\n",
      "3431 tensor(26055.1172, grad_fn=<MseLossBackward0>)\n",
      "3432 tensor(26044.1406, grad_fn=<MseLossBackward0>)\n",
      "3433 tensor(26033.1738, grad_fn=<MseLossBackward0>)\n",
      "3434 tensor(26022.2168, grad_fn=<MseLossBackward0>)\n",
      "3435 tensor(26011.2715, grad_fn=<MseLossBackward0>)\n",
      "3436 tensor(26000.3359, grad_fn=<MseLossBackward0>)\n",
      "3437 tensor(25989.4141, grad_fn=<MseLossBackward0>)\n",
      "3438 tensor(25978.5020, grad_fn=<MseLossBackward0>)\n",
      "3439 tensor(25967.5996, grad_fn=<MseLossBackward0>)\n",
      "3440 tensor(25956.7051, grad_fn=<MseLossBackward0>)\n",
      "3441 tensor(25945.8242, grad_fn=<MseLossBackward0>)\n",
      "3442 tensor(25934.9531, grad_fn=<MseLossBackward0>)\n",
      "3443 tensor(25924.0957, grad_fn=<MseLossBackward0>)\n",
      "3444 tensor(25913.2461, grad_fn=<MseLossBackward0>)\n",
      "3445 tensor(25902.4082, grad_fn=<MseLossBackward0>)\n",
      "3446 tensor(25891.5781, grad_fn=<MseLossBackward0>)\n",
      "3447 tensor(25880.7598, grad_fn=<MseLossBackward0>)\n",
      "3448 tensor(25869.9531, grad_fn=<MseLossBackward0>)\n",
      "3449 tensor(25859.1562, grad_fn=<MseLossBackward0>)\n",
      "3450 tensor(25848.3730, grad_fn=<MseLossBackward0>)\n",
      "3451 tensor(25837.5977, grad_fn=<MseLossBackward0>)\n",
      "3452 tensor(25826.8359, grad_fn=<MseLossBackward0>)\n",
      "3453 tensor(25816.0801, grad_fn=<MseLossBackward0>)\n",
      "3454 tensor(25805.3379, grad_fn=<MseLossBackward0>)\n",
      "3455 tensor(25794.6055, grad_fn=<MseLossBackward0>)\n",
      "3456 tensor(25783.8828, grad_fn=<MseLossBackward0>)\n",
      "3457 tensor(25773.1738, grad_fn=<MseLossBackward0>)\n",
      "3458 tensor(25762.4707, grad_fn=<MseLossBackward0>)\n",
      "3459 tensor(25751.7812, grad_fn=<MseLossBackward0>)\n",
      "3460 tensor(25741.1035, grad_fn=<MseLossBackward0>)\n",
      "3461 tensor(25730.4375, grad_fn=<MseLossBackward0>)\n",
      "3462 tensor(25719.7773, grad_fn=<MseLossBackward0>)\n",
      "3463 tensor(25709.1289, grad_fn=<MseLossBackward0>)\n",
      "3464 tensor(25698.4922, grad_fn=<MseLossBackward0>)\n",
      "3465 tensor(25687.8672, grad_fn=<MseLossBackward0>)\n",
      "3466 tensor(25677.2500, grad_fn=<MseLossBackward0>)\n",
      "3467 tensor(25666.6465, grad_fn=<MseLossBackward0>)\n",
      "3468 tensor(25656.0508, grad_fn=<MseLossBackward0>)\n",
      "3469 tensor(25645.4707, grad_fn=<MseLossBackward0>)\n",
      "3470 tensor(25634.8965, grad_fn=<MseLossBackward0>)\n",
      "3471 tensor(25624.3340, grad_fn=<MseLossBackward0>)\n",
      "3472 tensor(25613.7852, grad_fn=<MseLossBackward0>)\n",
      "3473 tensor(25603.2422, grad_fn=<MseLossBackward0>)\n",
      "3474 tensor(25592.7109, grad_fn=<MseLossBackward0>)\n",
      "3475 tensor(25582.1914, grad_fn=<MseLossBackward0>)\n",
      "3476 tensor(25571.6797, grad_fn=<MseLossBackward0>)\n",
      "3477 tensor(25561.1836, grad_fn=<MseLossBackward0>)\n",
      "3478 tensor(25550.6934, grad_fn=<MseLossBackward0>)\n",
      "3479 tensor(25540.2148, grad_fn=<MseLossBackward0>)\n",
      "3480 tensor(25529.7461, grad_fn=<MseLossBackward0>)\n",
      "3481 tensor(25519.2930, grad_fn=<MseLossBackward0>)\n",
      "3482 tensor(25508.8457, grad_fn=<MseLossBackward0>)\n",
      "3483 tensor(25498.4102, grad_fn=<MseLossBackward0>)\n",
      "3484 tensor(25487.9844, grad_fn=<MseLossBackward0>)\n",
      "3485 tensor(25477.5723, grad_fn=<MseLossBackward0>)\n",
      "3486 tensor(25467.1680, grad_fn=<MseLossBackward0>)\n",
      "3487 tensor(25456.7734, grad_fn=<MseLossBackward0>)\n",
      "3488 tensor(25446.3926, grad_fn=<MseLossBackward0>)\n",
      "3489 tensor(25436.0195, grad_fn=<MseLossBackward0>)\n",
      "3490 tensor(25425.6582, grad_fn=<MseLossBackward0>)\n",
      "3491 tensor(25415.3066, grad_fn=<MseLossBackward0>)\n",
      "3492 tensor(25404.9668, grad_fn=<MseLossBackward0>)\n",
      "3493 tensor(25394.6387, grad_fn=<MseLossBackward0>)\n",
      "3494 tensor(25384.3184, grad_fn=<MseLossBackward0>)\n",
      "3495 tensor(25374.0078, grad_fn=<MseLossBackward0>)\n",
      "3496 tensor(25363.7109, grad_fn=<MseLossBackward0>)\n",
      "3497 tensor(25353.4219, grad_fn=<MseLossBackward0>)\n",
      "3498 tensor(25343.1465, grad_fn=<MseLossBackward0>)\n",
      "3499 tensor(25332.8809, grad_fn=<MseLossBackward0>)\n",
      "3500 tensor(25322.6230, grad_fn=<MseLossBackward0>)\n",
      "3501 tensor(25312.3789, grad_fn=<MseLossBackward0>)\n",
      "3502 tensor(25302.1406, grad_fn=<MseLossBackward0>)\n",
      "3503 tensor(25291.9199, grad_fn=<MseLossBackward0>)\n",
      "3504 tensor(25281.7031, grad_fn=<MseLossBackward0>)\n",
      "3505 tensor(25271.5000, grad_fn=<MseLossBackward0>)\n",
      "3506 tensor(25261.3066, grad_fn=<MseLossBackward0>)\n",
      "3507 tensor(25251.1250, grad_fn=<MseLossBackward0>)\n",
      "3508 tensor(25240.9531, grad_fn=<MseLossBackward0>)\n",
      "3509 tensor(25230.7910, grad_fn=<MseLossBackward0>)\n",
      "3510 tensor(25220.6387, grad_fn=<MseLossBackward0>)\n",
      "3511 tensor(25210.5000, grad_fn=<MseLossBackward0>)\n",
      "3512 tensor(25200.3691, grad_fn=<MseLossBackward0>)\n",
      "3513 tensor(25190.2520, grad_fn=<MseLossBackward0>)\n",
      "3514 tensor(25180.1406, grad_fn=<MseLossBackward0>)\n",
      "3515 tensor(25170.0430, grad_fn=<MseLossBackward0>)\n",
      "3516 tensor(25159.9531, grad_fn=<MseLossBackward0>)\n",
      "3517 tensor(25149.8770, grad_fn=<MseLossBackward0>)\n",
      "3518 tensor(25139.8086, grad_fn=<MseLossBackward0>)\n",
      "3519 tensor(25129.7520, grad_fn=<MseLossBackward0>)\n",
      "3520 tensor(25119.7070, grad_fn=<MseLossBackward0>)\n",
      "3521 tensor(25109.6719, grad_fn=<MseLossBackward0>)\n",
      "3522 tensor(25099.6445, grad_fn=<MseLossBackward0>)\n",
      "3523 tensor(25089.6309, grad_fn=<MseLossBackward0>)\n",
      "3524 tensor(25079.6270, grad_fn=<MseLossBackward0>)\n",
      "3525 tensor(25069.6309, grad_fn=<MseLossBackward0>)\n",
      "3526 tensor(25059.6484, grad_fn=<MseLossBackward0>)\n",
      "3527 tensor(25049.6777, grad_fn=<MseLossBackward0>)\n",
      "3528 tensor(25039.7148, grad_fn=<MseLossBackward0>)\n",
      "3529 tensor(25029.7637, grad_fn=<MseLossBackward0>)\n",
      "3530 tensor(25019.8203, grad_fn=<MseLossBackward0>)\n",
      "3531 tensor(25009.8887, grad_fn=<MseLossBackward0>)\n",
      "3532 tensor(24999.9707, grad_fn=<MseLossBackward0>)\n",
      "3533 tensor(24990.0586, grad_fn=<MseLossBackward0>)\n",
      "3534 tensor(24980.1602, grad_fn=<MseLossBackward0>)\n",
      "3535 tensor(24970.2676, grad_fn=<MseLossBackward0>)\n",
      "3536 tensor(24960.3906, grad_fn=<MseLossBackward0>)\n",
      "3537 tensor(24950.5215, grad_fn=<MseLossBackward0>)\n",
      "3538 tensor(24940.6641, grad_fn=<MseLossBackward0>)\n",
      "3539 tensor(24930.8145, grad_fn=<MseLossBackward0>)\n",
      "3540 tensor(24920.9785, grad_fn=<MseLossBackward0>)\n",
      "3541 tensor(24911.1523, grad_fn=<MseLossBackward0>)\n",
      "3542 tensor(24901.3340, grad_fn=<MseLossBackward0>)\n",
      "3543 tensor(24891.5293, grad_fn=<MseLossBackward0>)\n",
      "3544 tensor(24881.7344, grad_fn=<MseLossBackward0>)\n",
      "3545 tensor(24871.9512, grad_fn=<MseLossBackward0>)\n",
      "3546 tensor(24862.1738, grad_fn=<MseLossBackward0>)\n",
      "3547 tensor(24852.4102, grad_fn=<MseLossBackward0>)\n",
      "3548 tensor(24842.6562, grad_fn=<MseLossBackward0>)\n",
      "3549 tensor(24832.9121, grad_fn=<MseLossBackward0>)\n",
      "3550 tensor(24823.1777, grad_fn=<MseLossBackward0>)\n",
      "3551 tensor(24813.4570, grad_fn=<MseLossBackward0>)\n",
      "3552 tensor(24803.7422, grad_fn=<MseLossBackward0>)\n",
      "3553 tensor(24794.0410, grad_fn=<MseLossBackward0>)\n",
      "3554 tensor(24784.3496, grad_fn=<MseLossBackward0>)\n",
      "3555 tensor(24774.6680, grad_fn=<MseLossBackward0>)\n",
      "3556 tensor(24764.9980, grad_fn=<MseLossBackward0>)\n",
      "3557 tensor(24755.3379, grad_fn=<MseLossBackward0>)\n",
      "3558 tensor(24745.6875, grad_fn=<MseLossBackward0>)\n",
      "3559 tensor(24736.0449, grad_fn=<MseLossBackward0>)\n",
      "3560 tensor(24726.4160, grad_fn=<MseLossBackward0>)\n",
      "3561 tensor(24716.7969, grad_fn=<MseLossBackward0>)\n",
      "3562 tensor(24707.1875, grad_fn=<MseLossBackward0>)\n",
      "3563 tensor(24697.5879, grad_fn=<MseLossBackward0>)\n",
      "3564 tensor(24688.0020, grad_fn=<MseLossBackward0>)\n",
      "3565 tensor(24678.4219, grad_fn=<MseLossBackward0>)\n",
      "3566 tensor(24668.8535, grad_fn=<MseLossBackward0>)\n",
      "3567 tensor(24659.2988, grad_fn=<MseLossBackward0>)\n",
      "3568 tensor(24649.7500, grad_fn=<MseLossBackward0>)\n",
      "3569 tensor(24640.2148, grad_fn=<MseLossBackward0>)\n",
      "3570 tensor(24630.6875, grad_fn=<MseLossBackward0>)\n",
      "3571 tensor(24621.1719, grad_fn=<MseLossBackward0>)\n",
      "3572 tensor(24611.6641, grad_fn=<MseLossBackward0>)\n",
      "3573 tensor(24602.1719, grad_fn=<MseLossBackward0>)\n",
      "3574 tensor(24592.6855, grad_fn=<MseLossBackward0>)\n",
      "3575 tensor(24583.2109, grad_fn=<MseLossBackward0>)\n",
      "3576 tensor(24573.7461, grad_fn=<MseLossBackward0>)\n",
      "3577 tensor(24564.2949, grad_fn=<MseLossBackward0>)\n",
      "3578 tensor(24554.8477, grad_fn=<MseLossBackward0>)\n",
      "3579 tensor(24545.4180, grad_fn=<MseLossBackward0>)\n",
      "3580 tensor(24535.9922, grad_fn=<MseLossBackward0>)\n",
      "3581 tensor(24526.5801, grad_fn=<MseLossBackward0>)\n",
      "3582 tensor(24517.1738, grad_fn=<MseLossBackward0>)\n",
      "3583 tensor(24507.7832, grad_fn=<MseLossBackward0>)\n",
      "3584 tensor(24498.4023, grad_fn=<MseLossBackward0>)\n",
      "3585 tensor(24489.0312, grad_fn=<MseLossBackward0>)\n",
      "3586 tensor(24479.6680, grad_fn=<MseLossBackward0>)\n",
      "3587 tensor(24470.3164, grad_fn=<MseLossBackward0>)\n",
      "3588 tensor(24460.9746, grad_fn=<MseLossBackward0>)\n",
      "3589 tensor(24451.6445, grad_fn=<MseLossBackward0>)\n",
      "3590 tensor(24442.3242, grad_fn=<MseLossBackward0>)\n",
      "3591 tensor(24433.0137, grad_fn=<MseLossBackward0>)\n",
      "3592 tensor(24423.7148, grad_fn=<MseLossBackward0>)\n",
      "3593 tensor(24414.4258, grad_fn=<MseLossBackward0>)\n",
      "3594 tensor(24405.1445, grad_fn=<MseLossBackward0>)\n",
      "3595 tensor(24395.8750, grad_fn=<MseLossBackward0>)\n",
      "3596 tensor(24386.6172, grad_fn=<MseLossBackward0>)\n",
      "3597 tensor(24377.3672, grad_fn=<MseLossBackward0>)\n",
      "3598 tensor(24368.1270, grad_fn=<MseLossBackward0>)\n",
      "3599 tensor(24358.8984, grad_fn=<MseLossBackward0>)\n",
      "3600 tensor(24349.6836, grad_fn=<MseLossBackward0>)\n",
      "3601 tensor(24340.4727, grad_fn=<MseLossBackward0>)\n",
      "3602 tensor(24331.2754, grad_fn=<MseLossBackward0>)\n",
      "3603 tensor(24322.0879, grad_fn=<MseLossBackward0>)\n",
      "3604 tensor(24312.9102, grad_fn=<MseLossBackward0>)\n",
      "3605 tensor(24303.7422, grad_fn=<MseLossBackward0>)\n",
      "3606 tensor(24294.5879, grad_fn=<MseLossBackward0>)\n",
      "3607 tensor(24285.4375, grad_fn=<MseLossBackward0>)\n",
      "3608 tensor(24276.3027, grad_fn=<MseLossBackward0>)\n",
      "3609 tensor(24267.1738, grad_fn=<MseLossBackward0>)\n",
      "3610 tensor(24258.0605, grad_fn=<MseLossBackward0>)\n",
      "3611 tensor(24248.9531, grad_fn=<MseLossBackward0>)\n",
      "3612 tensor(24239.8594, grad_fn=<MseLossBackward0>)\n",
      "3613 tensor(24230.7715, grad_fn=<MseLossBackward0>)\n",
      "3614 tensor(24221.6953, grad_fn=<MseLossBackward0>)\n",
      "3615 tensor(24212.6309, grad_fn=<MseLossBackward0>)\n",
      "3616 tensor(24203.5742, grad_fn=<MseLossBackward0>)\n",
      "3617 tensor(24194.5293, grad_fn=<MseLossBackward0>)\n",
      "3618 tensor(24185.4961, grad_fn=<MseLossBackward0>)\n",
      "3619 tensor(24176.4707, grad_fn=<MseLossBackward0>)\n",
      "3620 tensor(24167.4531, grad_fn=<MseLossBackward0>)\n",
      "3621 tensor(24158.4492, grad_fn=<MseLossBackward0>)\n",
      "3622 tensor(24149.4551, grad_fn=<MseLossBackward0>)\n",
      "3623 tensor(24140.4727, grad_fn=<MseLossBackward0>)\n",
      "3624 tensor(24131.4961, grad_fn=<MseLossBackward0>)\n",
      "3625 tensor(24122.5312, grad_fn=<MseLossBackward0>)\n",
      "3626 tensor(24113.5801, grad_fn=<MseLossBackward0>)\n",
      "3627 tensor(24104.6328, grad_fn=<MseLossBackward0>)\n",
      "3628 tensor(24095.7012, grad_fn=<MseLossBackward0>)\n",
      "3629 tensor(24086.7773, grad_fn=<MseLossBackward0>)\n",
      "3630 tensor(24077.8652, grad_fn=<MseLossBackward0>)\n",
      "3631 tensor(24068.9629, grad_fn=<MseLossBackward0>)\n",
      "3632 tensor(24060.0684, grad_fn=<MseLossBackward0>)\n",
      "3633 tensor(24051.1836, grad_fn=<MseLossBackward0>)\n",
      "3634 tensor(24042.3125, grad_fn=<MseLossBackward0>)\n",
      "3635 tensor(24033.4453, grad_fn=<MseLossBackward0>)\n",
      "3636 tensor(24024.5938, grad_fn=<MseLossBackward0>)\n",
      "3637 tensor(24015.7520, grad_fn=<MseLossBackward0>)\n",
      "3638 tensor(24006.9180, grad_fn=<MseLossBackward0>)\n",
      "3639 tensor(23998.0977, grad_fn=<MseLossBackward0>)\n",
      "3640 tensor(23989.2832, grad_fn=<MseLossBackward0>)\n",
      "3641 tensor(23980.4805, grad_fn=<MseLossBackward0>)\n",
      "3642 tensor(23971.6875, grad_fn=<MseLossBackward0>)\n",
      "3643 tensor(23962.9043, grad_fn=<MseLossBackward0>)\n",
      "3644 tensor(23954.1328, grad_fn=<MseLossBackward0>)\n",
      "3645 tensor(23945.3711, grad_fn=<MseLossBackward0>)\n",
      "3646 tensor(23936.6172, grad_fn=<MseLossBackward0>)\n",
      "3647 tensor(23927.8750, grad_fn=<MseLossBackward0>)\n",
      "3648 tensor(23919.1406, grad_fn=<MseLossBackward0>)\n",
      "3649 tensor(23910.4180, grad_fn=<MseLossBackward0>)\n",
      "3650 tensor(23901.7070, grad_fn=<MseLossBackward0>)\n",
      "3651 tensor(23893.0059, grad_fn=<MseLossBackward0>)\n",
      "3652 tensor(23884.3125, grad_fn=<MseLossBackward0>)\n",
      "3653 tensor(23875.6328, grad_fn=<MseLossBackward0>)\n",
      "3654 tensor(23866.9590, grad_fn=<MseLossBackward0>)\n",
      "3655 tensor(23858.2969, grad_fn=<MseLossBackward0>)\n",
      "3656 tensor(23849.6445, grad_fn=<MseLossBackward0>)\n",
      "3657 tensor(23841.0039, grad_fn=<MseLossBackward0>)\n",
      "3658 tensor(23832.3691, grad_fn=<MseLossBackward0>)\n",
      "3659 tensor(23823.7461, grad_fn=<MseLossBackward0>)\n",
      "3660 tensor(23815.1367, grad_fn=<MseLossBackward0>)\n",
      "3661 tensor(23806.5312, grad_fn=<MseLossBackward0>)\n",
      "3662 tensor(23797.9395, grad_fn=<MseLossBackward0>)\n",
      "3663 tensor(23789.3574, grad_fn=<MseLossBackward0>)\n",
      "3664 tensor(23780.7832, grad_fn=<MseLossBackward0>)\n",
      "3665 tensor(23772.2227, grad_fn=<MseLossBackward0>)\n",
      "3666 tensor(23763.6680, grad_fn=<MseLossBackward0>)\n",
      "3667 tensor(23755.1270, grad_fn=<MseLossBackward0>)\n",
      "3668 tensor(23746.5938, grad_fn=<MseLossBackward0>)\n",
      "3669 tensor(23738.0703, grad_fn=<MseLossBackward0>)\n",
      "3670 tensor(23729.5586, grad_fn=<MseLossBackward0>)\n",
      "3671 tensor(23721.0547, grad_fn=<MseLossBackward0>)\n",
      "3672 tensor(23712.5605, grad_fn=<MseLossBackward0>)\n",
      "3673 tensor(23704.0781, grad_fn=<MseLossBackward0>)\n",
      "3674 tensor(23695.6055, grad_fn=<MseLossBackward0>)\n",
      "3675 tensor(23687.1445, grad_fn=<MseLossBackward0>)\n",
      "3676 tensor(23678.6895, grad_fn=<MseLossBackward0>)\n",
      "3677 tensor(23670.2461, grad_fn=<MseLossBackward0>)\n",
      "3678 tensor(23661.8125, grad_fn=<MseLossBackward0>)\n",
      "3679 tensor(23653.3887, grad_fn=<MseLossBackward0>)\n",
      "3680 tensor(23644.9766, grad_fn=<MseLossBackward0>)\n",
      "3681 tensor(23636.5723, grad_fn=<MseLossBackward0>)\n",
      "3682 tensor(23628.1758, grad_fn=<MseLossBackward0>)\n",
      "3683 tensor(23619.7949, grad_fn=<MseLossBackward0>)\n",
      "3684 tensor(23611.4199, grad_fn=<MseLossBackward0>)\n",
      "3685 tensor(23603.0566, grad_fn=<MseLossBackward0>)\n",
      "3686 tensor(23594.7012, grad_fn=<MseLossBackward0>)\n",
      "3687 tensor(23586.3574, grad_fn=<MseLossBackward0>)\n",
      "3688 tensor(23578.0215, grad_fn=<MseLossBackward0>)\n",
      "3689 tensor(23569.6973, grad_fn=<MseLossBackward0>)\n",
      "3690 tensor(23561.3809, grad_fn=<MseLossBackward0>)\n",
      "3691 tensor(23553.0762, grad_fn=<MseLossBackward0>)\n",
      "3692 tensor(23544.7832, grad_fn=<MseLossBackward0>)\n",
      "3693 tensor(23536.4922, grad_fn=<MseLossBackward0>)\n",
      "3694 tensor(23528.2188, grad_fn=<MseLossBackward0>)\n",
      "3695 tensor(23519.9551, grad_fn=<MseLossBackward0>)\n",
      "3696 tensor(23511.6973, grad_fn=<MseLossBackward0>)\n",
      "3697 tensor(23503.4531, grad_fn=<MseLossBackward0>)\n",
      "3698 tensor(23495.2168, grad_fn=<MseLossBackward0>)\n",
      "3699 tensor(23486.9883, grad_fn=<MseLossBackward0>)\n",
      "3700 tensor(23478.7734, grad_fn=<MseLossBackward0>)\n",
      "3701 tensor(23470.5645, grad_fn=<MseLossBackward0>)\n",
      "3702 tensor(23462.3672, grad_fn=<MseLossBackward0>)\n",
      "3703 tensor(23454.1797, grad_fn=<MseLossBackward0>)\n",
      "3704 tensor(23446.0020, grad_fn=<MseLossBackward0>)\n",
      "3705 tensor(23437.8340, grad_fn=<MseLossBackward0>)\n",
      "3706 tensor(23429.6758, grad_fn=<MseLossBackward0>)\n",
      "3707 tensor(23421.5273, grad_fn=<MseLossBackward0>)\n",
      "3708 tensor(23413.3887, grad_fn=<MseLossBackward0>)\n",
      "3709 tensor(23405.2617, grad_fn=<MseLossBackward0>)\n",
      "3710 tensor(23397.1426, grad_fn=<MseLossBackward0>)\n",
      "3711 tensor(23389.0312, grad_fn=<MseLossBackward0>)\n",
      "3712 tensor(23380.9355, grad_fn=<MseLossBackward0>)\n",
      "3713 tensor(23372.8438, grad_fn=<MseLossBackward0>)\n",
      "3714 tensor(23364.7637, grad_fn=<MseLossBackward0>)\n",
      "3715 tensor(23356.6953, grad_fn=<MseLossBackward0>)\n",
      "3716 tensor(23348.6309, grad_fn=<MseLossBackward0>)\n",
      "3717 tensor(23340.5820, grad_fn=<MseLossBackward0>)\n",
      "3718 tensor(23332.5410, grad_fn=<MseLossBackward0>)\n",
      "3719 tensor(23324.5098, grad_fn=<MseLossBackward0>)\n",
      "3720 tensor(23316.4883, grad_fn=<MseLossBackward0>)\n",
      "3721 tensor(23308.4785, grad_fn=<MseLossBackward0>)\n",
      "3722 tensor(23300.4766, grad_fn=<MseLossBackward0>)\n",
      "3723 tensor(23292.4805, grad_fn=<MseLossBackward0>)\n",
      "3724 tensor(23284.5000, grad_fn=<MseLossBackward0>)\n",
      "3725 tensor(23276.5273, grad_fn=<MseLossBackward0>)\n",
      "3726 tensor(23268.5625, grad_fn=<MseLossBackward0>)\n",
      "3727 tensor(23260.6094, grad_fn=<MseLossBackward0>)\n",
      "3728 tensor(23252.6621, grad_fn=<MseLossBackward0>)\n",
      "3729 tensor(23244.7305, grad_fn=<MseLossBackward0>)\n",
      "3730 tensor(23236.8066, grad_fn=<MseLossBackward0>)\n",
      "3731 tensor(23228.8906, grad_fn=<MseLossBackward0>)\n",
      "3732 tensor(23220.9844, grad_fn=<MseLossBackward0>)\n",
      "3733 tensor(23213.0898, grad_fn=<MseLossBackward0>)\n",
      "3734 tensor(23205.2012, grad_fn=<MseLossBackward0>)\n",
      "3735 tensor(23197.3281, grad_fn=<MseLossBackward0>)\n",
      "3736 tensor(23189.4590, grad_fn=<MseLossBackward0>)\n",
      "3737 tensor(23181.6035, grad_fn=<MseLossBackward0>)\n",
      "3738 tensor(23173.7539, grad_fn=<MseLossBackward0>)\n",
      "3739 tensor(23165.9160, grad_fn=<MseLossBackward0>)\n",
      "3740 tensor(23158.0859, grad_fn=<MseLossBackward0>)\n",
      "3741 tensor(23150.2676, grad_fn=<MseLossBackward0>)\n",
      "3742 tensor(23142.4590, grad_fn=<MseLossBackward0>)\n",
      "3743 tensor(23134.6602, grad_fn=<MseLossBackward0>)\n",
      "3744 tensor(23126.8711, grad_fn=<MseLossBackward0>)\n",
      "3745 tensor(23119.0898, grad_fn=<MseLossBackward0>)\n",
      "3746 tensor(23111.3184, grad_fn=<MseLossBackward0>)\n",
      "3747 tensor(23103.5586, grad_fn=<MseLossBackward0>)\n",
      "3748 tensor(23095.8066, grad_fn=<MseLossBackward0>)\n",
      "3749 tensor(23088.0625, grad_fn=<MseLossBackward0>)\n",
      "3750 tensor(23080.3301, grad_fn=<MseLossBackward0>)\n",
      "3751 tensor(23072.6055, grad_fn=<MseLossBackward0>)\n",
      "3752 tensor(23064.8926, grad_fn=<MseLossBackward0>)\n",
      "3753 tensor(23057.1895, grad_fn=<MseLossBackward0>)\n",
      "3754 tensor(23049.4961, grad_fn=<MseLossBackward0>)\n",
      "3755 tensor(23041.8105, grad_fn=<MseLossBackward0>)\n",
      "3756 tensor(23034.1348, grad_fn=<MseLossBackward0>)\n",
      "3757 tensor(23026.4688, grad_fn=<MseLossBackward0>)\n",
      "3758 tensor(23018.8145, grad_fn=<MseLossBackward0>)\n",
      "3759 tensor(23011.1680, grad_fn=<MseLossBackward0>)\n",
      "3760 tensor(23003.5293, grad_fn=<MseLossBackward0>)\n",
      "3761 tensor(22995.9023, grad_fn=<MseLossBackward0>)\n",
      "3762 tensor(22988.2852, grad_fn=<MseLossBackward0>)\n",
      "3763 tensor(22980.6777, grad_fn=<MseLossBackward0>)\n",
      "3764 tensor(22973.0762, grad_fn=<MseLossBackward0>)\n",
      "3765 tensor(22965.4844, grad_fn=<MseLossBackward0>)\n",
      "3766 tensor(22957.9043, grad_fn=<MseLossBackward0>)\n",
      "3767 tensor(22950.3379, grad_fn=<MseLossBackward0>)\n",
      "3768 tensor(22942.7754, grad_fn=<MseLossBackward0>)\n",
      "3769 tensor(22935.2246, grad_fn=<MseLossBackward0>)\n",
      "3770 tensor(22927.6797, grad_fn=<MseLossBackward0>)\n",
      "3771 tensor(22920.1465, grad_fn=<MseLossBackward0>)\n",
      "3772 tensor(22912.6230, grad_fn=<MseLossBackward0>)\n",
      "3773 tensor(22905.1094, grad_fn=<MseLossBackward0>)\n",
      "3774 tensor(22897.6055, grad_fn=<MseLossBackward0>)\n",
      "3775 tensor(22890.1094, grad_fn=<MseLossBackward0>)\n",
      "3776 tensor(22882.6250, grad_fn=<MseLossBackward0>)\n",
      "3777 tensor(22875.1504, grad_fn=<MseLossBackward0>)\n",
      "3778 tensor(22867.6816, grad_fn=<MseLossBackward0>)\n",
      "3779 tensor(22860.2227, grad_fn=<MseLossBackward0>)\n",
      "3780 tensor(22852.7754, grad_fn=<MseLossBackward0>)\n",
      "3781 tensor(22845.3359, grad_fn=<MseLossBackward0>)\n",
      "3782 tensor(22837.9062, grad_fn=<MseLossBackward0>)\n",
      "3783 tensor(22830.4902, grad_fn=<MseLossBackward0>)\n",
      "3784 tensor(22823.0762, grad_fn=<MseLossBackward0>)\n",
      "3785 tensor(22815.6758, grad_fn=<MseLossBackward0>)\n",
      "3786 tensor(22808.2832, grad_fn=<MseLossBackward0>)\n",
      "3787 tensor(22800.9043, grad_fn=<MseLossBackward0>)\n",
      "3788 tensor(22793.5312, grad_fn=<MseLossBackward0>)\n",
      "3789 tensor(22786.1680, grad_fn=<MseLossBackward0>)\n",
      "3790 tensor(22778.8145, grad_fn=<MseLossBackward0>)\n",
      "3791 tensor(22771.4707, grad_fn=<MseLossBackward0>)\n",
      "3792 tensor(22764.1328, grad_fn=<MseLossBackward0>)\n",
      "3793 tensor(22756.8086, grad_fn=<MseLossBackward0>)\n",
      "3794 tensor(22749.4883, grad_fn=<MseLossBackward0>)\n",
      "3795 tensor(22742.1836, grad_fn=<MseLossBackward0>)\n",
      "3796 tensor(22734.8848, grad_fn=<MseLossBackward0>)\n",
      "3797 tensor(22727.5957, grad_fn=<MseLossBackward0>)\n",
      "3798 tensor(22720.3164, grad_fn=<MseLossBackward0>)\n",
      "3799 tensor(22713.0469, grad_fn=<MseLossBackward0>)\n",
      "3800 tensor(22705.7832, grad_fn=<MseLossBackward0>)\n",
      "3801 tensor(22698.5332, grad_fn=<MseLossBackward0>)\n",
      "3802 tensor(22691.2891, grad_fn=<MseLossBackward0>)\n",
      "3803 tensor(22684.0586, grad_fn=<MseLossBackward0>)\n",
      "3804 tensor(22676.8340, grad_fn=<MseLossBackward0>)\n",
      "3805 tensor(22669.6211, grad_fn=<MseLossBackward0>)\n",
      "3806 tensor(22662.4180, grad_fn=<MseLossBackward0>)\n",
      "3807 tensor(22655.2188, grad_fn=<MseLossBackward0>)\n",
      "3808 tensor(22648.0332, grad_fn=<MseLossBackward0>)\n",
      "3809 tensor(22640.8555, grad_fn=<MseLossBackward0>)\n",
      "3810 tensor(22633.6875, grad_fn=<MseLossBackward0>)\n",
      "3811 tensor(22626.5273, grad_fn=<MseLossBackward0>)\n",
      "3812 tensor(22619.3809, grad_fn=<MseLossBackward0>)\n",
      "3813 tensor(22612.2383, grad_fn=<MseLossBackward0>)\n",
      "3814 tensor(22605.1074, grad_fn=<MseLossBackward0>)\n",
      "3815 tensor(22597.9863, grad_fn=<MseLossBackward0>)\n",
      "3816 tensor(22590.8730, grad_fn=<MseLossBackward0>)\n",
      "3817 tensor(22583.7695, grad_fn=<MseLossBackward0>)\n",
      "3818 tensor(22576.6738, grad_fn=<MseLossBackward0>)\n",
      "3819 tensor(22569.5898, grad_fn=<MseLossBackward0>)\n",
      "3820 tensor(22562.5156, grad_fn=<MseLossBackward0>)\n",
      "3821 tensor(22555.4473, grad_fn=<MseLossBackward0>)\n",
      "3822 tensor(22548.3887, grad_fn=<MseLossBackward0>)\n",
      "3823 tensor(22541.3398, grad_fn=<MseLossBackward0>)\n",
      "3824 tensor(22534.3008, grad_fn=<MseLossBackward0>)\n",
      "3825 tensor(22527.2715, grad_fn=<MseLossBackward0>)\n",
      "3826 tensor(22520.2520, grad_fn=<MseLossBackward0>)\n",
      "3827 tensor(22513.2363, grad_fn=<MseLossBackward0>)\n",
      "3828 tensor(22506.2383, grad_fn=<MseLossBackward0>)\n",
      "3829 tensor(22499.2441, grad_fn=<MseLossBackward0>)\n",
      "3830 tensor(22492.2578, grad_fn=<MseLossBackward0>)\n",
      "3831 tensor(22485.2871, grad_fn=<MseLossBackward0>)\n",
      "3832 tensor(22478.3184, grad_fn=<MseLossBackward0>)\n",
      "3833 tensor(22471.3633, grad_fn=<MseLossBackward0>)\n",
      "3834 tensor(22464.4141, grad_fn=<MseLossBackward0>)\n",
      "3835 tensor(22457.4746, grad_fn=<MseLossBackward0>)\n",
      "3836 tensor(22450.5469, grad_fn=<MseLossBackward0>)\n",
      "3837 tensor(22443.6270, grad_fn=<MseLossBackward0>)\n",
      "3838 tensor(22436.7129, grad_fn=<MseLossBackward0>)\n",
      "3839 tensor(22429.8125, grad_fn=<MseLossBackward0>)\n",
      "3840 tensor(22422.9199, grad_fn=<MseLossBackward0>)\n",
      "3841 tensor(22416.0352, grad_fn=<MseLossBackward0>)\n",
      "3842 tensor(22409.1602, grad_fn=<MseLossBackward0>)\n",
      "3843 tensor(22402.2910, grad_fn=<MseLossBackward0>)\n",
      "3844 tensor(22395.4355, grad_fn=<MseLossBackward0>)\n",
      "3845 tensor(22388.5879, grad_fn=<MseLossBackward0>)\n",
      "3846 tensor(22381.7500, grad_fn=<MseLossBackward0>)\n",
      "3847 tensor(22374.9199, grad_fn=<MseLossBackward0>)\n",
      "3848 tensor(22368.0977, grad_fn=<MseLossBackward0>)\n",
      "3849 tensor(22361.2871, grad_fn=<MseLossBackward0>)\n",
      "3850 tensor(22354.4863, grad_fn=<MseLossBackward0>)\n",
      "3851 tensor(22347.6934, grad_fn=<MseLossBackward0>)\n",
      "3852 tensor(22340.9082, grad_fn=<MseLossBackward0>)\n",
      "3853 tensor(22334.1328, grad_fn=<MseLossBackward0>)\n",
      "3854 tensor(22327.3672, grad_fn=<MseLossBackward0>)\n",
      "3855 tensor(22320.6094, grad_fn=<MseLossBackward0>)\n",
      "3856 tensor(22313.8574, grad_fn=<MseLossBackward0>)\n",
      "3857 tensor(22307.1191, grad_fn=<MseLossBackward0>)\n",
      "3858 tensor(22300.3906, grad_fn=<MseLossBackward0>)\n",
      "3859 tensor(22293.6680, grad_fn=<MseLossBackward0>)\n",
      "3860 tensor(22286.9551, grad_fn=<MseLossBackward0>)\n",
      "3861 tensor(22280.2520, grad_fn=<MseLossBackward0>)\n",
      "3862 tensor(22273.5566, grad_fn=<MseLossBackward0>)\n",
      "3863 tensor(22266.8691, grad_fn=<MseLossBackward0>)\n",
      "3864 tensor(22260.1934, grad_fn=<MseLossBackward0>)\n",
      "3865 tensor(22253.5273, grad_fn=<MseLossBackward0>)\n",
      "3866 tensor(22246.8672, grad_fn=<MseLossBackward0>)\n",
      "3867 tensor(22240.2168, grad_fn=<MseLossBackward0>)\n",
      "3868 tensor(22233.5762, grad_fn=<MseLossBackward0>)\n",
      "3869 tensor(22226.9434, grad_fn=<MseLossBackward0>)\n",
      "3870 tensor(22220.3223, grad_fn=<MseLossBackward0>)\n",
      "3871 tensor(22213.7070, grad_fn=<MseLossBackward0>)\n",
      "3872 tensor(22207.0996, grad_fn=<MseLossBackward0>)\n",
      "3873 tensor(22200.5020, grad_fn=<MseLossBackward0>)\n",
      "3874 tensor(22193.9160, grad_fn=<MseLossBackward0>)\n",
      "3875 tensor(22187.3379, grad_fn=<MseLossBackward0>)\n",
      "3876 tensor(22180.7676, grad_fn=<MseLossBackward0>)\n",
      "3877 tensor(22174.2070, grad_fn=<MseLossBackward0>)\n",
      "3878 tensor(22167.6562, grad_fn=<MseLossBackward0>)\n",
      "3879 tensor(22161.1113, grad_fn=<MseLossBackward0>)\n",
      "3880 tensor(22154.5781, grad_fn=<MseLossBackward0>)\n",
      "3881 tensor(22148.0508, grad_fn=<MseLossBackward0>)\n",
      "3882 tensor(22141.5332, grad_fn=<MseLossBackward0>)\n",
      "3883 tensor(22135.0293, grad_fn=<MseLossBackward0>)\n",
      "3884 tensor(22128.5293, grad_fn=<MseLossBackward0>)\n",
      "3885 tensor(22122.0391, grad_fn=<MseLossBackward0>)\n",
      "3886 tensor(22115.5547, grad_fn=<MseLossBackward0>)\n",
      "3887 tensor(22109.0859, grad_fn=<MseLossBackward0>)\n",
      "3888 tensor(22102.6172, grad_fn=<MseLossBackward0>)\n",
      "3889 tensor(22096.1641, grad_fn=<MseLossBackward0>)\n",
      "3890 tensor(22089.7188, grad_fn=<MseLossBackward0>)\n",
      "3891 tensor(22083.2812, grad_fn=<MseLossBackward0>)\n",
      "3892 tensor(22076.8516, grad_fn=<MseLossBackward0>)\n",
      "3893 tensor(22070.4297, grad_fn=<MseLossBackward0>)\n",
      "3894 tensor(22064.0215, grad_fn=<MseLossBackward0>)\n",
      "3895 tensor(22057.6172, grad_fn=<MseLossBackward0>)\n",
      "3896 tensor(22051.2266, grad_fn=<MseLossBackward0>)\n",
      "3897 tensor(22044.8398, grad_fn=<MseLossBackward0>)\n",
      "3898 tensor(22038.4629, grad_fn=<MseLossBackward0>)\n",
      "3899 tensor(22032.0957, grad_fn=<MseLossBackward0>)\n",
      "3900 tensor(22025.7363, grad_fn=<MseLossBackward0>)\n",
      "3901 tensor(22019.3867, grad_fn=<MseLossBackward0>)\n",
      "3902 tensor(22013.0449, grad_fn=<MseLossBackward0>)\n",
      "3903 tensor(22006.7129, grad_fn=<MseLossBackward0>)\n",
      "3904 tensor(22000.3906, grad_fn=<MseLossBackward0>)\n",
      "3905 tensor(21994.0762, grad_fn=<MseLossBackward0>)\n",
      "3906 tensor(21987.7676, grad_fn=<MseLossBackward0>)\n",
      "3907 tensor(21981.4688, grad_fn=<MseLossBackward0>)\n",
      "3908 tensor(21975.1816, grad_fn=<MseLossBackward0>)\n",
      "3909 tensor(21968.9004, grad_fn=<MseLossBackward0>)\n",
      "3910 tensor(21962.6309, grad_fn=<MseLossBackward0>)\n",
      "3911 tensor(21956.3672, grad_fn=<MseLossBackward0>)\n",
      "3912 tensor(21950.1113, grad_fn=<MseLossBackward0>)\n",
      "3913 tensor(21943.8652, grad_fn=<MseLossBackward0>)\n",
      "3914 tensor(21937.6289, grad_fn=<MseLossBackward0>)\n",
      "3915 tensor(21931.4004, grad_fn=<MseLossBackward0>)\n",
      "3916 tensor(21925.1816, grad_fn=<MseLossBackward0>)\n",
      "3917 tensor(21918.9707, grad_fn=<MseLossBackward0>)\n",
      "3918 tensor(21912.7676, grad_fn=<MseLossBackward0>)\n",
      "3919 tensor(21906.5723, grad_fn=<MseLossBackward0>)\n",
      "3920 tensor(21900.3867, grad_fn=<MseLossBackward0>)\n",
      "3921 tensor(21894.2109, grad_fn=<MseLossBackward0>)\n",
      "3922 tensor(21888.0430, grad_fn=<MseLossBackward0>)\n",
      "3923 tensor(21881.8848, grad_fn=<MseLossBackward0>)\n",
      "3924 tensor(21875.7344, grad_fn=<MseLossBackward0>)\n",
      "3925 tensor(21869.5898, grad_fn=<MseLossBackward0>)\n",
      "3926 tensor(21863.4570, grad_fn=<MseLossBackward0>)\n",
      "3927 tensor(21857.3320, grad_fn=<MseLossBackward0>)\n",
      "3928 tensor(21851.2168, grad_fn=<MseLossBackward0>)\n",
      "3929 tensor(21845.1074, grad_fn=<MseLossBackward0>)\n",
      "3930 tensor(21839.0078, grad_fn=<MseLossBackward0>)\n",
      "3931 tensor(21832.9160, grad_fn=<MseLossBackward0>)\n",
      "3932 tensor(21826.8340, grad_fn=<MseLossBackward0>)\n",
      "3933 tensor(21820.7578, grad_fn=<MseLossBackward0>)\n",
      "3934 tensor(21814.6953, grad_fn=<MseLossBackward0>)\n",
      "3935 tensor(21808.6367, grad_fn=<MseLossBackward0>)\n",
      "3936 tensor(21802.5879, grad_fn=<MseLossBackward0>)\n",
      "3937 tensor(21796.5488, grad_fn=<MseLossBackward0>)\n",
      "3938 tensor(21790.5176, grad_fn=<MseLossBackward0>)\n",
      "3939 tensor(21784.4941, grad_fn=<MseLossBackward0>)\n",
      "3940 tensor(21778.4805, grad_fn=<MseLossBackward0>)\n",
      "3941 tensor(21772.4746, grad_fn=<MseLossBackward0>)\n",
      "3942 tensor(21766.4785, grad_fn=<MseLossBackward0>)\n",
      "3943 tensor(21760.4883, grad_fn=<MseLossBackward0>)\n",
      "3944 tensor(21754.5078, grad_fn=<MseLossBackward0>)\n",
      "3945 tensor(21748.5352, grad_fn=<MseLossBackward0>)\n",
      "3946 tensor(21742.5723, grad_fn=<MseLossBackward0>)\n",
      "3947 tensor(21736.6191, grad_fn=<MseLossBackward0>)\n",
      "3948 tensor(21730.6719, grad_fn=<MseLossBackward0>)\n",
      "3949 tensor(21724.7344, grad_fn=<MseLossBackward0>)\n",
      "3950 tensor(21718.8027, grad_fn=<MseLossBackward0>)\n",
      "3951 tensor(21712.8828, grad_fn=<MseLossBackward0>)\n",
      "3952 tensor(21706.9688, grad_fn=<MseLossBackward0>)\n",
      "3953 tensor(21701.0645, grad_fn=<MseLossBackward0>)\n",
      "3954 tensor(21695.1660, grad_fn=<MseLossBackward0>)\n",
      "3955 tensor(21689.2793, grad_fn=<MseLossBackward0>)\n",
      "3956 tensor(21683.4004, grad_fn=<MseLossBackward0>)\n",
      "3957 tensor(21677.5273, grad_fn=<MseLossBackward0>)\n",
      "3958 tensor(21671.6660, grad_fn=<MseLossBackward0>)\n",
      "3959 tensor(21665.8105, grad_fn=<MseLossBackward0>)\n",
      "3960 tensor(21659.9668, grad_fn=<MseLossBackward0>)\n",
      "3961 tensor(21654.1270, grad_fn=<MseLossBackward0>)\n",
      "3962 tensor(21648.2988, grad_fn=<MseLossBackward0>)\n",
      "3963 tensor(21642.4746, grad_fn=<MseLossBackward0>)\n",
      "3964 tensor(21636.6660, grad_fn=<MseLossBackward0>)\n",
      "3965 tensor(21630.8594, grad_fn=<MseLossBackward0>)\n",
      "3966 tensor(21625.0645, grad_fn=<MseLossBackward0>)\n",
      "3967 tensor(21619.2773, grad_fn=<MseLossBackward0>)\n",
      "3968 tensor(21613.4961, grad_fn=<MseLossBackward0>)\n",
      "3969 tensor(21607.7246, grad_fn=<MseLossBackward0>)\n",
      "3970 tensor(21601.9609, grad_fn=<MseLossBackward0>)\n",
      "3971 tensor(21596.2109, grad_fn=<MseLossBackward0>)\n",
      "3972 tensor(21590.4629, grad_fn=<MseLossBackward0>)\n",
      "3973 tensor(21584.7266, grad_fn=<MseLossBackward0>)\n",
      "3974 tensor(21578.9941, grad_fn=<MseLossBackward0>)\n",
      "3975 tensor(21573.2754, grad_fn=<MseLossBackward0>)\n",
      "3976 tensor(21567.5605, grad_fn=<MseLossBackward0>)\n",
      "3977 tensor(21561.8555, grad_fn=<MseLossBackward0>)\n",
      "3978 tensor(21556.1602, grad_fn=<MseLossBackward0>)\n",
      "3979 tensor(21550.4707, grad_fn=<MseLossBackward0>)\n",
      "3980 tensor(21544.7910, grad_fn=<MseLossBackward0>)\n",
      "3981 tensor(21539.1172, grad_fn=<MseLossBackward0>)\n",
      "3982 tensor(21533.4531, grad_fn=<MseLossBackward0>)\n",
      "3983 tensor(21527.7988, grad_fn=<MseLossBackward0>)\n",
      "3984 tensor(21522.1523, grad_fn=<MseLossBackward0>)\n",
      "3985 tensor(21516.5117, grad_fn=<MseLossBackward0>)\n",
      "3986 tensor(21510.8828, grad_fn=<MseLossBackward0>)\n",
      "3987 tensor(21505.2598, grad_fn=<MseLossBackward0>)\n",
      "3988 tensor(21499.6445, grad_fn=<MseLossBackward0>)\n",
      "3989 tensor(21494.0391, grad_fn=<MseLossBackward0>)\n",
      "3990 tensor(21488.4395, grad_fn=<MseLossBackward0>)\n",
      "3991 tensor(21482.8516, grad_fn=<MseLossBackward0>)\n",
      "3992 tensor(21477.2676, grad_fn=<MseLossBackward0>)\n",
      "3993 tensor(21471.6953, grad_fn=<MseLossBackward0>)\n",
      "3994 tensor(21466.1289, grad_fn=<MseLossBackward0>)\n",
      "3995 tensor(21460.5703, grad_fn=<MseLossBackward0>)\n",
      "3996 tensor(21455.0215, grad_fn=<MseLossBackward0>)\n",
      "3997 tensor(21449.4824, grad_fn=<MseLossBackward0>)\n",
      "3998 tensor(21443.9473, grad_fn=<MseLossBackward0>)\n",
      "3999 tensor(21438.4219, grad_fn=<MseLossBackward0>)\n",
      "4000 tensor(21432.9082, grad_fn=<MseLossBackward0>)\n",
      "4001 tensor(21427.3965, grad_fn=<MseLossBackward0>)\n",
      "4002 tensor(21421.8965, grad_fn=<MseLossBackward0>)\n",
      "4003 tensor(21416.4043, grad_fn=<MseLossBackward0>)\n",
      "4004 tensor(21410.9219, grad_fn=<MseLossBackward0>)\n",
      "4005 tensor(21405.4414, grad_fn=<MseLossBackward0>)\n",
      "4006 tensor(21399.9746, grad_fn=<MseLossBackward0>)\n",
      "4007 tensor(21394.5137, grad_fn=<MseLossBackward0>)\n",
      "4008 tensor(21389.0586, grad_fn=<MseLossBackward0>)\n",
      "4009 tensor(21383.6172, grad_fn=<MseLossBackward0>)\n",
      "4010 tensor(21378.1816, grad_fn=<MseLossBackward0>)\n",
      "4011 tensor(21372.7520, grad_fn=<MseLossBackward0>)\n",
      "4012 tensor(21367.3320, grad_fn=<MseLossBackward0>)\n",
      "4013 tensor(21361.9219, grad_fn=<MseLossBackward0>)\n",
      "4014 tensor(21356.5156, grad_fn=<MseLossBackward0>)\n",
      "4015 tensor(21351.1211, grad_fn=<MseLossBackward0>)\n",
      "4016 tensor(21345.7324, grad_fn=<MseLossBackward0>)\n",
      "4017 tensor(21340.3516, grad_fn=<MseLossBackward0>)\n",
      "4018 tensor(21334.9785, grad_fn=<MseLossBackward0>)\n",
      "4019 tensor(21329.6133, grad_fn=<MseLossBackward0>)\n",
      "4020 tensor(21324.2578, grad_fn=<MseLossBackward0>)\n",
      "4021 tensor(21318.9102, grad_fn=<MseLossBackward0>)\n",
      "4022 tensor(21313.5684, grad_fn=<MseLossBackward0>)\n",
      "4023 tensor(21308.2363, grad_fn=<MseLossBackward0>)\n",
      "4024 tensor(21302.9141, grad_fn=<MseLossBackward0>)\n",
      "4025 tensor(21297.5957, grad_fn=<MseLossBackward0>)\n",
      "4026 tensor(21292.2852, grad_fn=<MseLossBackward0>)\n",
      "4027 tensor(21286.9883, grad_fn=<MseLossBackward0>)\n",
      "4028 tensor(21281.6934, grad_fn=<MseLossBackward0>)\n",
      "4029 tensor(21276.4102, grad_fn=<MseLossBackward0>)\n",
      "4030 tensor(21271.1328, grad_fn=<MseLossBackward0>)\n",
      "4031 tensor(21265.8633, grad_fn=<MseLossBackward0>)\n",
      "4032 tensor(21260.5996, grad_fn=<MseLossBackward0>)\n",
      "4033 tensor(21255.3496, grad_fn=<MseLossBackward0>)\n",
      "4034 tensor(21250.1016, grad_fn=<MseLossBackward0>)\n",
      "4035 tensor(21244.8652, grad_fn=<MseLossBackward0>)\n",
      "4036 tensor(21239.6348, grad_fn=<MseLossBackward0>)\n",
      "4037 tensor(21234.4141, grad_fn=<MseLossBackward0>)\n",
      "4038 tensor(21229.1992, grad_fn=<MseLossBackward0>)\n",
      "4039 tensor(21223.9922, grad_fn=<MseLossBackward0>)\n",
      "4040 tensor(21218.7969, grad_fn=<MseLossBackward0>)\n",
      "4041 tensor(21213.6035, grad_fn=<MseLossBackward0>)\n",
      "4042 tensor(21208.4219, grad_fn=<MseLossBackward0>)\n",
      "4043 tensor(21203.2461, grad_fn=<MseLossBackward0>)\n",
      "4044 tensor(21198.0801, grad_fn=<MseLossBackward0>)\n",
      "4045 tensor(21192.9219, grad_fn=<MseLossBackward0>)\n",
      "4046 tensor(21187.7695, grad_fn=<MseLossBackward0>)\n",
      "4047 tensor(21182.6270, grad_fn=<MseLossBackward0>)\n",
      "4048 tensor(21177.4883, grad_fn=<MseLossBackward0>)\n",
      "4049 tensor(21172.3613, grad_fn=<MseLossBackward0>)\n",
      "4050 tensor(21167.2422, grad_fn=<MseLossBackward0>)\n",
      "4051 tensor(21162.1289, grad_fn=<MseLossBackward0>)\n",
      "4052 tensor(21157.0215, grad_fn=<MseLossBackward0>)\n",
      "4053 tensor(21151.9258, grad_fn=<MseLossBackward0>)\n",
      "4054 tensor(21146.8379, grad_fn=<MseLossBackward0>)\n",
      "4055 tensor(21141.7559, grad_fn=<MseLossBackward0>)\n",
      "4056 tensor(21136.6816, grad_fn=<MseLossBackward0>)\n",
      "4057 tensor(21131.6152, grad_fn=<MseLossBackward0>)\n",
      "4058 tensor(21126.5586, grad_fn=<MseLossBackward0>)\n",
      "4059 tensor(21121.5059, grad_fn=<MseLossBackward0>)\n",
      "4060 tensor(21116.4648, grad_fn=<MseLossBackward0>)\n",
      "4061 tensor(21111.4277, grad_fn=<MseLossBackward0>)\n",
      "4062 tensor(21106.4004, grad_fn=<MseLossBackward0>)\n",
      "4063 tensor(21101.3789, grad_fn=<MseLossBackward0>)\n",
      "4064 tensor(21096.3672, grad_fn=<MseLossBackward0>)\n",
      "4065 tensor(21091.3633, grad_fn=<MseLossBackward0>)\n",
      "4066 tensor(21086.3672, grad_fn=<MseLossBackward0>)\n",
      "4067 tensor(21081.3750, grad_fn=<MseLossBackward0>)\n",
      "4068 tensor(21076.3965, grad_fn=<MseLossBackward0>)\n",
      "4069 tensor(21071.4199, grad_fn=<MseLossBackward0>)\n",
      "4070 tensor(21066.4551, grad_fn=<MseLossBackward0>)\n",
      "4071 tensor(21061.4941, grad_fn=<MseLossBackward0>)\n",
      "4072 tensor(21056.5430, grad_fn=<MseLossBackward0>)\n",
      "4073 tensor(21051.5996, grad_fn=<MseLossBackward0>)\n",
      "4074 tensor(21046.6641, grad_fn=<MseLossBackward0>)\n",
      "4075 tensor(21041.7363, grad_fn=<MseLossBackward0>)\n",
      "4076 tensor(21036.8145, grad_fn=<MseLossBackward0>)\n",
      "4077 tensor(21031.9043, grad_fn=<MseLossBackward0>)\n",
      "4078 tensor(21026.9961, grad_fn=<MseLossBackward0>)\n",
      "4079 tensor(21022.0977, grad_fn=<MseLossBackward0>)\n",
      "4080 tensor(21017.2070, grad_fn=<MseLossBackward0>)\n",
      "4081 tensor(21012.3262, grad_fn=<MseLossBackward0>)\n",
      "4082 tensor(21007.4512, grad_fn=<MseLossBackward0>)\n",
      "4083 tensor(21002.5820, grad_fn=<MseLossBackward0>)\n",
      "4084 tensor(20997.7227, grad_fn=<MseLossBackward0>)\n",
      "4085 tensor(20992.8672, grad_fn=<MseLossBackward0>)\n",
      "4086 tensor(20988.0234, grad_fn=<MseLossBackward0>)\n",
      "4087 tensor(20983.1855, grad_fn=<MseLossBackward0>)\n",
      "4088 tensor(20978.3535, grad_fn=<MseLossBackward0>)\n",
      "4089 tensor(20973.5332, grad_fn=<MseLossBackward0>)\n",
      "4090 tensor(20968.7168, grad_fn=<MseLossBackward0>)\n",
      "4091 tensor(20963.9082, grad_fn=<MseLossBackward0>)\n",
      "4092 tensor(20959.1094, grad_fn=<MseLossBackward0>)\n",
      "4093 tensor(20954.3164, grad_fn=<MseLossBackward0>)\n",
      "4094 tensor(20949.5293, grad_fn=<MseLossBackward0>)\n",
      "4095 tensor(20944.7520, grad_fn=<MseLossBackward0>)\n",
      "4096 tensor(20939.9824, grad_fn=<MseLossBackward0>)\n",
      "4097 tensor(20935.2188, grad_fn=<MseLossBackward0>)\n",
      "4098 tensor(20930.4629, grad_fn=<MseLossBackward0>)\n",
      "4099 tensor(20925.7148, grad_fn=<MseLossBackward0>)\n",
      "4100 tensor(20920.9746, grad_fn=<MseLossBackward0>)\n",
      "4101 tensor(20916.2422, grad_fn=<MseLossBackward0>)\n",
      "4102 tensor(20911.5156, grad_fn=<MseLossBackward0>)\n",
      "4103 tensor(20906.7988, grad_fn=<MseLossBackward0>)\n",
      "4104 tensor(20902.0859, grad_fn=<MseLossBackward0>)\n",
      "4105 tensor(20897.3828, grad_fn=<MseLossBackward0>)\n",
      "4106 tensor(20892.6875, grad_fn=<MseLossBackward0>)\n",
      "4107 tensor(20887.9941, grad_fn=<MseLossBackward0>)\n",
      "4108 tensor(20883.3145, grad_fn=<MseLossBackward0>)\n",
      "4109 tensor(20878.6387, grad_fn=<MseLossBackward0>)\n",
      "4110 tensor(20873.9746, grad_fn=<MseLossBackward0>)\n",
      "4111 tensor(20869.3145, grad_fn=<MseLossBackward0>)\n",
      "4112 tensor(20864.6602, grad_fn=<MseLossBackward0>)\n",
      "4113 tensor(20860.0176, grad_fn=<MseLossBackward0>)\n",
      "4114 tensor(20855.3789, grad_fn=<MseLossBackward0>)\n",
      "4115 tensor(20850.7500, grad_fn=<MseLossBackward0>)\n",
      "4116 tensor(20846.1270, grad_fn=<MseLossBackward0>)\n",
      "4117 tensor(20841.5098, grad_fn=<MseLossBackward0>)\n",
      "4118 tensor(20836.9023, grad_fn=<MseLossBackward0>)\n",
      "4119 tensor(20832.3008, grad_fn=<MseLossBackward0>)\n",
      "4120 tensor(20827.7070, grad_fn=<MseLossBackward0>)\n",
      "4121 tensor(20823.1211, grad_fn=<MseLossBackward0>)\n",
      "4122 tensor(20818.5430, grad_fn=<MseLossBackward0>)\n",
      "4123 tensor(20813.9727, grad_fn=<MseLossBackward0>)\n",
      "4124 tensor(20809.4062, grad_fn=<MseLossBackward0>)\n",
      "4125 tensor(20804.8477, grad_fn=<MseLossBackward0>)\n",
      "4126 tensor(20800.2969, grad_fn=<MseLossBackward0>)\n",
      "4127 tensor(20795.7559, grad_fn=<MseLossBackward0>)\n",
      "4128 tensor(20791.2207, grad_fn=<MseLossBackward0>)\n",
      "4129 tensor(20786.6914, grad_fn=<MseLossBackward0>)\n",
      "4130 tensor(20782.1699, grad_fn=<MseLossBackward0>)\n",
      "4131 tensor(20777.6562, grad_fn=<MseLossBackward0>)\n",
      "4132 tensor(20773.1504, grad_fn=<MseLossBackward0>)\n",
      "4133 tensor(20768.6504, grad_fn=<MseLossBackward0>)\n",
      "4134 tensor(20764.1562, grad_fn=<MseLossBackward0>)\n",
      "4135 tensor(20759.6738, grad_fn=<MseLossBackward0>)\n",
      "4136 tensor(20755.1953, grad_fn=<MseLossBackward0>)\n",
      "4137 tensor(20750.7227, grad_fn=<MseLossBackward0>)\n",
      "4138 tensor(20746.2578, grad_fn=<MseLossBackward0>)\n",
      "4139 tensor(20741.8008, grad_fn=<MseLossBackward0>)\n",
      "4140 tensor(20737.3535, grad_fn=<MseLossBackward0>)\n",
      "4141 tensor(20732.9121, grad_fn=<MseLossBackward0>)\n",
      "4142 tensor(20728.4746, grad_fn=<MseLossBackward0>)\n",
      "4143 tensor(20724.0469, grad_fn=<MseLossBackward0>)\n",
      "4144 tensor(20719.6270, grad_fn=<MseLossBackward0>)\n",
      "4145 tensor(20715.2129, grad_fn=<MseLossBackward0>)\n",
      "4146 tensor(20710.8066, grad_fn=<MseLossBackward0>)\n",
      "4147 tensor(20706.4062, grad_fn=<MseLossBackward0>)\n",
      "4148 tensor(20702.0137, grad_fn=<MseLossBackward0>)\n",
      "4149 tensor(20697.6289, grad_fn=<MseLossBackward0>)\n",
      "4150 tensor(20693.2480, grad_fn=<MseLossBackward0>)\n",
      "4151 tensor(20688.8789, grad_fn=<MseLossBackward0>)\n",
      "4152 tensor(20684.5156, grad_fn=<MseLossBackward0>)\n",
      "4153 tensor(20680.1543, grad_fn=<MseLossBackward0>)\n",
      "4154 tensor(20675.8066, grad_fn=<MseLossBackward0>)\n",
      "4155 tensor(20671.4629, grad_fn=<MseLossBackward0>)\n",
      "4156 tensor(20667.1270, grad_fn=<MseLossBackward0>)\n",
      "4157 tensor(20662.7969, grad_fn=<MseLossBackward0>)\n",
      "4158 tensor(20658.4766, grad_fn=<MseLossBackward0>)\n",
      "4159 tensor(20654.1621, grad_fn=<MseLossBackward0>)\n",
      "4160 tensor(20649.8535, grad_fn=<MseLossBackward0>)\n",
      "4161 tensor(20645.5527, grad_fn=<MseLossBackward0>)\n",
      "4162 tensor(20641.2578, grad_fn=<MseLossBackward0>)\n",
      "4163 tensor(20636.9707, grad_fn=<MseLossBackward0>)\n",
      "4164 tensor(20632.6895, grad_fn=<MseLossBackward0>)\n",
      "4165 tensor(20628.4180, grad_fn=<MseLossBackward0>)\n",
      "4166 tensor(20624.1523, grad_fn=<MseLossBackward0>)\n",
      "4167 tensor(20619.8926, grad_fn=<MseLossBackward0>)\n",
      "4168 tensor(20615.6406, grad_fn=<MseLossBackward0>)\n",
      "4169 tensor(20611.3945, grad_fn=<MseLossBackward0>)\n",
      "4170 tensor(20607.1562, grad_fn=<MseLossBackward0>)\n",
      "4171 tensor(20602.9258, grad_fn=<MseLossBackward0>)\n",
      "4172 tensor(20598.6992, grad_fn=<MseLossBackward0>)\n",
      "4173 tensor(20594.4863, grad_fn=<MseLossBackward0>)\n",
      "4174 tensor(20590.2715, grad_fn=<MseLossBackward0>)\n",
      "4175 tensor(20586.0703, grad_fn=<MseLossBackward0>)\n",
      "4176 tensor(20581.8711, grad_fn=<MseLossBackward0>)\n",
      "4177 tensor(20577.6797, grad_fn=<MseLossBackward0>)\n",
      "4178 tensor(20573.4980, grad_fn=<MseLossBackward0>)\n",
      "4179 tensor(20569.3223, grad_fn=<MseLossBackward0>)\n",
      "4180 tensor(20565.1523, grad_fn=<MseLossBackward0>)\n",
      "4181 tensor(20560.9902, grad_fn=<MseLossBackward0>)\n",
      "4182 tensor(20556.8340, grad_fn=<MseLossBackward0>)\n",
      "4183 tensor(20552.6855, grad_fn=<MseLossBackward0>)\n",
      "4184 tensor(20548.5449, grad_fn=<MseLossBackward0>)\n",
      "4185 tensor(20544.4082, grad_fn=<MseLossBackward0>)\n",
      "4186 tensor(20540.2812, grad_fn=<MseLossBackward0>)\n",
      "4187 tensor(20536.1582, grad_fn=<MseLossBackward0>)\n",
      "4188 tensor(20532.0449, grad_fn=<MseLossBackward0>)\n",
      "4189 tensor(20527.9355, grad_fn=<MseLossBackward0>)\n",
      "4190 tensor(20523.8340, grad_fn=<MseLossBackward0>)\n",
      "4191 tensor(20519.7402, grad_fn=<MseLossBackward0>)\n",
      "4192 tensor(20515.6523, grad_fn=<MseLossBackward0>)\n",
      "4193 tensor(20511.5703, grad_fn=<MseLossBackward0>)\n",
      "4194 tensor(20507.4980, grad_fn=<MseLossBackward0>)\n",
      "4195 tensor(20503.4316, grad_fn=<MseLossBackward0>)\n",
      "4196 tensor(20499.3711, grad_fn=<MseLossBackward0>)\n",
      "4197 tensor(20495.3164, grad_fn=<MseLossBackward0>)\n",
      "4198 tensor(20491.2676, grad_fn=<MseLossBackward0>)\n",
      "4199 tensor(20487.2266, grad_fn=<MseLossBackward0>)\n",
      "4200 tensor(20483.1953, grad_fn=<MseLossBackward0>)\n",
      "4201 tensor(20479.1680, grad_fn=<MseLossBackward0>)\n",
      "4202 tensor(20475.1484, grad_fn=<MseLossBackward0>)\n",
      "4203 tensor(20471.1348, grad_fn=<MseLossBackward0>)\n",
      "4204 tensor(20467.1270, grad_fn=<MseLossBackward0>)\n",
      "4205 tensor(20463.1289, grad_fn=<MseLossBackward0>)\n",
      "4206 tensor(20459.1348, grad_fn=<MseLossBackward0>)\n",
      "4207 tensor(20455.1465, grad_fn=<MseLossBackward0>)\n",
      "4208 tensor(20451.1680, grad_fn=<MseLossBackward0>)\n",
      "4209 tensor(20447.1934, grad_fn=<MseLossBackward0>)\n",
      "4210 tensor(20443.2285, grad_fn=<MseLossBackward0>)\n",
      "4211 tensor(20439.2656, grad_fn=<MseLossBackward0>)\n",
      "4212 tensor(20435.3105, grad_fn=<MseLossBackward0>)\n",
      "4213 tensor(20431.3672, grad_fn=<MseLossBackward0>)\n",
      "4214 tensor(20427.4238, grad_fn=<MseLossBackward0>)\n",
      "4215 tensor(20423.4922, grad_fn=<MseLossBackward0>)\n",
      "4216 tensor(20419.5625, grad_fn=<MseLossBackward0>)\n",
      "4217 tensor(20415.6445, grad_fn=<MseLossBackward0>)\n",
      "4218 tensor(20411.7305, grad_fn=<MseLossBackward0>)\n",
      "4219 tensor(20407.8223, grad_fn=<MseLossBackward0>)\n",
      "4220 tensor(20403.9219, grad_fn=<MseLossBackward0>)\n",
      "4221 tensor(20400.0273, grad_fn=<MseLossBackward0>)\n",
      "4222 tensor(20396.1406, grad_fn=<MseLossBackward0>)\n",
      "4223 tensor(20392.2598, grad_fn=<MseLossBackward0>)\n",
      "4224 tensor(20388.3848, grad_fn=<MseLossBackward0>)\n",
      "4225 tensor(20384.5176, grad_fn=<MseLossBackward0>)\n",
      "4226 tensor(20380.6562, grad_fn=<MseLossBackward0>)\n",
      "4227 tensor(20376.8008, grad_fn=<MseLossBackward0>)\n",
      "4228 tensor(20372.9531, grad_fn=<MseLossBackward0>)\n",
      "4229 tensor(20369.1113, grad_fn=<MseLossBackward0>)\n",
      "4230 tensor(20365.2734, grad_fn=<MseLossBackward0>)\n",
      "4231 tensor(20361.4453, grad_fn=<MseLossBackward0>)\n",
      "4232 tensor(20357.6250, grad_fn=<MseLossBackward0>)\n",
      "4233 tensor(20353.8066, grad_fn=<MseLossBackward0>)\n",
      "4234 tensor(20349.9980, grad_fn=<MseLossBackward0>)\n",
      "4235 tensor(20346.1973, grad_fn=<MseLossBackward0>)\n",
      "4236 tensor(20342.4004, grad_fn=<MseLossBackward0>)\n",
      "4237 tensor(20338.6074, grad_fn=<MseLossBackward0>)\n",
      "4238 tensor(20334.8262, grad_fn=<MseLossBackward0>)\n",
      "4239 tensor(20331.0488, grad_fn=<MseLossBackward0>)\n",
      "4240 tensor(20327.2793, grad_fn=<MseLossBackward0>)\n",
      "4241 tensor(20323.5156, grad_fn=<MseLossBackward0>)\n",
      "4242 tensor(20319.7539, grad_fn=<MseLossBackward0>)\n",
      "4243 tensor(20316.0059, grad_fn=<MseLossBackward0>)\n",
      "4244 tensor(20312.2598, grad_fn=<MseLossBackward0>)\n",
      "4245 tensor(20308.5215, grad_fn=<MseLossBackward0>)\n",
      "4246 tensor(20304.7891, grad_fn=<MseLossBackward0>)\n",
      "4247 tensor(20301.0625, grad_fn=<MseLossBackward0>)\n",
      "4248 tensor(20297.3438, grad_fn=<MseLossBackward0>)\n",
      "4249 tensor(20293.6328, grad_fn=<MseLossBackward0>)\n",
      "4250 tensor(20289.9258, grad_fn=<MseLossBackward0>)\n",
      "4251 tensor(20286.2227, grad_fn=<MseLossBackward0>)\n",
      "4252 tensor(20282.5293, grad_fn=<MseLossBackward0>)\n",
      "4253 tensor(20278.8418, grad_fn=<MseLossBackward0>)\n",
      "4254 tensor(20275.1621, grad_fn=<MseLossBackward0>)\n",
      "4255 tensor(20271.4863, grad_fn=<MseLossBackward0>)\n",
      "4256 tensor(20267.8164, grad_fn=<MseLossBackward0>)\n",
      "4257 tensor(20264.1543, grad_fn=<MseLossBackward0>)\n",
      "4258 tensor(20260.5000, grad_fn=<MseLossBackward0>)\n",
      "4259 tensor(20256.8496, grad_fn=<MseLossBackward0>)\n",
      "4260 tensor(20253.2051, grad_fn=<MseLossBackward0>)\n",
      "4261 tensor(20249.5684, grad_fn=<MseLossBackward0>)\n",
      "4262 tensor(20245.9395, grad_fn=<MseLossBackward0>)\n",
      "4263 tensor(20242.3125, grad_fn=<MseLossBackward0>)\n",
      "4264 tensor(20238.6934, grad_fn=<MseLossBackward0>)\n",
      "4265 tensor(20235.0820, grad_fn=<MseLossBackward0>)\n",
      "4266 tensor(20231.4766, grad_fn=<MseLossBackward0>)\n",
      "4267 tensor(20227.8789, grad_fn=<MseLossBackward0>)\n",
      "4268 tensor(20224.2832, grad_fn=<MseLossBackward0>)\n",
      "4269 tensor(20220.6953, grad_fn=<MseLossBackward0>)\n",
      "4270 tensor(20217.1133, grad_fn=<MseLossBackward0>)\n",
      "4271 tensor(20213.5430, grad_fn=<MseLossBackward0>)\n",
      "4272 tensor(20209.9746, grad_fn=<MseLossBackward0>)\n",
      "4273 tensor(20206.4121, grad_fn=<MseLossBackward0>)\n",
      "4274 tensor(20202.8535, grad_fn=<MseLossBackward0>)\n",
      "4275 tensor(20199.3027, grad_fn=<MseLossBackward0>)\n",
      "4276 tensor(20195.7617, grad_fn=<MseLossBackward0>)\n",
      "4277 tensor(20192.2227, grad_fn=<MseLossBackward0>)\n",
      "4278 tensor(20188.6914, grad_fn=<MseLossBackward0>)\n",
      "4279 tensor(20185.1680, grad_fn=<MseLossBackward0>)\n",
      "4280 tensor(20181.6465, grad_fn=<MseLossBackward0>)\n",
      "4281 tensor(20178.1348, grad_fn=<MseLossBackward0>)\n",
      "4282 tensor(20174.6289, grad_fn=<MseLossBackward0>)\n",
      "4283 tensor(20171.1270, grad_fn=<MseLossBackward0>)\n",
      "4284 tensor(20167.6328, grad_fn=<MseLossBackward0>)\n",
      "4285 tensor(20164.1445, grad_fn=<MseLossBackward0>)\n",
      "4286 tensor(20160.6602, grad_fn=<MseLossBackward0>)\n",
      "4287 tensor(20157.1855, grad_fn=<MseLossBackward0>)\n",
      "4288 tensor(20153.7148, grad_fn=<MseLossBackward0>)\n",
      "4289 tensor(20150.2500, grad_fn=<MseLossBackward0>)\n",
      "4290 tensor(20146.7930, grad_fn=<MseLossBackward0>)\n",
      "4291 tensor(20143.3438, grad_fn=<MseLossBackward0>)\n",
      "4292 tensor(20139.8945, grad_fn=<MseLossBackward0>)\n",
      "4293 tensor(20136.4570, grad_fn=<MseLossBackward0>)\n",
      "4294 tensor(20133.0215, grad_fn=<MseLossBackward0>)\n",
      "4295 tensor(20129.5938, grad_fn=<MseLossBackward0>)\n",
      "4296 tensor(20126.1719, grad_fn=<MseLossBackward0>)\n",
      "4297 tensor(20122.7559, grad_fn=<MseLossBackward0>)\n",
      "4298 tensor(20119.3457, grad_fn=<MseLossBackward0>)\n",
      "4299 tensor(20115.9434, grad_fn=<MseLossBackward0>)\n",
      "4300 tensor(20112.5469, grad_fn=<MseLossBackward0>)\n",
      "4301 tensor(20109.1562, grad_fn=<MseLossBackward0>)\n",
      "4302 tensor(20105.7676, grad_fn=<MseLossBackward0>)\n",
      "4303 tensor(20102.3926, grad_fn=<MseLossBackward0>)\n",
      "4304 tensor(20099.0156, grad_fn=<MseLossBackward0>)\n",
      "4305 tensor(20095.6504, grad_fn=<MseLossBackward0>)\n",
      "4306 tensor(20092.2871, grad_fn=<MseLossBackward0>)\n",
      "4307 tensor(20088.9316, grad_fn=<MseLossBackward0>)\n",
      "4308 tensor(20085.5820, grad_fn=<MseLossBackward0>)\n",
      "4309 tensor(20082.2363, grad_fn=<MseLossBackward0>)\n",
      "4310 tensor(20078.8984, grad_fn=<MseLossBackward0>)\n",
      "4311 tensor(20075.5664, grad_fn=<MseLossBackward0>)\n",
      "4312 tensor(20072.2402, grad_fn=<MseLossBackward0>)\n",
      "4313 tensor(20068.9219, grad_fn=<MseLossBackward0>)\n",
      "4314 tensor(20065.6074, grad_fn=<MseLossBackward0>)\n",
      "4315 tensor(20062.2988, grad_fn=<MseLossBackward0>)\n",
      "4316 tensor(20058.9980, grad_fn=<MseLossBackward0>)\n",
      "4317 tensor(20055.7012, grad_fn=<MseLossBackward0>)\n",
      "4318 tensor(20052.4082, grad_fn=<MseLossBackward0>)\n",
      "4319 tensor(20049.1270, grad_fn=<MseLossBackward0>)\n",
      "4320 tensor(20045.8457, grad_fn=<MseLossBackward0>)\n",
      "4321 tensor(20042.5723, grad_fn=<MseLossBackward0>)\n",
      "4322 tensor(20039.3047, grad_fn=<MseLossBackward0>)\n",
      "4323 tensor(20036.0410, grad_fn=<MseLossBackward0>)\n",
      "4324 tensor(20032.7871, grad_fn=<MseLossBackward0>)\n",
      "4325 tensor(20029.5391, grad_fn=<MseLossBackward0>)\n",
      "4326 tensor(20026.2949, grad_fn=<MseLossBackward0>)\n",
      "4327 tensor(20023.0547, grad_fn=<MseLossBackward0>)\n",
      "4328 tensor(20019.8242, grad_fn=<MseLossBackward0>)\n",
      "4329 tensor(20016.5957, grad_fn=<MseLossBackward0>)\n",
      "4330 tensor(20013.3770, grad_fn=<MseLossBackward0>)\n",
      "4331 tensor(20010.1621, grad_fn=<MseLossBackward0>)\n",
      "4332 tensor(20006.9531, grad_fn=<MseLossBackward0>)\n",
      "4333 tensor(20003.7500, grad_fn=<MseLossBackward0>)\n",
      "4334 tensor(20000.5508, grad_fn=<MseLossBackward0>)\n",
      "4335 tensor(19997.3594, grad_fn=<MseLossBackward0>)\n",
      "4336 tensor(19994.1758, grad_fn=<MseLossBackward0>)\n",
      "4337 tensor(19990.9941, grad_fn=<MseLossBackward0>)\n",
      "4338 tensor(19987.8203, grad_fn=<MseLossBackward0>)\n",
      "4339 tensor(19984.6504, grad_fn=<MseLossBackward0>)\n",
      "4340 tensor(19981.4883, grad_fn=<MseLossBackward0>)\n",
      "4341 tensor(19978.3281, grad_fn=<MseLossBackward0>)\n",
      "4342 tensor(19975.1758, grad_fn=<MseLossBackward0>)\n",
      "4343 tensor(19972.0312, grad_fn=<MseLossBackward0>)\n",
      "4344 tensor(19968.8906, grad_fn=<MseLossBackward0>)\n",
      "4345 tensor(19965.7578, grad_fn=<MseLossBackward0>)\n",
      "4346 tensor(19962.6289, grad_fn=<MseLossBackward0>)\n",
      "4347 tensor(19959.5059, grad_fn=<MseLossBackward0>)\n",
      "4348 tensor(19956.3887, grad_fn=<MseLossBackward0>)\n",
      "4349 tensor(19953.2754, grad_fn=<MseLossBackward0>)\n",
      "4350 tensor(19950.1699, grad_fn=<MseLossBackward0>)\n",
      "4351 tensor(19947.0664, grad_fn=<MseLossBackward0>)\n",
      "4352 tensor(19943.9746, grad_fn=<MseLossBackward0>)\n",
      "4353 tensor(19940.8828, grad_fn=<MseLossBackward0>)\n",
      "4354 tensor(19937.7988, grad_fn=<MseLossBackward0>)\n",
      "4355 tensor(19934.7207, grad_fn=<MseLossBackward0>)\n",
      "4356 tensor(19931.6504, grad_fn=<MseLossBackward0>)\n",
      "4357 tensor(19928.5820, grad_fn=<MseLossBackward0>)\n",
      "4358 tensor(19925.5215, grad_fn=<MseLossBackward0>)\n",
      "4359 tensor(19922.4648, grad_fn=<MseLossBackward0>)\n",
      "4360 tensor(19919.4141, grad_fn=<MseLossBackward0>)\n",
      "4361 tensor(19916.3711, grad_fn=<MseLossBackward0>)\n",
      "4362 tensor(19913.3320, grad_fn=<MseLossBackward0>)\n",
      "4363 tensor(19910.2969, grad_fn=<MseLossBackward0>)\n",
      "4364 tensor(19907.2676, grad_fn=<MseLossBackward0>)\n",
      "4365 tensor(19904.2461, grad_fn=<MseLossBackward0>)\n",
      "4366 tensor(19901.2305, grad_fn=<MseLossBackward0>)\n",
      "4367 tensor(19898.2188, grad_fn=<MseLossBackward0>)\n",
      "4368 tensor(19895.2129, grad_fn=<MseLossBackward0>)\n",
      "4369 tensor(19892.2129, grad_fn=<MseLossBackward0>)\n",
      "4370 tensor(19889.2188, grad_fn=<MseLossBackward0>)\n",
      "4371 tensor(19886.2285, grad_fn=<MseLossBackward0>)\n",
      "4372 tensor(19883.2441, grad_fn=<MseLossBackward0>)\n",
      "4373 tensor(19880.2656, grad_fn=<MseLossBackward0>)\n",
      "4374 tensor(19877.2930, grad_fn=<MseLossBackward0>)\n",
      "4375 tensor(19874.3262, grad_fn=<MseLossBackward0>)\n",
      "4376 tensor(19871.3652, grad_fn=<MseLossBackward0>)\n",
      "4377 tensor(19868.4082, grad_fn=<MseLossBackward0>)\n",
      "4378 tensor(19865.4570, grad_fn=<MseLossBackward0>)\n",
      "4379 tensor(19862.5117, grad_fn=<MseLossBackward0>)\n",
      "4380 tensor(19859.5742, grad_fn=<MseLossBackward0>)\n",
      "4381 tensor(19856.6348, grad_fn=<MseLossBackward0>)\n",
      "4382 tensor(19853.7070, grad_fn=<MseLossBackward0>)\n",
      "4383 tensor(19850.7832, grad_fn=<MseLossBackward0>)\n",
      "4384 tensor(19847.8652, grad_fn=<MseLossBackward0>)\n",
      "4385 tensor(19844.9551, grad_fn=<MseLossBackward0>)\n",
      "4386 tensor(19842.0449, grad_fn=<MseLossBackward0>)\n",
      "4387 tensor(19839.1426, grad_fn=<MseLossBackward0>)\n",
      "4388 tensor(19836.2461, grad_fn=<MseLossBackward0>)\n",
      "4389 tensor(19833.3555, grad_fn=<MseLossBackward0>)\n",
      "4390 tensor(19830.4688, grad_fn=<MseLossBackward0>)\n",
      "4391 tensor(19827.5879, grad_fn=<MseLossBackward0>)\n",
      "4392 tensor(19824.7109, grad_fn=<MseLossBackward0>)\n",
      "4393 tensor(19821.8418, grad_fn=<MseLossBackward0>)\n",
      "4394 tensor(19818.9785, grad_fn=<MseLossBackward0>)\n",
      "4395 tensor(19816.1191, grad_fn=<MseLossBackward0>)\n",
      "4396 tensor(19813.2656, grad_fn=<MseLossBackward0>)\n",
      "4397 tensor(19810.4141, grad_fn=<MseLossBackward0>)\n",
      "4398 tensor(19807.5723, grad_fn=<MseLossBackward0>)\n",
      "4399 tensor(19804.7344, grad_fn=<MseLossBackward0>)\n",
      "4400 tensor(19801.9023, grad_fn=<MseLossBackward0>)\n",
      "4401 tensor(19799.0742, grad_fn=<MseLossBackward0>)\n",
      "4402 tensor(19796.2520, grad_fn=<MseLossBackward0>)\n",
      "4403 tensor(19793.4336, grad_fn=<MseLossBackward0>)\n",
      "4404 tensor(19790.6230, grad_fn=<MseLossBackward0>)\n",
      "4405 tensor(19787.8145, grad_fn=<MseLossBackward0>)\n",
      "4406 tensor(19785.0156, grad_fn=<MseLossBackward0>)\n",
      "4407 tensor(19782.2188, grad_fn=<MseLossBackward0>)\n",
      "4408 tensor(19779.4297, grad_fn=<MseLossBackward0>)\n",
      "4409 tensor(19776.6445, grad_fn=<MseLossBackward0>)\n",
      "4410 tensor(19773.8652, grad_fn=<MseLossBackward0>)\n",
      "4411 tensor(19771.0898, grad_fn=<MseLossBackward0>)\n",
      "4412 tensor(19768.3184, grad_fn=<MseLossBackward0>)\n",
      "4413 tensor(19765.5527, grad_fn=<MseLossBackward0>)\n",
      "4414 tensor(19762.7930, grad_fn=<MseLossBackward0>)\n",
      "4415 tensor(19760.0391, grad_fn=<MseLossBackward0>)\n",
      "4416 tensor(19757.2910, grad_fn=<MseLossBackward0>)\n",
      "4417 tensor(19754.5469, grad_fn=<MseLossBackward0>)\n",
      "4418 tensor(19751.8105, grad_fn=<MseLossBackward0>)\n",
      "4419 tensor(19749.0742, grad_fn=<MseLossBackward0>)\n",
      "4420 tensor(19746.3457, grad_fn=<MseLossBackward0>)\n",
      "4421 tensor(19743.6230, grad_fn=<MseLossBackward0>)\n",
      "4422 tensor(19740.9043, grad_fn=<MseLossBackward0>)\n",
      "4423 tensor(19738.1914, grad_fn=<MseLossBackward0>)\n",
      "4424 tensor(19735.4844, grad_fn=<MseLossBackward0>)\n",
      "4425 tensor(19732.7832, grad_fn=<MseLossBackward0>)\n",
      "4426 tensor(19730.0859, grad_fn=<MseLossBackward0>)\n",
      "4427 tensor(19727.3926, grad_fn=<MseLossBackward0>)\n",
      "4428 tensor(19724.7070, grad_fn=<MseLossBackward0>)\n",
      "4429 tensor(19722.0215, grad_fn=<MseLossBackward0>)\n",
      "4430 tensor(19719.3457, grad_fn=<MseLossBackward0>)\n",
      "4431 tensor(19716.6738, grad_fn=<MseLossBackward0>)\n",
      "4432 tensor(19714.0059, grad_fn=<MseLossBackward0>)\n",
      "4433 tensor(19711.3438, grad_fn=<MseLossBackward0>)\n",
      "4434 tensor(19708.6895, grad_fn=<MseLossBackward0>)\n",
      "4435 tensor(19706.0352, grad_fn=<MseLossBackward0>)\n",
      "4436 tensor(19703.3887, grad_fn=<MseLossBackward0>)\n",
      "4437 tensor(19700.7480, grad_fn=<MseLossBackward0>)\n",
      "4438 tensor(19698.1113, grad_fn=<MseLossBackward0>)\n",
      "4439 tensor(19695.4785, grad_fn=<MseLossBackward0>)\n",
      "4440 tensor(19692.8555, grad_fn=<MseLossBackward0>)\n",
      "4441 tensor(19690.2324, grad_fn=<MseLossBackward0>)\n",
      "4442 tensor(19687.6152, grad_fn=<MseLossBackward0>)\n",
      "4443 tensor(19685.0020, grad_fn=<MseLossBackward0>)\n",
      "4444 tensor(19682.3965, grad_fn=<MseLossBackward0>)\n",
      "4445 tensor(19679.7949, grad_fn=<MseLossBackward0>)\n",
      "4446 tensor(19677.1992, grad_fn=<MseLossBackward0>)\n",
      "4447 tensor(19674.6055, grad_fn=<MseLossBackward0>)\n",
      "4448 tensor(19672.0176, grad_fn=<MseLossBackward0>)\n",
      "4449 tensor(19669.4375, grad_fn=<MseLossBackward0>)\n",
      "4450 tensor(19666.8613, grad_fn=<MseLossBackward0>)\n",
      "4451 tensor(19664.2891, grad_fn=<MseLossBackward0>)\n",
      "4452 tensor(19661.7227, grad_fn=<MseLossBackward0>)\n",
      "4453 tensor(19659.1621, grad_fn=<MseLossBackward0>)\n",
      "4454 tensor(19656.6055, grad_fn=<MseLossBackward0>)\n",
      "4455 tensor(19654.0508, grad_fn=<MseLossBackward0>)\n",
      "4456 tensor(19651.5039, grad_fn=<MseLossBackward0>)\n",
      "4457 tensor(19648.9629, grad_fn=<MseLossBackward0>)\n",
      "4458 tensor(19646.4238, grad_fn=<MseLossBackward0>)\n",
      "4459 tensor(19643.8926, grad_fn=<MseLossBackward0>)\n",
      "4460 tensor(19641.3633, grad_fn=<MseLossBackward0>)\n",
      "4461 tensor(19638.8418, grad_fn=<MseLossBackward0>)\n",
      "4462 tensor(19636.3242, grad_fn=<MseLossBackward0>)\n",
      "4463 tensor(19633.8105, grad_fn=<MseLossBackward0>)\n",
      "4464 tensor(19631.3008, grad_fn=<MseLossBackward0>)\n",
      "4465 tensor(19628.7988, grad_fn=<MseLossBackward0>)\n",
      "4466 tensor(19626.3008, grad_fn=<MseLossBackward0>)\n",
      "4467 tensor(19623.8047, grad_fn=<MseLossBackward0>)\n",
      "4468 tensor(19621.3164, grad_fn=<MseLossBackward0>)\n",
      "4469 tensor(19618.8340, grad_fn=<MseLossBackward0>)\n",
      "4470 tensor(19616.3535, grad_fn=<MseLossBackward0>)\n",
      "4471 tensor(19613.8789, grad_fn=<MseLossBackward0>)\n",
      "4472 tensor(19611.4102, grad_fn=<MseLossBackward0>)\n",
      "4473 tensor(19608.9453, grad_fn=<MseLossBackward0>)\n",
      "4474 tensor(19606.4844, grad_fn=<MseLossBackward0>)\n",
      "4475 tensor(19604.0293, grad_fn=<MseLossBackward0>)\n",
      "4476 tensor(19601.5801, grad_fn=<MseLossBackward0>)\n",
      "4477 tensor(19599.1348, grad_fn=<MseLossBackward0>)\n",
      "4478 tensor(19596.6914, grad_fn=<MseLossBackward0>)\n",
      "4479 tensor(19594.2559, grad_fn=<MseLossBackward0>)\n",
      "4480 tensor(19591.8242, grad_fn=<MseLossBackward0>)\n",
      "4481 tensor(19589.3965, grad_fn=<MseLossBackward0>)\n",
      "4482 tensor(19586.9766, grad_fn=<MseLossBackward0>)\n",
      "4483 tensor(19584.5566, grad_fn=<MseLossBackward0>)\n",
      "4484 tensor(19582.1445, grad_fn=<MseLossBackward0>)\n",
      "4485 tensor(19579.7383, grad_fn=<MseLossBackward0>)\n",
      "4486 tensor(19577.3359, grad_fn=<MseLossBackward0>)\n",
      "4487 tensor(19574.9355, grad_fn=<MseLossBackward0>)\n",
      "4488 tensor(19572.5410, grad_fn=<MseLossBackward0>)\n",
      "4489 tensor(19570.1523, grad_fn=<MseLossBackward0>)\n",
      "4490 tensor(19567.7676, grad_fn=<MseLossBackward0>)\n",
      "4491 tensor(19565.3887, grad_fn=<MseLossBackward0>)\n",
      "4492 tensor(19563.0137, grad_fn=<MseLossBackward0>)\n",
      "4493 tensor(19560.6426, grad_fn=<MseLossBackward0>)\n",
      "4494 tensor(19558.2773, grad_fn=<MseLossBackward0>)\n",
      "4495 tensor(19555.9141, grad_fn=<MseLossBackward0>)\n",
      "4496 tensor(19553.5605, grad_fn=<MseLossBackward0>)\n",
      "4497 tensor(19551.2070, grad_fn=<MseLossBackward0>)\n",
      "4498 tensor(19548.8594, grad_fn=<MseLossBackward0>)\n",
      "4499 tensor(19546.5156, grad_fn=<MseLossBackward0>)\n",
      "4500 tensor(19544.1777, grad_fn=<MseLossBackward0>)\n",
      "4501 tensor(19541.8457, grad_fn=<MseLossBackward0>)\n",
      "4502 tensor(19539.5176, grad_fn=<MseLossBackward0>)\n",
      "4503 tensor(19537.1934, grad_fn=<MseLossBackward0>)\n",
      "4504 tensor(19534.8711, grad_fn=<MseLossBackward0>)\n",
      "4505 tensor(19532.5586, grad_fn=<MseLossBackward0>)\n",
      "4506 tensor(19530.2461, grad_fn=<MseLossBackward0>)\n",
      "4507 tensor(19527.9414, grad_fn=<MseLossBackward0>)\n",
      "4508 tensor(19525.6387, grad_fn=<MseLossBackward0>)\n",
      "4509 tensor(19523.3438, grad_fn=<MseLossBackward0>)\n",
      "4510 tensor(19521.0488, grad_fn=<MseLossBackward0>)\n",
      "4511 tensor(19518.7598, grad_fn=<MseLossBackward0>)\n",
      "4512 tensor(19516.4785, grad_fn=<MseLossBackward0>)\n",
      "4513 tensor(19514.1992, grad_fn=<MseLossBackward0>)\n",
      "4514 tensor(19511.9258, grad_fn=<MseLossBackward0>)\n",
      "4515 tensor(19509.6562, grad_fn=<MseLossBackward0>)\n",
      "4516 tensor(19507.3926, grad_fn=<MseLossBackward0>)\n",
      "4517 tensor(19505.1270, grad_fn=<MseLossBackward0>)\n",
      "4518 tensor(19502.8730, grad_fn=<MseLossBackward0>)\n",
      "4519 tensor(19500.6211, grad_fn=<MseLossBackward0>)\n",
      "4520 tensor(19498.3750, grad_fn=<MseLossBackward0>)\n",
      "4521 tensor(19496.1309, grad_fn=<MseLossBackward0>)\n",
      "4522 tensor(19493.8926, grad_fn=<MseLossBackward0>)\n",
      "4523 tensor(19491.6562, grad_fn=<MseLossBackward0>)\n",
      "4524 tensor(19489.4297, grad_fn=<MseLossBackward0>)\n",
      "4525 tensor(19487.2051, grad_fn=<MseLossBackward0>)\n",
      "4526 tensor(19484.9824, grad_fn=<MseLossBackward0>)\n",
      "4527 tensor(19482.7676, grad_fn=<MseLossBackward0>)\n",
      "4528 tensor(19480.5566, grad_fn=<MseLossBackward0>)\n",
      "4529 tensor(19478.3477, grad_fn=<MseLossBackward0>)\n",
      "4530 tensor(19476.1465, grad_fn=<MseLossBackward0>)\n",
      "4531 tensor(19473.9473, grad_fn=<MseLossBackward0>)\n",
      "4532 tensor(19471.7539, grad_fn=<MseLossBackward0>)\n",
      "4533 tensor(19469.5625, grad_fn=<MseLossBackward0>)\n",
      "4534 tensor(19467.3770, grad_fn=<MseLossBackward0>)\n",
      "4535 tensor(19465.1953, grad_fn=<MseLossBackward0>)\n",
      "4536 tensor(19463.0195, grad_fn=<MseLossBackward0>)\n",
      "4537 tensor(19460.8477, grad_fn=<MseLossBackward0>)\n",
      "4538 tensor(19458.6797, grad_fn=<MseLossBackward0>)\n",
      "4539 tensor(19456.5156, grad_fn=<MseLossBackward0>)\n",
      "4540 tensor(19454.3555, grad_fn=<MseLossBackward0>)\n",
      "4541 tensor(19452.2012, grad_fn=<MseLossBackward0>)\n",
      "4542 tensor(19450.0508, grad_fn=<MseLossBackward0>)\n",
      "4543 tensor(19447.9043, grad_fn=<MseLossBackward0>)\n",
      "4544 tensor(19445.7617, grad_fn=<MseLossBackward0>)\n",
      "4545 tensor(19443.6230, grad_fn=<MseLossBackward0>)\n",
      "4546 tensor(19441.4941, grad_fn=<MseLossBackward0>)\n",
      "4547 tensor(19439.3613, grad_fn=<MseLossBackward0>)\n",
      "4548 tensor(19437.2363, grad_fn=<MseLossBackward0>)\n",
      "4549 tensor(19435.1172, grad_fn=<MseLossBackward0>)\n",
      "4550 tensor(19433., grad_fn=<MseLossBackward0>)\n",
      "4551 tensor(19430.8867, grad_fn=<MseLossBackward0>)\n",
      "4552 tensor(19428.7793, grad_fn=<MseLossBackward0>)\n",
      "4553 tensor(19426.6797, grad_fn=<MseLossBackward0>)\n",
      "4554 tensor(19424.5781, grad_fn=<MseLossBackward0>)\n",
      "4555 tensor(19422.4824, grad_fn=<MseLossBackward0>)\n",
      "4556 tensor(19420.3906, grad_fn=<MseLossBackward0>)\n",
      "4557 tensor(19418.3047, grad_fn=<MseLossBackward0>)\n",
      "4558 tensor(19416.2246, grad_fn=<MseLossBackward0>)\n",
      "4559 tensor(19414.1465, grad_fn=<MseLossBackward0>)\n",
      "4560 tensor(19412.0703, grad_fn=<MseLossBackward0>)\n",
      "4561 tensor(19410.0020, grad_fn=<MseLossBackward0>)\n",
      "4562 tensor(19407.9375, grad_fn=<MseLossBackward0>)\n",
      "4563 tensor(19405.8730, grad_fn=<MseLossBackward0>)\n",
      "4564 tensor(19403.8184, grad_fn=<MseLossBackward0>)\n",
      "4565 tensor(19401.7656, grad_fn=<MseLossBackward0>)\n",
      "4566 tensor(19399.7168, grad_fn=<MseLossBackward0>)\n",
      "4567 tensor(19397.6699, grad_fn=<MseLossBackward0>)\n",
      "4568 tensor(19395.6309, grad_fn=<MseLossBackward0>)\n",
      "4569 tensor(19393.5938, grad_fn=<MseLossBackward0>)\n",
      "4570 tensor(19391.5625, grad_fn=<MseLossBackward0>)\n",
      "4571 tensor(19389.5352, grad_fn=<MseLossBackward0>)\n",
      "4572 tensor(19387.5098, grad_fn=<MseLossBackward0>)\n",
      "4573 tensor(19385.4902, grad_fn=<MseLossBackward0>)\n",
      "4574 tensor(19383.4746, grad_fn=<MseLossBackward0>)\n",
      "4575 tensor(19381.4629, grad_fn=<MseLossBackward0>)\n",
      "4576 tensor(19379.4570, grad_fn=<MseLossBackward0>)\n",
      "4577 tensor(19377.4531, grad_fn=<MseLossBackward0>)\n",
      "4578 tensor(19375.4531, grad_fn=<MseLossBackward0>)\n",
      "4579 tensor(19373.4570, grad_fn=<MseLossBackward0>)\n",
      "4580 tensor(19371.4668, grad_fn=<MseLossBackward0>)\n",
      "4581 tensor(19369.4805, grad_fn=<MseLossBackward0>)\n",
      "4582 tensor(19367.4980, grad_fn=<MseLossBackward0>)\n",
      "4583 tensor(19365.5176, grad_fn=<MseLossBackward0>)\n",
      "4584 tensor(19363.5430, grad_fn=<MseLossBackward0>)\n",
      "4585 tensor(19361.5742, grad_fn=<MseLossBackward0>)\n",
      "4586 tensor(19359.6055, grad_fn=<MseLossBackward0>)\n",
      "4587 tensor(19357.6445, grad_fn=<MseLossBackward0>)\n",
      "4588 tensor(19355.6855, grad_fn=<MseLossBackward0>)\n",
      "4589 tensor(19353.7305, grad_fn=<MseLossBackward0>)\n",
      "4590 tensor(19351.7793, grad_fn=<MseLossBackward0>)\n",
      "4591 tensor(19349.8340, grad_fn=<MseLossBackward0>)\n",
      "4592 tensor(19347.8906, grad_fn=<MseLossBackward0>)\n",
      "4593 tensor(19345.9531, grad_fn=<MseLossBackward0>)\n",
      "4594 tensor(19344.0176, grad_fn=<MseLossBackward0>)\n",
      "4595 tensor(19342.0840, grad_fn=<MseLossBackward0>)\n",
      "4596 tensor(19340.1602, grad_fn=<MseLossBackward0>)\n",
      "4597 tensor(19338.2363, grad_fn=<MseLossBackward0>)\n",
      "4598 tensor(19336.3184, grad_fn=<MseLossBackward0>)\n",
      "4599 tensor(19334.4043, grad_fn=<MseLossBackward0>)\n",
      "4600 tensor(19332.4941, grad_fn=<MseLossBackward0>)\n",
      "4601 tensor(19330.5859, grad_fn=<MseLossBackward0>)\n",
      "4602 tensor(19328.6836, grad_fn=<MseLossBackward0>)\n",
      "4603 tensor(19326.7832, grad_fn=<MseLossBackward0>)\n",
      "4604 tensor(19324.8867, grad_fn=<MseLossBackward0>)\n",
      "4605 tensor(19322.9961, grad_fn=<MseLossBackward0>)\n",
      "4606 tensor(19321.1113, grad_fn=<MseLossBackward0>)\n",
      "4607 tensor(19319.2246, grad_fn=<MseLossBackward0>)\n",
      "4608 tensor(19317.3477, grad_fn=<MseLossBackward0>)\n",
      "4609 tensor(19315.4727, grad_fn=<MseLossBackward0>)\n",
      "4610 tensor(19313.5996, grad_fn=<MseLossBackward0>)\n",
      "4611 tensor(19311.7305, grad_fn=<MseLossBackward0>)\n",
      "4612 tensor(19309.8652, grad_fn=<MseLossBackward0>)\n",
      "4613 tensor(19308.0078, grad_fn=<MseLossBackward0>)\n",
      "4614 tensor(19306.1504, grad_fn=<MseLossBackward0>)\n",
      "4615 tensor(19304.2988, grad_fn=<MseLossBackward0>)\n",
      "4616 tensor(19302.4492, grad_fn=<MseLossBackward0>)\n",
      "4617 tensor(19300.6035, grad_fn=<MseLossBackward0>)\n",
      "4618 tensor(19298.7637, grad_fn=<MseLossBackward0>)\n",
      "4619 tensor(19296.9277, grad_fn=<MseLossBackward0>)\n",
      "4620 tensor(19295.0918, grad_fn=<MseLossBackward0>)\n",
      "4621 tensor(19293.2656, grad_fn=<MseLossBackward0>)\n",
      "4622 tensor(19291.4375, grad_fn=<MseLossBackward0>)\n",
      "4623 tensor(19289.6191, grad_fn=<MseLossBackward0>)\n",
      "4624 tensor(19287.7988, grad_fn=<MseLossBackward0>)\n",
      "4625 tensor(19285.9844, grad_fn=<MseLossBackward0>)\n",
      "4626 tensor(19284.1758, grad_fn=<MseLossBackward0>)\n",
      "4627 tensor(19282.3691, grad_fn=<MseLossBackward0>)\n",
      "4628 tensor(19280.5664, grad_fn=<MseLossBackward0>)\n",
      "4629 tensor(19278.7656, grad_fn=<MseLossBackward0>)\n",
      "4630 tensor(19276.9707, grad_fn=<MseLossBackward0>)\n",
      "4631 tensor(19275.1777, grad_fn=<MseLossBackward0>)\n",
      "4632 tensor(19273.3906, grad_fn=<MseLossBackward0>)\n",
      "4633 tensor(19271.6074, grad_fn=<MseLossBackward0>)\n",
      "4634 tensor(19269.8281, grad_fn=<MseLossBackward0>)\n",
      "4635 tensor(19268.0508, grad_fn=<MseLossBackward0>)\n",
      "4636 tensor(19266.2773, grad_fn=<MseLossBackward0>)\n",
      "4637 tensor(19264.5078, grad_fn=<MseLossBackward0>)\n",
      "4638 tensor(19262.7422, grad_fn=<MseLossBackward0>)\n",
      "4639 tensor(19260.9824, grad_fn=<MseLossBackward0>)\n",
      "4640 tensor(19259.2246, grad_fn=<MseLossBackward0>)\n",
      "4641 tensor(19257.4707, grad_fn=<MseLossBackward0>)\n",
      "4642 tensor(19255.7188, grad_fn=<MseLossBackward0>)\n",
      "4643 tensor(19253.9688, grad_fn=<MseLossBackward0>)\n",
      "4644 tensor(19252.2266, grad_fn=<MseLossBackward0>)\n",
      "4645 tensor(19250.4863, grad_fn=<MseLossBackward0>)\n",
      "4646 tensor(19248.7500, grad_fn=<MseLossBackward0>)\n",
      "4647 tensor(19247.0176, grad_fn=<MseLossBackward0>)\n",
      "4648 tensor(19245.2930, grad_fn=<MseLossBackward0>)\n",
      "4649 tensor(19243.5645, grad_fn=<MseLossBackward0>)\n",
      "4650 tensor(19241.8438, grad_fn=<MseLossBackward0>)\n",
      "4651 tensor(19240.1270, grad_fn=<MseLossBackward0>)\n",
      "4652 tensor(19238.4121, grad_fn=<MseLossBackward0>)\n",
      "4653 tensor(19236.7031, grad_fn=<MseLossBackward0>)\n",
      "4654 tensor(19234.9922, grad_fn=<MseLossBackward0>)\n",
      "4655 tensor(19233.2930, grad_fn=<MseLossBackward0>)\n",
      "4656 tensor(19231.5918, grad_fn=<MseLossBackward0>)\n",
      "4657 tensor(19229.8945, grad_fn=<MseLossBackward0>)\n",
      "4658 tensor(19228.2031, grad_fn=<MseLossBackward0>)\n",
      "4659 tensor(19226.5137, grad_fn=<MseLossBackward0>)\n",
      "4660 tensor(19224.8281, grad_fn=<MseLossBackward0>)\n",
      "4661 tensor(19223.1465, grad_fn=<MseLossBackward0>)\n",
      "4662 tensor(19221.4668, grad_fn=<MseLossBackward0>)\n",
      "4663 tensor(19219.7949, grad_fn=<MseLossBackward0>)\n",
      "4664 tensor(19218.1230, grad_fn=<MseLossBackward0>)\n",
      "4665 tensor(19216.4551, grad_fn=<MseLossBackward0>)\n",
      "4666 tensor(19214.7910, grad_fn=<MseLossBackward0>)\n",
      "4667 tensor(19213.1309, grad_fn=<MseLossBackward0>)\n",
      "4668 tensor(19211.4727, grad_fn=<MseLossBackward0>)\n",
      "4669 tensor(19209.8203, grad_fn=<MseLossBackward0>)\n",
      "4670 tensor(19208.1680, grad_fn=<MseLossBackward0>)\n",
      "4671 tensor(19206.5234, grad_fn=<MseLossBackward0>)\n",
      "4672 tensor(19204.8828, grad_fn=<MseLossBackward0>)\n",
      "4673 tensor(19203.2422, grad_fn=<MseLossBackward0>)\n",
      "4674 tensor(19201.6055, grad_fn=<MseLossBackward0>)\n",
      "4675 tensor(19199.9727, grad_fn=<MseLossBackward0>)\n",
      "4676 tensor(19198.3457, grad_fn=<MseLossBackward0>)\n",
      "4677 tensor(19196.7188, grad_fn=<MseLossBackward0>)\n",
      "4678 tensor(19195.0957, grad_fn=<MseLossBackward0>)\n",
      "4679 tensor(19193.4766, grad_fn=<MseLossBackward0>)\n",
      "4680 tensor(19191.8613, grad_fn=<MseLossBackward0>)\n",
      "4681 tensor(19190.2500, grad_fn=<MseLossBackward0>)\n",
      "4682 tensor(19188.6406, grad_fn=<MseLossBackward0>)\n",
      "4683 tensor(19187.0371, grad_fn=<MseLossBackward0>)\n",
      "4684 tensor(19185.4355, grad_fn=<MseLossBackward0>)\n",
      "4685 tensor(19183.8359, grad_fn=<MseLossBackward0>)\n",
      "4686 tensor(19182.2422, grad_fn=<MseLossBackward0>)\n",
      "4687 tensor(19180.6523, grad_fn=<MseLossBackward0>)\n",
      "4688 tensor(19179.0645, grad_fn=<MseLossBackward0>)\n",
      "4689 tensor(19177.4805, grad_fn=<MseLossBackward0>)\n",
      "4690 tensor(19175.8984, grad_fn=<MseLossBackward0>)\n",
      "4691 tensor(19174.3203, grad_fn=<MseLossBackward0>)\n",
      "4692 tensor(19172.7461, grad_fn=<MseLossBackward0>)\n",
      "4693 tensor(19171.1758, grad_fn=<MseLossBackward0>)\n",
      "4694 tensor(19169.6094, grad_fn=<MseLossBackward0>)\n",
      "4695 tensor(19168.0430, grad_fn=<MseLossBackward0>)\n",
      "4696 tensor(19166.4863, grad_fn=<MseLossBackward0>)\n",
      "4697 tensor(19164.9277, grad_fn=<MseLossBackward0>)\n",
      "4698 tensor(19163.3730, grad_fn=<MseLossBackward0>)\n",
      "4699 tensor(19161.8203, grad_fn=<MseLossBackward0>)\n",
      "4700 tensor(19160.2754, grad_fn=<MseLossBackward0>)\n",
      "4701 tensor(19158.7305, grad_fn=<MseLossBackward0>)\n",
      "4702 tensor(19157.1895, grad_fn=<MseLossBackward0>)\n",
      "4703 tensor(19155.6523, grad_fn=<MseLossBackward0>)\n",
      "4704 tensor(19154.1191, grad_fn=<MseLossBackward0>)\n",
      "4705 tensor(19152.5859, grad_fn=<MseLossBackward0>)\n",
      "4706 tensor(19151.0605, grad_fn=<MseLossBackward0>)\n",
      "4707 tensor(19149.5352, grad_fn=<MseLossBackward0>)\n",
      "4708 tensor(19148.0137, grad_fn=<MseLossBackward0>)\n",
      "4709 tensor(19146.5000, grad_fn=<MseLossBackward0>)\n",
      "4710 tensor(19144.9844, grad_fn=<MseLossBackward0>)\n",
      "4711 tensor(19143.4707, grad_fn=<MseLossBackward0>)\n",
      "4712 tensor(19141.9648, grad_fn=<MseLossBackward0>)\n",
      "4713 tensor(19140.4590, grad_fn=<MseLossBackward0>)\n",
      "4714 tensor(19138.9570, grad_fn=<MseLossBackward0>)\n",
      "4715 tensor(19137.4590, grad_fn=<MseLossBackward0>)\n",
      "4716 tensor(19135.9648, grad_fn=<MseLossBackward0>)\n",
      "4717 tensor(19134.4727, grad_fn=<MseLossBackward0>)\n",
      "4718 tensor(19132.9844, grad_fn=<MseLossBackward0>)\n",
      "4719 tensor(19131.5000, grad_fn=<MseLossBackward0>)\n",
      "4720 tensor(19130.0176, grad_fn=<MseLossBackward0>)\n",
      "4721 tensor(19128.5391, grad_fn=<MseLossBackward0>)\n",
      "4722 tensor(19127.0645, grad_fn=<MseLossBackward0>)\n",
      "4723 tensor(19125.5918, grad_fn=<MseLossBackward0>)\n",
      "4724 tensor(19124.1230, grad_fn=<MseLossBackward0>)\n",
      "4725 tensor(19122.6562, grad_fn=<MseLossBackward0>)\n",
      "4726 tensor(19121.1934, grad_fn=<MseLossBackward0>)\n",
      "4727 tensor(19119.7344, grad_fn=<MseLossBackward0>)\n",
      "4728 tensor(19118.2773, grad_fn=<MseLossBackward0>)\n",
      "4729 tensor(19116.8242, grad_fn=<MseLossBackward0>)\n",
      "4730 tensor(19115.3770, grad_fn=<MseLossBackward0>)\n",
      "4731 tensor(19113.9277, grad_fn=<MseLossBackward0>)\n",
      "4732 tensor(19112.4824, grad_fn=<MseLossBackward0>)\n",
      "4733 tensor(19111.0430, grad_fn=<MseLossBackward0>)\n",
      "4734 tensor(19109.6055, grad_fn=<MseLossBackward0>)\n",
      "4735 tensor(19108.1719, grad_fn=<MseLossBackward0>)\n",
      "4736 tensor(19106.7402, grad_fn=<MseLossBackward0>)\n",
      "4737 tensor(19105.3105, grad_fn=<MseLossBackward0>)\n",
      "4738 tensor(19103.8867, grad_fn=<MseLossBackward0>)\n",
      "4739 tensor(19102.4668, grad_fn=<MseLossBackward0>)\n",
      "4740 tensor(19101.0469, grad_fn=<MseLossBackward0>)\n",
      "4741 tensor(19099.6309, grad_fn=<MseLossBackward0>)\n",
      "4742 tensor(19098.2188, grad_fn=<MseLossBackward0>)\n",
      "4743 tensor(19096.8086, grad_fn=<MseLossBackward0>)\n",
      "4744 tensor(19095.4023, grad_fn=<MseLossBackward0>)\n",
      "4745 tensor(19094., grad_fn=<MseLossBackward0>)\n",
      "4746 tensor(19092.5996, grad_fn=<MseLossBackward0>)\n",
      "4747 tensor(19091.2012, grad_fn=<MseLossBackward0>)\n",
      "4748 tensor(19089.8066, grad_fn=<MseLossBackward0>)\n",
      "4749 tensor(19088.4180, grad_fn=<MseLossBackward0>)\n",
      "4750 tensor(19087.0293, grad_fn=<MseLossBackward0>)\n",
      "4751 tensor(19085.6445, grad_fn=<MseLossBackward0>)\n",
      "4752 tensor(19084.2617, grad_fn=<MseLossBackward0>)\n",
      "4753 tensor(19082.8848, grad_fn=<MseLossBackward0>)\n",
      "4754 tensor(19081.5078, grad_fn=<MseLossBackward0>)\n",
      "4755 tensor(19080.1348, grad_fn=<MseLossBackward0>)\n",
      "4756 tensor(19078.7656, grad_fn=<MseLossBackward0>)\n",
      "4757 tensor(19077.3965, grad_fn=<MseLossBackward0>)\n",
      "4758 tensor(19076.0352, grad_fn=<MseLossBackward0>)\n",
      "4759 tensor(19074.6719, grad_fn=<MseLossBackward0>)\n",
      "4760 tensor(19073.3145, grad_fn=<MseLossBackward0>)\n",
      "4761 tensor(19071.9590, grad_fn=<MseLossBackward0>)\n",
      "4762 tensor(19070.6074, grad_fn=<MseLossBackward0>)\n",
      "4763 tensor(19069.2578, grad_fn=<MseLossBackward0>)\n",
      "4764 tensor(19067.9141, grad_fn=<MseLossBackward0>)\n",
      "4765 tensor(19066.5703, grad_fn=<MseLossBackward0>)\n",
      "4766 tensor(19065.2305, grad_fn=<MseLossBackward0>)\n",
      "4767 tensor(19063.8926, grad_fn=<MseLossBackward0>)\n",
      "4768 tensor(19062.5566, grad_fn=<MseLossBackward0>)\n",
      "4769 tensor(19061.2266, grad_fn=<MseLossBackward0>)\n",
      "4770 tensor(19059.8984, grad_fn=<MseLossBackward0>)\n",
      "4771 tensor(19058.5723, grad_fn=<MseLossBackward0>)\n",
      "4772 tensor(19057.2520, grad_fn=<MseLossBackward0>)\n",
      "4773 tensor(19055.9297, grad_fn=<MseLossBackward0>)\n",
      "4774 tensor(19054.6152, grad_fn=<MseLossBackward0>)\n",
      "4775 tensor(19053.3008, grad_fn=<MseLossBackward0>)\n",
      "4776 tensor(19051.9902, grad_fn=<MseLossBackward0>)\n",
      "4777 tensor(19050.6816, grad_fn=<MseLossBackward0>)\n",
      "4778 tensor(19049.3770, grad_fn=<MseLossBackward0>)\n",
      "4779 tensor(19048.0742, grad_fn=<MseLossBackward0>)\n",
      "4780 tensor(19046.7773, grad_fn=<MseLossBackward0>)\n",
      "4781 tensor(19045.4785, grad_fn=<MseLossBackward0>)\n",
      "4782 tensor(19044.1875, grad_fn=<MseLossBackward0>)\n",
      "4783 tensor(19042.8945, grad_fn=<MseLossBackward0>)\n",
      "4784 tensor(19041.6074, grad_fn=<MseLossBackward0>)\n",
      "4785 tensor(19040.3203, grad_fn=<MseLossBackward0>)\n",
      "4786 tensor(19039.0410, grad_fn=<MseLossBackward0>)\n",
      "4787 tensor(19037.7598, grad_fn=<MseLossBackward0>)\n",
      "4788 tensor(19036.4844, grad_fn=<MseLossBackward0>)\n",
      "4789 tensor(19035.2109, grad_fn=<MseLossBackward0>)\n",
      "4790 tensor(19033.9414, grad_fn=<MseLossBackward0>)\n",
      "4791 tensor(19032.6719, grad_fn=<MseLossBackward0>)\n",
      "4792 tensor(19031.4082, grad_fn=<MseLossBackward0>)\n",
      "4793 tensor(19030.1445, grad_fn=<MseLossBackward0>)\n",
      "4794 tensor(19028.8867, grad_fn=<MseLossBackward0>)\n",
      "4795 tensor(19027.6289, grad_fn=<MseLossBackward0>)\n",
      "4796 tensor(19026.3750, grad_fn=<MseLossBackward0>)\n",
      "4797 tensor(19025.1250, grad_fn=<MseLossBackward0>)\n",
      "4798 tensor(19023.8750, grad_fn=<MseLossBackward0>)\n",
      "4799 tensor(19022.6309, grad_fn=<MseLossBackward0>)\n",
      "4800 tensor(19021.3867, grad_fn=<MseLossBackward0>)\n",
      "4801 tensor(19020.1465, grad_fn=<MseLossBackward0>)\n",
      "4802 tensor(19018.9082, grad_fn=<MseLossBackward0>)\n",
      "4803 tensor(19017.6758, grad_fn=<MseLossBackward0>)\n",
      "4804 tensor(19016.4434, grad_fn=<MseLossBackward0>)\n",
      "4805 tensor(19015.2148, grad_fn=<MseLossBackward0>)\n",
      "4806 tensor(19013.9883, grad_fn=<MseLossBackward0>)\n",
      "4807 tensor(19012.7637, grad_fn=<MseLossBackward0>)\n",
      "4808 tensor(19011.5430, grad_fn=<MseLossBackward0>)\n",
      "4809 tensor(19010.3262, grad_fn=<MseLossBackward0>)\n",
      "4810 tensor(19009.1113, grad_fn=<MseLossBackward0>)\n",
      "4811 tensor(19007.8965, grad_fn=<MseLossBackward0>)\n",
      "4812 tensor(19006.6875, grad_fn=<MseLossBackward0>)\n",
      "4813 tensor(19005.4785, grad_fn=<MseLossBackward0>)\n",
      "4814 tensor(19004.2754, grad_fn=<MseLossBackward0>)\n",
      "4815 tensor(19003.0742, grad_fn=<MseLossBackward0>)\n",
      "4816 tensor(19001.8730, grad_fn=<MseLossBackward0>)\n",
      "4817 tensor(19000.6797, grad_fn=<MseLossBackward0>)\n",
      "4818 tensor(18999.4844, grad_fn=<MseLossBackward0>)\n",
      "4819 tensor(18998.2930, grad_fn=<MseLossBackward0>)\n",
      "4820 tensor(18997.1035, grad_fn=<MseLossBackward0>)\n",
      "4821 tensor(18995.9160, grad_fn=<MseLossBackward0>)\n",
      "4822 tensor(18994.7344, grad_fn=<MseLossBackward0>)\n",
      "4823 tensor(18993.5547, grad_fn=<MseLossBackward0>)\n",
      "4824 tensor(18992.3750, grad_fn=<MseLossBackward0>)\n",
      "4825 tensor(18991.2031, grad_fn=<MseLossBackward0>)\n",
      "4826 tensor(18990.0293, grad_fn=<MseLossBackward0>)\n",
      "4827 tensor(18988.8574, grad_fn=<MseLossBackward0>)\n",
      "4828 tensor(18987.6914, grad_fn=<MseLossBackward0>)\n",
      "4829 tensor(18986.5254, grad_fn=<MseLossBackward0>)\n",
      "4830 tensor(18985.3652, grad_fn=<MseLossBackward0>)\n",
      "4831 tensor(18984.2051, grad_fn=<MseLossBackward0>)\n",
      "4832 tensor(18983.0469, grad_fn=<MseLossBackward0>)\n",
      "4833 tensor(18981.8945, grad_fn=<MseLossBackward0>)\n",
      "4834 tensor(18980.7402, grad_fn=<MseLossBackward0>)\n",
      "4835 tensor(18979.5938, grad_fn=<MseLossBackward0>)\n",
      "4836 tensor(18978.4453, grad_fn=<MseLossBackward0>)\n",
      "4837 tensor(18977.3008, grad_fn=<MseLossBackward0>)\n",
      "4838 tensor(18976.1602, grad_fn=<MseLossBackward0>)\n",
      "4839 tensor(18975.0195, grad_fn=<MseLossBackward0>)\n",
      "4840 tensor(18973.8848, grad_fn=<MseLossBackward0>)\n",
      "4841 tensor(18972.7500, grad_fn=<MseLossBackward0>)\n",
      "4842 tensor(18971.6191, grad_fn=<MseLossBackward0>)\n",
      "4843 tensor(18970.4902, grad_fn=<MseLossBackward0>)\n",
      "4844 tensor(18969.3652, grad_fn=<MseLossBackward0>)\n",
      "4845 tensor(18968.2422, grad_fn=<MseLossBackward0>)\n",
      "4846 tensor(18967.1211, grad_fn=<MseLossBackward0>)\n",
      "4847 tensor(18966., grad_fn=<MseLossBackward0>)\n",
      "4848 tensor(18964.8867, grad_fn=<MseLossBackward0>)\n",
      "4849 tensor(18963.7695, grad_fn=<MseLossBackward0>)\n",
      "4850 tensor(18962.6602, grad_fn=<MseLossBackward0>)\n",
      "4851 tensor(18961.5508, grad_fn=<MseLossBackward0>)\n",
      "4852 tensor(18960.4473, grad_fn=<MseLossBackward0>)\n",
      "4853 tensor(18959.3438, grad_fn=<MseLossBackward0>)\n",
      "4854 tensor(18958.2402, grad_fn=<MseLossBackward0>)\n",
      "4855 tensor(18957.1445, grad_fn=<MseLossBackward0>)\n",
      "4856 tensor(18956.0469, grad_fn=<MseLossBackward0>)\n",
      "4857 tensor(18954.9531, grad_fn=<MseLossBackward0>)\n",
      "4858 tensor(18953.8633, grad_fn=<MseLossBackward0>)\n",
      "4859 tensor(18952.7715, grad_fn=<MseLossBackward0>)\n",
      "4860 tensor(18951.6875, grad_fn=<MseLossBackward0>)\n",
      "4861 tensor(18950.6055, grad_fn=<MseLossBackward0>)\n",
      "4862 tensor(18949.5234, grad_fn=<MseLossBackward0>)\n",
      "4863 tensor(18948.4434, grad_fn=<MseLossBackward0>)\n",
      "4864 tensor(18947.3672, grad_fn=<MseLossBackward0>)\n",
      "4865 tensor(18946.2930, grad_fn=<MseLossBackward0>)\n",
      "4866 tensor(18945.2227, grad_fn=<MseLossBackward0>)\n",
      "4867 tensor(18944.1523, grad_fn=<MseLossBackward0>)\n",
      "4868 tensor(18943.0859, grad_fn=<MseLossBackward0>)\n",
      "4869 tensor(18942.0215, grad_fn=<MseLossBackward0>)\n",
      "4870 tensor(18940.9609, grad_fn=<MseLossBackward0>)\n",
      "4871 tensor(18939.9004, grad_fn=<MseLossBackward0>)\n",
      "4872 tensor(18938.8438, grad_fn=<MseLossBackward0>)\n",
      "4873 tensor(18937.7871, grad_fn=<MseLossBackward0>)\n",
      "4874 tensor(18936.7363, grad_fn=<MseLossBackward0>)\n",
      "4875 tensor(18935.6855, grad_fn=<MseLossBackward0>)\n",
      "4876 tensor(18934.6387, grad_fn=<MseLossBackward0>)\n",
      "4877 tensor(18933.5938, grad_fn=<MseLossBackward0>)\n",
      "4878 tensor(18932.5508, grad_fn=<MseLossBackward0>)\n",
      "4879 tensor(18931.5098, grad_fn=<MseLossBackward0>)\n",
      "4880 tensor(18930.4727, grad_fn=<MseLossBackward0>)\n",
      "4881 tensor(18929.4375, grad_fn=<MseLossBackward0>)\n",
      "4882 tensor(18928.4023, grad_fn=<MseLossBackward0>)\n",
      "4883 tensor(18927.3730, grad_fn=<MseLossBackward0>)\n",
      "4884 tensor(18926.3438, grad_fn=<MseLossBackward0>)\n",
      "4885 tensor(18925.3164, grad_fn=<MseLossBackward0>)\n",
      "4886 tensor(18924.2930, grad_fn=<MseLossBackward0>)\n",
      "4887 tensor(18923.2695, grad_fn=<MseLossBackward0>)\n",
      "4888 tensor(18922.2520, grad_fn=<MseLossBackward0>)\n",
      "4889 tensor(18921.2344, grad_fn=<MseLossBackward0>)\n",
      "4890 tensor(18920.2227, grad_fn=<MseLossBackward0>)\n",
      "4891 tensor(18919.2090, grad_fn=<MseLossBackward0>)\n",
      "4892 tensor(18918.1992, grad_fn=<MseLossBackward0>)\n",
      "4893 tensor(18917.1895, grad_fn=<MseLossBackward0>)\n",
      "4894 tensor(18916.1855, grad_fn=<MseLossBackward0>)\n",
      "4895 tensor(18915.1816, grad_fn=<MseLossBackward0>)\n",
      "4896 tensor(18914.1816, grad_fn=<MseLossBackward0>)\n",
      "4897 tensor(18913.1816, grad_fn=<MseLossBackward0>)\n",
      "4898 tensor(18912.1875, grad_fn=<MseLossBackward0>)\n",
      "4899 tensor(18911.1914, grad_fn=<MseLossBackward0>)\n",
      "4900 tensor(18910.1973, grad_fn=<MseLossBackward0>)\n",
      "4901 tensor(18909.2109, grad_fn=<MseLossBackward0>)\n",
      "4902 tensor(18908.2227, grad_fn=<MseLossBackward0>)\n",
      "4903 tensor(18907.2383, grad_fn=<MseLossBackward0>)\n",
      "4904 tensor(18906.2539, grad_fn=<MseLossBackward0>)\n",
      "4905 tensor(18905.2754, grad_fn=<MseLossBackward0>)\n",
      "4906 tensor(18904.2949, grad_fn=<MseLossBackward0>)\n",
      "4907 tensor(18903.3223, grad_fn=<MseLossBackward0>)\n",
      "4908 tensor(18902.3457, grad_fn=<MseLossBackward0>)\n",
      "4909 tensor(18901.3770, grad_fn=<MseLossBackward0>)\n",
      "4910 tensor(18900.4043, grad_fn=<MseLossBackward0>)\n",
      "4911 tensor(18899.4355, grad_fn=<MseLossBackward0>)\n",
      "4912 tensor(18898.4727, grad_fn=<MseLossBackward0>)\n",
      "4913 tensor(18897.5098, grad_fn=<MseLossBackward0>)\n",
      "4914 tensor(18896.5469, grad_fn=<MseLossBackward0>)\n",
      "4915 tensor(18895.5918, grad_fn=<MseLossBackward0>)\n",
      "4916 tensor(18894.6328, grad_fn=<MseLossBackward0>)\n",
      "4917 tensor(18893.6816, grad_fn=<MseLossBackward0>)\n",
      "4918 tensor(18892.7285, grad_fn=<MseLossBackward0>)\n",
      "4919 tensor(18891.7773, grad_fn=<MseLossBackward0>)\n",
      "4920 tensor(18890.8301, grad_fn=<MseLossBackward0>)\n",
      "4921 tensor(18889.8828, grad_fn=<MseLossBackward0>)\n",
      "4922 tensor(18888.9414, grad_fn=<MseLossBackward0>)\n",
      "4923 tensor(18888.0020, grad_fn=<MseLossBackward0>)\n",
      "4924 tensor(18887.0605, grad_fn=<MseLossBackward0>)\n",
      "4925 tensor(18886.1250, grad_fn=<MseLossBackward0>)\n",
      "4926 tensor(18885.1914, grad_fn=<MseLossBackward0>)\n",
      "4927 tensor(18884.2598, grad_fn=<MseLossBackward0>)\n",
      "4928 tensor(18883.3281, grad_fn=<MseLossBackward0>)\n",
      "4929 tensor(18882.4004, grad_fn=<MseLossBackward0>)\n",
      "4930 tensor(18881.4727, grad_fn=<MseLossBackward0>)\n",
      "4931 tensor(18880.5488, grad_fn=<MseLossBackward0>)\n",
      "4932 tensor(18879.6270, grad_fn=<MseLossBackward0>)\n",
      "4933 tensor(18878.7070, grad_fn=<MseLossBackward0>)\n",
      "4934 tensor(18877.7910, grad_fn=<MseLossBackward0>)\n",
      "4935 tensor(18876.8750, grad_fn=<MseLossBackward0>)\n",
      "4936 tensor(18875.9609, grad_fn=<MseLossBackward0>)\n",
      "4937 tensor(18875.0488, grad_fn=<MseLossBackward0>)\n",
      "4938 tensor(18874.1387, grad_fn=<MseLossBackward0>)\n",
      "4939 tensor(18873.2324, grad_fn=<MseLossBackward0>)\n",
      "4940 tensor(18872.3262, grad_fn=<MseLossBackward0>)\n",
      "4941 tensor(18871.4238, grad_fn=<MseLossBackward0>)\n",
      "4942 tensor(18870.5234, grad_fn=<MseLossBackward0>)\n",
      "4943 tensor(18869.6250, grad_fn=<MseLossBackward0>)\n",
      "4944 tensor(18868.7246, grad_fn=<MseLossBackward0>)\n",
      "4945 tensor(18867.8320, grad_fn=<MseLossBackward0>)\n",
      "4946 tensor(18866.9414, grad_fn=<MseLossBackward0>)\n",
      "4947 tensor(18866.0488, grad_fn=<MseLossBackward0>)\n",
      "4948 tensor(18865.1602, grad_fn=<MseLossBackward0>)\n",
      "4949 tensor(18864.2715, grad_fn=<MseLossBackward0>)\n",
      "4950 tensor(18863.3887, grad_fn=<MseLossBackward0>)\n",
      "4951 tensor(18862.5059, grad_fn=<MseLossBackward0>)\n",
      "4952 tensor(18861.6230, grad_fn=<MseLossBackward0>)\n",
      "4953 tensor(18860.7461, grad_fn=<MseLossBackward0>)\n",
      "4954 tensor(18859.8691, grad_fn=<MseLossBackward0>)\n",
      "4955 tensor(18858.9941, grad_fn=<MseLossBackward0>)\n",
      "4956 tensor(18858.1250, grad_fn=<MseLossBackward0>)\n",
      "4957 tensor(18857.2539, grad_fn=<MseLossBackward0>)\n",
      "4958 tensor(18856.3848, grad_fn=<MseLossBackward0>)\n",
      "4959 tensor(18855.5176, grad_fn=<MseLossBackward0>)\n",
      "4960 tensor(18854.6523, grad_fn=<MseLossBackward0>)\n",
      "4961 tensor(18853.7910, grad_fn=<MseLossBackward0>)\n",
      "4962 tensor(18852.9297, grad_fn=<MseLossBackward0>)\n",
      "4963 tensor(18852.0723, grad_fn=<MseLossBackward0>)\n",
      "4964 tensor(18851.2148, grad_fn=<MseLossBackward0>)\n",
      "4965 tensor(18850.3574, grad_fn=<MseLossBackward0>)\n",
      "4966 tensor(18849.5078, grad_fn=<MseLossBackward0>)\n",
      "4967 tensor(18848.6562, grad_fn=<MseLossBackward0>)\n",
      "4968 tensor(18847.8066, grad_fn=<MseLossBackward0>)\n",
      "4969 tensor(18846.9609, grad_fn=<MseLossBackward0>)\n",
      "4970 tensor(18846.1172, grad_fn=<MseLossBackward0>)\n",
      "4971 tensor(18845.2734, grad_fn=<MseLossBackward0>)\n",
      "4972 tensor(18844.4316, grad_fn=<MseLossBackward0>)\n",
      "4973 tensor(18843.5938, grad_fn=<MseLossBackward0>)\n",
      "4974 tensor(18842.7559, grad_fn=<MseLossBackward0>)\n",
      "4975 tensor(18841.9199, grad_fn=<MseLossBackward0>)\n",
      "4976 tensor(18841.0879, grad_fn=<MseLossBackward0>)\n",
      "4977 tensor(18840.2539, grad_fn=<MseLossBackward0>)\n",
      "4978 tensor(18839.4258, grad_fn=<MseLossBackward0>)\n",
      "4979 tensor(18838.5996, grad_fn=<MseLossBackward0>)\n",
      "4980 tensor(18837.7715, grad_fn=<MseLossBackward0>)\n",
      "4981 tensor(18836.9492, grad_fn=<MseLossBackward0>)\n",
      "4982 tensor(18836.1250, grad_fn=<MseLossBackward0>)\n",
      "4983 tensor(18835.3066, grad_fn=<MseLossBackward0>)\n",
      "4984 tensor(18834.4883, grad_fn=<MseLossBackward0>)\n",
      "4985 tensor(18833.6738, grad_fn=<MseLossBackward0>)\n",
      "4986 tensor(18832.8574, grad_fn=<MseLossBackward0>)\n",
      "4987 tensor(18832.0449, grad_fn=<MseLossBackward0>)\n",
      "4988 tensor(18831.2344, grad_fn=<MseLossBackward0>)\n",
      "4989 tensor(18830.4258, grad_fn=<MseLossBackward0>)\n",
      "4990 tensor(18829.6191, grad_fn=<MseLossBackward0>)\n",
      "4991 tensor(18828.8164, grad_fn=<MseLossBackward0>)\n",
      "4992 tensor(18828.0098, grad_fn=<MseLossBackward0>)\n",
      "4993 tensor(18827.2070, grad_fn=<MseLossBackward0>)\n",
      "4994 tensor(18826.4102, grad_fn=<MseLossBackward0>)\n",
      "4995 tensor(18825.6113, grad_fn=<MseLossBackward0>)\n",
      "4996 tensor(18824.8164, grad_fn=<MseLossBackward0>)\n",
      "4997 tensor(18824.0215, grad_fn=<MseLossBackward0>)\n",
      "4998 tensor(18823.2305, grad_fn=<MseLossBackward0>)\n",
      "4999 tensor(18822.4395, grad_fn=<MseLossBackward0>)\n",
      "5000 tensor(18821.6523, grad_fn=<MseLossBackward0>)\n",
      "5001 tensor(18820.8633, grad_fn=<MseLossBackward0>)\n",
      "5002 tensor(18820.0801, grad_fn=<MseLossBackward0>)\n",
      "5003 tensor(18819.2949, grad_fn=<MseLossBackward0>)\n",
      "5004 tensor(18818.5156, grad_fn=<MseLossBackward0>)\n",
      "5005 tensor(18817.7344, grad_fn=<MseLossBackward0>)\n",
      "5006 tensor(18816.9570, grad_fn=<MseLossBackward0>)\n",
      "5007 tensor(18816.1797, grad_fn=<MseLossBackward0>)\n",
      "5008 tensor(18815.4062, grad_fn=<MseLossBackward0>)\n",
      "5009 tensor(18814.6328, grad_fn=<MseLossBackward0>)\n",
      "5010 tensor(18813.8633, grad_fn=<MseLossBackward0>)\n",
      "5011 tensor(18813.0938, grad_fn=<MseLossBackward0>)\n",
      "5012 tensor(18812.3262, grad_fn=<MseLossBackward0>)\n",
      "5013 tensor(18811.5605, grad_fn=<MseLossBackward0>)\n",
      "5014 tensor(18810.7949, grad_fn=<MseLossBackward0>)\n",
      "5015 tensor(18810.0352, grad_fn=<MseLossBackward0>)\n",
      "5016 tensor(18809.2773, grad_fn=<MseLossBackward0>)\n",
      "5017 tensor(18808.5176, grad_fn=<MseLossBackward0>)\n",
      "5018 tensor(18807.7598, grad_fn=<MseLossBackward0>)\n",
      "5019 tensor(18807.0059, grad_fn=<MseLossBackward0>)\n",
      "5020 tensor(18806.2539, grad_fn=<MseLossBackward0>)\n",
      "5021 tensor(18805.5020, grad_fn=<MseLossBackward0>)\n",
      "5022 tensor(18804.7539, grad_fn=<MseLossBackward0>)\n",
      "5023 tensor(18804.0059, grad_fn=<MseLossBackward0>)\n",
      "5024 tensor(18803.2578, grad_fn=<MseLossBackward0>)\n",
      "5025 tensor(18802.5156, grad_fn=<MseLossBackward0>)\n",
      "5026 tensor(18801.7695, grad_fn=<MseLossBackward0>)\n",
      "5027 tensor(18801.0293, grad_fn=<MseLossBackward0>)\n",
      "5028 tensor(18800.2891, grad_fn=<MseLossBackward0>)\n",
      "5029 tensor(18799.5527, grad_fn=<MseLossBackward0>)\n",
      "5030 tensor(18798.8184, grad_fn=<MseLossBackward0>)\n",
      "5031 tensor(18798.0840, grad_fn=<MseLossBackward0>)\n",
      "5032 tensor(18797.3496, grad_fn=<MseLossBackward0>)\n",
      "5033 tensor(18796.6211, grad_fn=<MseLossBackward0>)\n",
      "5034 tensor(18795.8887, grad_fn=<MseLossBackward0>)\n",
      "5035 tensor(18795.1641, grad_fn=<MseLossBackward0>)\n",
      "5036 tensor(18794.4375, grad_fn=<MseLossBackward0>)\n",
      "5037 tensor(18793.7109, grad_fn=<MseLossBackward0>)\n",
      "5038 tensor(18792.9902, grad_fn=<MseLossBackward0>)\n",
      "5039 tensor(18792.2695, grad_fn=<MseLossBackward0>)\n",
      "5040 tensor(18791.5469, grad_fn=<MseLossBackward0>)\n",
      "5041 tensor(18790.8320, grad_fn=<MseLossBackward0>)\n",
      "5042 tensor(18790.1152, grad_fn=<MseLossBackward0>)\n",
      "5043 tensor(18789.4004, grad_fn=<MseLossBackward0>)\n",
      "5044 tensor(18788.6875, grad_fn=<MseLossBackward0>)\n",
      "5045 tensor(18787.9766, grad_fn=<MseLossBackward0>)\n",
      "5046 tensor(18787.2676, grad_fn=<MseLossBackward0>)\n",
      "5047 tensor(18786.5625, grad_fn=<MseLossBackward0>)\n",
      "5048 tensor(18785.8555, grad_fn=<MseLossBackward0>)\n",
      "5049 tensor(18785.1504, grad_fn=<MseLossBackward0>)\n",
      "5050 tensor(18784.4453, grad_fn=<MseLossBackward0>)\n",
      "5051 tensor(18783.7461, grad_fn=<MseLossBackward0>)\n",
      "5052 tensor(18783.0449, grad_fn=<MseLossBackward0>)\n",
      "5053 tensor(18782.3457, grad_fn=<MseLossBackward0>)\n",
      "5054 tensor(18781.6504, grad_fn=<MseLossBackward0>)\n",
      "5055 tensor(18780.9531, grad_fn=<MseLossBackward0>)\n",
      "5056 tensor(18780.2637, grad_fn=<MseLossBackward0>)\n",
      "5057 tensor(18779.5703, grad_fn=<MseLossBackward0>)\n",
      "5058 tensor(18778.8809, grad_fn=<MseLossBackward0>)\n",
      "5059 tensor(18778.1914, grad_fn=<MseLossBackward0>)\n",
      "5060 tensor(18777.5059, grad_fn=<MseLossBackward0>)\n",
      "5061 tensor(18776.8203, grad_fn=<MseLossBackward0>)\n",
      "5062 tensor(18776.1348, grad_fn=<MseLossBackward0>)\n",
      "5063 tensor(18775.4551, grad_fn=<MseLossBackward0>)\n",
      "5064 tensor(18774.7734, grad_fn=<MseLossBackward0>)\n",
      "5065 tensor(18774.0938, grad_fn=<MseLossBackward0>)\n",
      "5066 tensor(18773.4141, grad_fn=<MseLossBackward0>)\n",
      "5067 tensor(18772.7402, grad_fn=<MseLossBackward0>)\n",
      "5068 tensor(18772.0664, grad_fn=<MseLossBackward0>)\n",
      "5069 tensor(18771.3926, grad_fn=<MseLossBackward0>)\n",
      "5070 tensor(18770.7227, grad_fn=<MseLossBackward0>)\n",
      "5071 tensor(18770.0508, grad_fn=<MseLossBackward0>)\n",
      "5072 tensor(18769.3828, grad_fn=<MseLossBackward0>)\n",
      "5073 tensor(18768.7168, grad_fn=<MseLossBackward0>)\n",
      "5074 tensor(18768.0508, grad_fn=<MseLossBackward0>)\n",
      "5075 tensor(18767.3867, grad_fn=<MseLossBackward0>)\n",
      "5076 tensor(18766.7246, grad_fn=<MseLossBackward0>)\n",
      "5077 tensor(18766.0625, grad_fn=<MseLossBackward0>)\n",
      "5078 tensor(18765.4043, grad_fn=<MseLossBackward0>)\n",
      "5079 tensor(18764.7461, grad_fn=<MseLossBackward0>)\n",
      "5080 tensor(18764.0898, grad_fn=<MseLossBackward0>)\n",
      "5081 tensor(18763.4355, grad_fn=<MseLossBackward0>)\n",
      "5082 tensor(18762.7832, grad_fn=<MseLossBackward0>)\n",
      "5083 tensor(18762.1309, grad_fn=<MseLossBackward0>)\n",
      "5084 tensor(18761.4805, grad_fn=<MseLossBackward0>)\n",
      "5085 tensor(18760.8320, grad_fn=<MseLossBackward0>)\n",
      "5086 tensor(18760.1855, grad_fn=<MseLossBackward0>)\n",
      "5087 tensor(18759.5391, grad_fn=<MseLossBackward0>)\n",
      "5088 tensor(18758.8945, grad_fn=<MseLossBackward0>)\n",
      "5089 tensor(18758.2500, grad_fn=<MseLossBackward0>)\n",
      "5090 tensor(18757.6113, grad_fn=<MseLossBackward0>)\n",
      "5091 tensor(18756.9707, grad_fn=<MseLossBackward0>)\n",
      "5092 tensor(18756.3301, grad_fn=<MseLossBackward0>)\n",
      "5093 tensor(18755.6953, grad_fn=<MseLossBackward0>)\n",
      "5094 tensor(18755.0586, grad_fn=<MseLossBackward0>)\n",
      "5095 tensor(18754.4258, grad_fn=<MseLossBackward0>)\n",
      "5096 tensor(18753.7930, grad_fn=<MseLossBackward0>)\n",
      "5097 tensor(18753.1621, grad_fn=<MseLossBackward0>)\n",
      "5098 tensor(18752.5312, grad_fn=<MseLossBackward0>)\n",
      "5099 tensor(18751.9023, grad_fn=<MseLossBackward0>)\n",
      "5100 tensor(18751.2773, grad_fn=<MseLossBackward0>)\n",
      "5101 tensor(18750.6504, grad_fn=<MseLossBackward0>)\n",
      "5102 tensor(18750.0254, grad_fn=<MseLossBackward0>)\n",
      "5103 tensor(18749.4043, grad_fn=<MseLossBackward0>)\n",
      "5104 tensor(18748.7812, grad_fn=<MseLossBackward0>)\n",
      "5105 tensor(18748.1621, grad_fn=<MseLossBackward0>)\n",
      "5106 tensor(18747.5449, grad_fn=<MseLossBackward0>)\n",
      "5107 tensor(18746.9277, grad_fn=<MseLossBackward0>)\n",
      "5108 tensor(18746.3105, grad_fn=<MseLossBackward0>)\n",
      "5109 tensor(18745.6992, grad_fn=<MseLossBackward0>)\n",
      "5110 tensor(18745.0859, grad_fn=<MseLossBackward0>)\n",
      "5111 tensor(18744.4707, grad_fn=<MseLossBackward0>)\n",
      "5112 tensor(18743.8633, grad_fn=<MseLossBackward0>)\n",
      "5113 tensor(18743.2559, grad_fn=<MseLossBackward0>)\n",
      "5114 tensor(18742.6484, grad_fn=<MseLossBackward0>)\n",
      "5115 tensor(18742.0430, grad_fn=<MseLossBackward0>)\n",
      "5116 tensor(18741.4355, grad_fn=<MseLossBackward0>)\n",
      "5117 tensor(18740.8340, grad_fn=<MseLossBackward0>)\n",
      "5118 tensor(18740.2324, grad_fn=<MseLossBackward0>)\n",
      "5119 tensor(18739.6309, grad_fn=<MseLossBackward0>)\n",
      "5120 tensor(18739.0332, grad_fn=<MseLossBackward0>)\n",
      "5121 tensor(18738.4336, grad_fn=<MseLossBackward0>)\n",
      "5122 tensor(18737.8379, grad_fn=<MseLossBackward0>)\n",
      "5123 tensor(18737.2422, grad_fn=<MseLossBackward0>)\n",
      "5124 tensor(18736.6484, grad_fn=<MseLossBackward0>)\n",
      "5125 tensor(18736.0566, grad_fn=<MseLossBackward0>)\n",
      "5126 tensor(18735.4648, grad_fn=<MseLossBackward0>)\n",
      "5127 tensor(18734.8750, grad_fn=<MseLossBackward0>)\n",
      "5128 tensor(18734.2871, grad_fn=<MseLossBackward0>)\n",
      "5129 tensor(18733.7012, grad_fn=<MseLossBackward0>)\n",
      "5130 tensor(18733.1133, grad_fn=<MseLossBackward0>)\n",
      "5131 tensor(18732.5312, grad_fn=<MseLossBackward0>)\n",
      "5132 tensor(18731.9473, grad_fn=<MseLossBackward0>)\n",
      "5133 tensor(18731.3672, grad_fn=<MseLossBackward0>)\n",
      "5134 tensor(18730.7852, grad_fn=<MseLossBackward0>)\n",
      "5135 tensor(18730.2070, grad_fn=<MseLossBackward0>)\n",
      "5136 tensor(18729.6289, grad_fn=<MseLossBackward0>)\n",
      "5137 tensor(18729.0508, grad_fn=<MseLossBackward0>)\n",
      "5138 tensor(18728.4766, grad_fn=<MseLossBackward0>)\n",
      "5139 tensor(18727.9043, grad_fn=<MseLossBackward0>)\n",
      "5140 tensor(18727.3301, grad_fn=<MseLossBackward0>)\n",
      "5141 tensor(18726.7598, grad_fn=<MseLossBackward0>)\n",
      "5142 tensor(18726.1914, grad_fn=<MseLossBackward0>)\n",
      "5143 tensor(18725.6211, grad_fn=<MseLossBackward0>)\n",
      "5144 tensor(18725.0527, grad_fn=<MseLossBackward0>)\n",
      "5145 tensor(18724.4863, grad_fn=<MseLossBackward0>)\n",
      "5146 tensor(18723.9219, grad_fn=<MseLossBackward0>)\n",
      "5147 tensor(18723.3574, grad_fn=<MseLossBackward0>)\n",
      "5148 tensor(18722.7969, grad_fn=<MseLossBackward0>)\n",
      "5149 tensor(18722.2344, grad_fn=<MseLossBackward0>)\n",
      "5150 tensor(18721.6758, grad_fn=<MseLossBackward0>)\n",
      "5151 tensor(18721.1172, grad_fn=<MseLossBackward0>)\n",
      "5152 tensor(18720.5586, grad_fn=<MseLossBackward0>)\n",
      "5153 tensor(18720.0039, grad_fn=<MseLossBackward0>)\n",
      "5154 tensor(18719.4492, grad_fn=<MseLossBackward0>)\n",
      "5155 tensor(18718.8945, grad_fn=<MseLossBackward0>)\n",
      "5156 tensor(18718.3457, grad_fn=<MseLossBackward0>)\n",
      "5157 tensor(18717.7910, grad_fn=<MseLossBackward0>)\n",
      "5158 tensor(18717.2422, grad_fn=<MseLossBackward0>)\n",
      "5159 tensor(18716.6934, grad_fn=<MseLossBackward0>)\n",
      "5160 tensor(18716.1445, grad_fn=<MseLossBackward0>)\n",
      "5161 tensor(18715.5996, grad_fn=<MseLossBackward0>)\n",
      "5162 tensor(18715.0527, grad_fn=<MseLossBackward0>)\n",
      "5163 tensor(18714.5117, grad_fn=<MseLossBackward0>)\n",
      "5164 tensor(18713.9688, grad_fn=<MseLossBackward0>)\n",
      "5165 tensor(18713.4258, grad_fn=<MseLossBackward0>)\n",
      "5166 tensor(18712.8867, grad_fn=<MseLossBackward0>)\n",
      "5167 tensor(18712.3477, grad_fn=<MseLossBackward0>)\n",
      "5168 tensor(18711.8086, grad_fn=<MseLossBackward0>)\n",
      "5169 tensor(18711.2754, grad_fn=<MseLossBackward0>)\n",
      "5170 tensor(18710.7383, grad_fn=<MseLossBackward0>)\n",
      "5171 tensor(18710.2031, grad_fn=<MseLossBackward0>)\n",
      "5172 tensor(18709.6719, grad_fn=<MseLossBackward0>)\n",
      "5173 tensor(18709.1387, grad_fn=<MseLossBackward0>)\n",
      "5174 tensor(18708.6094, grad_fn=<MseLossBackward0>)\n",
      "5175 tensor(18708.0762, grad_fn=<MseLossBackward0>)\n",
      "5176 tensor(18707.5508, grad_fn=<MseLossBackward0>)\n",
      "5177 tensor(18707.0234, grad_fn=<MseLossBackward0>)\n",
      "5178 tensor(18706.5000, grad_fn=<MseLossBackward0>)\n",
      "5179 tensor(18705.9727, grad_fn=<MseLossBackward0>)\n",
      "5180 tensor(18705.4492, grad_fn=<MseLossBackward0>)\n",
      "5181 tensor(18704.9277, grad_fn=<MseLossBackward0>)\n",
      "5182 tensor(18704.4062, grad_fn=<MseLossBackward0>)\n",
      "5183 tensor(18703.8848, grad_fn=<MseLossBackward0>)\n",
      "5184 tensor(18703.3672, grad_fn=<MseLossBackward0>)\n",
      "5185 tensor(18702.8496, grad_fn=<MseLossBackward0>)\n",
      "5186 tensor(18702.3320, grad_fn=<MseLossBackward0>)\n",
      "5187 tensor(18701.8164, grad_fn=<MseLossBackward0>)\n",
      "5188 tensor(18701.3027, grad_fn=<MseLossBackward0>)\n",
      "5189 tensor(18700.7910, grad_fn=<MseLossBackward0>)\n",
      "5190 tensor(18700.2773, grad_fn=<MseLossBackward0>)\n",
      "5191 tensor(18699.7676, grad_fn=<MseLossBackward0>)\n",
      "5192 tensor(18699.2559, grad_fn=<MseLossBackward0>)\n",
      "5193 tensor(18698.7480, grad_fn=<MseLossBackward0>)\n",
      "5194 tensor(18698.2383, grad_fn=<MseLossBackward0>)\n",
      "5195 tensor(18697.7344, grad_fn=<MseLossBackward0>)\n",
      "5196 tensor(18697.2285, grad_fn=<MseLossBackward0>)\n",
      "5197 tensor(18696.7246, grad_fn=<MseLossBackward0>)\n",
      "5198 tensor(18696.2246, grad_fn=<MseLossBackward0>)\n",
      "5199 tensor(18695.7188, grad_fn=<MseLossBackward0>)\n",
      "5200 tensor(18695.2188, grad_fn=<MseLossBackward0>)\n",
      "5201 tensor(18694.7188, grad_fn=<MseLossBackward0>)\n",
      "5202 tensor(18694.2207, grad_fn=<MseLossBackward0>)\n",
      "5203 tensor(18693.7246, grad_fn=<MseLossBackward0>)\n",
      "5204 tensor(18693.2266, grad_fn=<MseLossBackward0>)\n",
      "5205 tensor(18692.7324, grad_fn=<MseLossBackward0>)\n",
      "5206 tensor(18692.2383, grad_fn=<MseLossBackward0>)\n",
      "5207 tensor(18691.7422, grad_fn=<MseLossBackward0>)\n",
      "5208 tensor(18691.2500, grad_fn=<MseLossBackward0>)\n",
      "5209 tensor(18690.7578, grad_fn=<MseLossBackward0>)\n",
      "5210 tensor(18690.2695, grad_fn=<MseLossBackward0>)\n",
      "5211 tensor(18689.7812, grad_fn=<MseLossBackward0>)\n",
      "5212 tensor(18689.2910, grad_fn=<MseLossBackward0>)\n",
      "5213 tensor(18688.8066, grad_fn=<MseLossBackward0>)\n",
      "5214 tensor(18688.3223, grad_fn=<MseLossBackward0>)\n",
      "5215 tensor(18687.8359, grad_fn=<MseLossBackward0>)\n",
      "5216 tensor(18687.3535, grad_fn=<MseLossBackward0>)\n",
      "5217 tensor(18686.8691, grad_fn=<MseLossBackward0>)\n",
      "5218 tensor(18686.3887, grad_fn=<MseLossBackward0>)\n",
      "5219 tensor(18685.9082, grad_fn=<MseLossBackward0>)\n",
      "5220 tensor(18685.4277, grad_fn=<MseLossBackward0>)\n",
      "5221 tensor(18684.9492, grad_fn=<MseLossBackward0>)\n",
      "5222 tensor(18684.4707, grad_fn=<MseLossBackward0>)\n",
      "5223 tensor(18683.9961, grad_fn=<MseLossBackward0>)\n",
      "5224 tensor(18683.5215, grad_fn=<MseLossBackward0>)\n",
      "5225 tensor(18683.0449, grad_fn=<MseLossBackward0>)\n",
      "5226 tensor(18682.5742, grad_fn=<MseLossBackward0>)\n",
      "5227 tensor(18682.1016, grad_fn=<MseLossBackward0>)\n",
      "5228 tensor(18681.6289, grad_fn=<MseLossBackward0>)\n",
      "5229 tensor(18681.1602, grad_fn=<MseLossBackward0>)\n",
      "5230 tensor(18680.6914, grad_fn=<MseLossBackward0>)\n",
      "5231 tensor(18680.2227, grad_fn=<MseLossBackward0>)\n",
      "5232 tensor(18679.7578, grad_fn=<MseLossBackward0>)\n",
      "5233 tensor(18679.2871, grad_fn=<MseLossBackward0>)\n",
      "5234 tensor(18678.8242, grad_fn=<MseLossBackward0>)\n",
      "5235 tensor(18678.3594, grad_fn=<MseLossBackward0>)\n",
      "5236 tensor(18677.8984, grad_fn=<MseLossBackward0>)\n",
      "5237 tensor(18677.4355, grad_fn=<MseLossBackward0>)\n",
      "5238 tensor(18676.9727, grad_fn=<MseLossBackward0>)\n",
      "5239 tensor(18676.5137, grad_fn=<MseLossBackward0>)\n",
      "5240 tensor(18676.0527, grad_fn=<MseLossBackward0>)\n",
      "5241 tensor(18675.5938, grad_fn=<MseLossBackward0>)\n",
      "5242 tensor(18675.1387, grad_fn=<MseLossBackward0>)\n",
      "5243 tensor(18674.6836, grad_fn=<MseLossBackward0>)\n",
      "5244 tensor(18674.2285, grad_fn=<MseLossBackward0>)\n",
      "5245 tensor(18673.7734, grad_fn=<MseLossBackward0>)\n",
      "5246 tensor(18673.3184, grad_fn=<MseLossBackward0>)\n",
      "5247 tensor(18672.8691, grad_fn=<MseLossBackward0>)\n",
      "5248 tensor(18672.4180, grad_fn=<MseLossBackward0>)\n",
      "5249 tensor(18671.9648, grad_fn=<MseLossBackward0>)\n",
      "5250 tensor(18671.5176, grad_fn=<MseLossBackward0>)\n",
      "5251 tensor(18671.0684, grad_fn=<MseLossBackward0>)\n",
      "5252 tensor(18670.6230, grad_fn=<MseLossBackward0>)\n",
      "5253 tensor(18670.1738, grad_fn=<MseLossBackward0>)\n",
      "5254 tensor(18669.7305, grad_fn=<MseLossBackward0>)\n",
      "5255 tensor(18669.2832, grad_fn=<MseLossBackward0>)\n",
      "5256 tensor(18668.8398, grad_fn=<MseLossBackward0>)\n",
      "5257 tensor(18668.3984, grad_fn=<MseLossBackward0>)\n",
      "5258 tensor(18667.9551, grad_fn=<MseLossBackward0>)\n",
      "5259 tensor(18667.5137, grad_fn=<MseLossBackward0>)\n",
      "5260 tensor(18667.0762, grad_fn=<MseLossBackward0>)\n",
      "5261 tensor(18666.6348, grad_fn=<MseLossBackward0>)\n",
      "5262 tensor(18666.1992, grad_fn=<MseLossBackward0>)\n",
      "5263 tensor(18665.7598, grad_fn=<MseLossBackward0>)\n",
      "5264 tensor(18665.3223, grad_fn=<MseLossBackward0>)\n",
      "5265 tensor(18664.8906, grad_fn=<MseLossBackward0>)\n",
      "5266 tensor(18664.4551, grad_fn=<MseLossBackward0>)\n",
      "5267 tensor(18664.0215, grad_fn=<MseLossBackward0>)\n",
      "5268 tensor(18663.5879, grad_fn=<MseLossBackward0>)\n",
      "5269 tensor(18663.1602, grad_fn=<MseLossBackward0>)\n",
      "5270 tensor(18662.7285, grad_fn=<MseLossBackward0>)\n",
      "5271 tensor(18662.2969, grad_fn=<MseLossBackward0>)\n",
      "5272 tensor(18661.8691, grad_fn=<MseLossBackward0>)\n",
      "5273 tensor(18661.4395, grad_fn=<MseLossBackward0>)\n",
      "5274 tensor(18661.0117, grad_fn=<MseLossBackward0>)\n",
      "5275 tensor(18660.5879, grad_fn=<MseLossBackward0>)\n",
      "5276 tensor(18660.1621, grad_fn=<MseLossBackward0>)\n",
      "5277 tensor(18659.7383, grad_fn=<MseLossBackward0>)\n",
      "5278 tensor(18659.3125, grad_fn=<MseLossBackward0>)\n",
      "5279 tensor(18658.8906, grad_fn=<MseLossBackward0>)\n",
      "5280 tensor(18658.4688, grad_fn=<MseLossBackward0>)\n",
      "5281 tensor(18658.0469, grad_fn=<MseLossBackward0>)\n",
      "5282 tensor(18657.6270, grad_fn=<MseLossBackward0>)\n",
      "5283 tensor(18657.2090, grad_fn=<MseLossBackward0>)\n",
      "5284 tensor(18656.7891, grad_fn=<MseLossBackward0>)\n",
      "5285 tensor(18656.3730, grad_fn=<MseLossBackward0>)\n",
      "5286 tensor(18655.9570, grad_fn=<MseLossBackward0>)\n",
      "5287 tensor(18655.5410, grad_fn=<MseLossBackward0>)\n",
      "5288 tensor(18655.1270, grad_fn=<MseLossBackward0>)\n",
      "5289 tensor(18654.7109, grad_fn=<MseLossBackward0>)\n",
      "5290 tensor(18654.2988, grad_fn=<MseLossBackward0>)\n",
      "5291 tensor(18653.8867, grad_fn=<MseLossBackward0>)\n",
      "5292 tensor(18653.4766, grad_fn=<MseLossBackward0>)\n",
      "5293 tensor(18653.0664, grad_fn=<MseLossBackward0>)\n",
      "5294 tensor(18652.6543, grad_fn=<MseLossBackward0>)\n",
      "5295 tensor(18652.2441, grad_fn=<MseLossBackward0>)\n",
      "5296 tensor(18651.8398, grad_fn=<MseLossBackward0>)\n",
      "5297 tensor(18651.4316, grad_fn=<MseLossBackward0>)\n",
      "5298 tensor(18651.0234, grad_fn=<MseLossBackward0>)\n",
      "5299 tensor(18650.6191, grad_fn=<MseLossBackward0>)\n",
      "5300 tensor(18650.2148, grad_fn=<MseLossBackward0>)\n",
      "5301 tensor(18649.8105, grad_fn=<MseLossBackward0>)\n",
      "5302 tensor(18649.4082, grad_fn=<MseLossBackward0>)\n",
      "5303 tensor(18649.0039, grad_fn=<MseLossBackward0>)\n",
      "5304 tensor(18648.6035, grad_fn=<MseLossBackward0>)\n",
      "5305 tensor(18648.2051, grad_fn=<MseLossBackward0>)\n",
      "5306 tensor(18647.8027, grad_fn=<MseLossBackward0>)\n",
      "5307 tensor(18647.4062, grad_fn=<MseLossBackward0>)\n",
      "5308 tensor(18647.0078, grad_fn=<MseLossBackward0>)\n",
      "5309 tensor(18646.6094, grad_fn=<MseLossBackward0>)\n",
      "5310 tensor(18646.2129, grad_fn=<MseLossBackward0>)\n",
      "5311 tensor(18645.8184, grad_fn=<MseLossBackward0>)\n",
      "5312 tensor(18645.4219, grad_fn=<MseLossBackward0>)\n",
      "5313 tensor(18645.0273, grad_fn=<MseLossBackward0>)\n",
      "5314 tensor(18644.6348, grad_fn=<MseLossBackward0>)\n",
      "5315 tensor(18644.2422, grad_fn=<MseLossBackward0>)\n",
      "5316 tensor(18643.8496, grad_fn=<MseLossBackward0>)\n",
      "5317 tensor(18643.4590, grad_fn=<MseLossBackward0>)\n",
      "5318 tensor(18643.0703, grad_fn=<MseLossBackward0>)\n",
      "5319 tensor(18642.6797, grad_fn=<MseLossBackward0>)\n",
      "5320 tensor(18642.2910, grad_fn=<MseLossBackward0>)\n",
      "5321 tensor(18641.9043, grad_fn=<MseLossBackward0>)\n",
      "5322 tensor(18641.5195, grad_fn=<MseLossBackward0>)\n",
      "5323 tensor(18641.1309, grad_fn=<MseLossBackward0>)\n",
      "5324 tensor(18640.7461, grad_fn=<MseLossBackward0>)\n",
      "5325 tensor(18640.3594, grad_fn=<MseLossBackward0>)\n",
      "5326 tensor(18639.9766, grad_fn=<MseLossBackward0>)\n",
      "5327 tensor(18639.5938, grad_fn=<MseLossBackward0>)\n",
      "5328 tensor(18639.2109, grad_fn=<MseLossBackward0>)\n",
      "5329 tensor(18638.8320, grad_fn=<MseLossBackward0>)\n",
      "5330 tensor(18638.4512, grad_fn=<MseLossBackward0>)\n",
      "5331 tensor(18638.0684, grad_fn=<MseLossBackward0>)\n",
      "5332 tensor(18637.6895, grad_fn=<MseLossBackward0>)\n",
      "5333 tensor(18637.3125, grad_fn=<MseLossBackward0>)\n",
      "5334 tensor(18636.9336, grad_fn=<MseLossBackward0>)\n",
      "5335 tensor(18636.5566, grad_fn=<MseLossBackward0>)\n",
      "5336 tensor(18636.1816, grad_fn=<MseLossBackward0>)\n",
      "5337 tensor(18635.8066, grad_fn=<MseLossBackward0>)\n",
      "5338 tensor(18635.4316, grad_fn=<MseLossBackward0>)\n",
      "5339 tensor(18635.0566, grad_fn=<MseLossBackward0>)\n",
      "5340 tensor(18634.6816, grad_fn=<MseLossBackward0>)\n",
      "5341 tensor(18634.3105, grad_fn=<MseLossBackward0>)\n",
      "5342 tensor(18633.9395, grad_fn=<MseLossBackward0>)\n",
      "5343 tensor(18633.5684, grad_fn=<MseLossBackward0>)\n",
      "5344 tensor(18633.1992, grad_fn=<MseLossBackward0>)\n",
      "5345 tensor(18632.8281, grad_fn=<MseLossBackward0>)\n",
      "5346 tensor(18632.4590, grad_fn=<MseLossBackward0>)\n",
      "5347 tensor(18632.0918, grad_fn=<MseLossBackward0>)\n",
      "5348 tensor(18631.7207, grad_fn=<MseLossBackward0>)\n",
      "5349 tensor(18631.3574, grad_fn=<MseLossBackward0>)\n",
      "5350 tensor(18630.9883, grad_fn=<MseLossBackward0>)\n",
      "5351 tensor(18630.6250, grad_fn=<MseLossBackward0>)\n",
      "5352 tensor(18630.2617, grad_fn=<MseLossBackward0>)\n",
      "5353 tensor(18629.8984, grad_fn=<MseLossBackward0>)\n",
      "5354 tensor(18629.5332, grad_fn=<MseLossBackward0>)\n",
      "5355 tensor(18629.1699, grad_fn=<MseLossBackward0>)\n",
      "5356 tensor(18628.8086, grad_fn=<MseLossBackward0>)\n",
      "5357 tensor(18628.4473, grad_fn=<MseLossBackward0>)\n",
      "5358 tensor(18628.0879, grad_fn=<MseLossBackward0>)\n",
      "5359 tensor(18627.7285, grad_fn=<MseLossBackward0>)\n",
      "5360 tensor(18627.3691, grad_fn=<MseLossBackward0>)\n",
      "5361 tensor(18627.0137, grad_fn=<MseLossBackward0>)\n",
      "5362 tensor(18626.6543, grad_fn=<MseLossBackward0>)\n",
      "5363 tensor(18626.2949, grad_fn=<MseLossBackward0>)\n",
      "5364 tensor(18625.9395, grad_fn=<MseLossBackward0>)\n",
      "5365 tensor(18625.5840, grad_fn=<MseLossBackward0>)\n",
      "5366 tensor(18625.2305, grad_fn=<MseLossBackward0>)\n",
      "5367 tensor(18624.8750, grad_fn=<MseLossBackward0>)\n",
      "5368 tensor(18624.5215, grad_fn=<MseLossBackward0>)\n",
      "5369 tensor(18624.1680, grad_fn=<MseLossBackward0>)\n",
      "5370 tensor(18623.8184, grad_fn=<MseLossBackward0>)\n",
      "5371 tensor(18623.4648, grad_fn=<MseLossBackward0>)\n",
      "5372 tensor(18623.1152, grad_fn=<MseLossBackward0>)\n",
      "5373 tensor(18622.7656, grad_fn=<MseLossBackward0>)\n",
      "5374 tensor(18622.4160, grad_fn=<MseLossBackward0>)\n",
      "5375 tensor(18622.0645, grad_fn=<MseLossBackward0>)\n",
      "5376 tensor(18621.7188, grad_fn=<MseLossBackward0>)\n",
      "5377 tensor(18621.3711, grad_fn=<MseLossBackward0>)\n",
      "5378 tensor(18621.0234, grad_fn=<MseLossBackward0>)\n",
      "5379 tensor(18620.6797, grad_fn=<MseLossBackward0>)\n",
      "5380 tensor(18620.3340, grad_fn=<MseLossBackward0>)\n",
      "5381 tensor(18619.9902, grad_fn=<MseLossBackward0>)\n",
      "5382 tensor(18619.6445, grad_fn=<MseLossBackward0>)\n",
      "5383 tensor(18619.3008, grad_fn=<MseLossBackward0>)\n",
      "5384 tensor(18618.9590, grad_fn=<MseLossBackward0>)\n",
      "5385 tensor(18618.6152, grad_fn=<MseLossBackward0>)\n",
      "5386 tensor(18618.2734, grad_fn=<MseLossBackward0>)\n",
      "5387 tensor(18617.9336, grad_fn=<MseLossBackward0>)\n",
      "5388 tensor(18617.5918, grad_fn=<MseLossBackward0>)\n",
      "5389 tensor(18617.2539, grad_fn=<MseLossBackward0>)\n",
      "5390 tensor(18616.9141, grad_fn=<MseLossBackward0>)\n",
      "5391 tensor(18616.5742, grad_fn=<MseLossBackward0>)\n",
      "5392 tensor(18616.2383, grad_fn=<MseLossBackward0>)\n",
      "5393 tensor(18615.9004, grad_fn=<MseLossBackward0>)\n",
      "5394 tensor(18615.5645, grad_fn=<MseLossBackward0>)\n",
      "5395 tensor(18615.2285, grad_fn=<MseLossBackward0>)\n",
      "5396 tensor(18614.8926, grad_fn=<MseLossBackward0>)\n",
      "5397 tensor(18614.5566, grad_fn=<MseLossBackward0>)\n",
      "5398 tensor(18614.2227, grad_fn=<MseLossBackward0>)\n",
      "5399 tensor(18613.8906, grad_fn=<MseLossBackward0>)\n",
      "5400 tensor(18613.5566, grad_fn=<MseLossBackward0>)\n",
      "5401 tensor(18613.2246, grad_fn=<MseLossBackward0>)\n",
      "5402 tensor(18612.8926, grad_fn=<MseLossBackward0>)\n",
      "5403 tensor(18612.5605, grad_fn=<MseLossBackward0>)\n",
      "5404 tensor(18612.2324, grad_fn=<MseLossBackward0>)\n",
      "5405 tensor(18611.9023, grad_fn=<MseLossBackward0>)\n",
      "5406 tensor(18611.5723, grad_fn=<MseLossBackward0>)\n",
      "5407 tensor(18611.2461, grad_fn=<MseLossBackward0>)\n",
      "5408 tensor(18610.9180, grad_fn=<MseLossBackward0>)\n",
      "5409 tensor(18610.5898, grad_fn=<MseLossBackward0>)\n",
      "5410 tensor(18610.2617, grad_fn=<MseLossBackward0>)\n",
      "5411 tensor(18609.9355, grad_fn=<MseLossBackward0>)\n",
      "5412 tensor(18609.6094, grad_fn=<MseLossBackward0>)\n",
      "5413 tensor(18609.2852, grad_fn=<MseLossBackward0>)\n",
      "5414 tensor(18608.9609, grad_fn=<MseLossBackward0>)\n",
      "5415 tensor(18608.6367, grad_fn=<MseLossBackward0>)\n",
      "5416 tensor(18608.3125, grad_fn=<MseLossBackward0>)\n",
      "5417 tensor(18607.9922, grad_fn=<MseLossBackward0>)\n",
      "5418 tensor(18607.6699, grad_fn=<MseLossBackward0>)\n",
      "5419 tensor(18607.3477, grad_fn=<MseLossBackward0>)\n",
      "5420 tensor(18607.0254, grad_fn=<MseLossBackward0>)\n",
      "5421 tensor(18606.7051, grad_fn=<MseLossBackward0>)\n",
      "5422 tensor(18606.3867, grad_fn=<MseLossBackward0>)\n",
      "5423 tensor(18606.0664, grad_fn=<MseLossBackward0>)\n",
      "5424 tensor(18605.7480, grad_fn=<MseLossBackward0>)\n",
      "5425 tensor(18605.4297, grad_fn=<MseLossBackward0>)\n",
      "5426 tensor(18605.1113, grad_fn=<MseLossBackward0>)\n",
      "5427 tensor(18604.7949, grad_fn=<MseLossBackward0>)\n",
      "5428 tensor(18604.4785, grad_fn=<MseLossBackward0>)\n",
      "5429 tensor(18604.1641, grad_fn=<MseLossBackward0>)\n",
      "5430 tensor(18603.8477, grad_fn=<MseLossBackward0>)\n",
      "5431 tensor(18603.5332, grad_fn=<MseLossBackward0>)\n",
      "5432 tensor(18603.2188, grad_fn=<MseLossBackward0>)\n",
      "5433 tensor(18602.9043, grad_fn=<MseLossBackward0>)\n",
      "5434 tensor(18602.5938, grad_fn=<MseLossBackward0>)\n",
      "5435 tensor(18602.2793, grad_fn=<MseLossBackward0>)\n",
      "5436 tensor(18601.9668, grad_fn=<MseLossBackward0>)\n",
      "5437 tensor(18601.6562, grad_fn=<MseLossBackward0>)\n",
      "5438 tensor(18601.3457, grad_fn=<MseLossBackward0>)\n",
      "5439 tensor(18601.0352, grad_fn=<MseLossBackward0>)\n",
      "5440 tensor(18600.7246, grad_fn=<MseLossBackward0>)\n",
      "5441 tensor(18600.4141, grad_fn=<MseLossBackward0>)\n",
      "5442 tensor(18600.1074, grad_fn=<MseLossBackward0>)\n",
      "5443 tensor(18599.7988, grad_fn=<MseLossBackward0>)\n",
      "5444 tensor(18599.4902, grad_fn=<MseLossBackward0>)\n",
      "5445 tensor(18599.1816, grad_fn=<MseLossBackward0>)\n",
      "5446 tensor(18598.8770, grad_fn=<MseLossBackward0>)\n",
      "5447 tensor(18598.5703, grad_fn=<MseLossBackward0>)\n",
      "5448 tensor(18598.2656, grad_fn=<MseLossBackward0>)\n",
      "5449 tensor(18597.9590, grad_fn=<MseLossBackward0>)\n",
      "5450 tensor(18597.6562, grad_fn=<MseLossBackward0>)\n",
      "5451 tensor(18597.3535, grad_fn=<MseLossBackward0>)\n",
      "5452 tensor(18597.0469, grad_fn=<MseLossBackward0>)\n",
      "5453 tensor(18596.7441, grad_fn=<MseLossBackward0>)\n",
      "5454 tensor(18596.4434, grad_fn=<MseLossBackward0>)\n",
      "5455 tensor(18596.1406, grad_fn=<MseLossBackward0>)\n",
      "5456 tensor(18595.8379, grad_fn=<MseLossBackward0>)\n",
      "5457 tensor(18595.5391, grad_fn=<MseLossBackward0>)\n",
      "5458 tensor(18595.2383, grad_fn=<MseLossBackward0>)\n",
      "5459 tensor(18594.9375, grad_fn=<MseLossBackward0>)\n",
      "5460 tensor(18594.6406, grad_fn=<MseLossBackward0>)\n",
      "5461 tensor(18594.3379, grad_fn=<MseLossBackward0>)\n",
      "5462 tensor(18594.0430, grad_fn=<MseLossBackward0>)\n",
      "5463 tensor(18593.7422, grad_fn=<MseLossBackward0>)\n",
      "5464 tensor(18593.4453, grad_fn=<MseLossBackward0>)\n",
      "5465 tensor(18593.1484, grad_fn=<MseLossBackward0>)\n",
      "5466 tensor(18592.8516, grad_fn=<MseLossBackward0>)\n",
      "5467 tensor(18592.5566, grad_fn=<MseLossBackward0>)\n",
      "5468 tensor(18592.2617, grad_fn=<MseLossBackward0>)\n",
      "5469 tensor(18591.9648, grad_fn=<MseLossBackward0>)\n",
      "5470 tensor(18591.6719, grad_fn=<MseLossBackward0>)\n",
      "5471 tensor(18591.3789, grad_fn=<MseLossBackward0>)\n",
      "5472 tensor(18591.0820, grad_fn=<MseLossBackward0>)\n",
      "5473 tensor(18590.7910, grad_fn=<MseLossBackward0>)\n",
      "5474 tensor(18590.4980, grad_fn=<MseLossBackward0>)\n",
      "5475 tensor(18590.2051, grad_fn=<MseLossBackward0>)\n",
      "5476 tensor(18589.9141, grad_fn=<MseLossBackward0>)\n",
      "5477 tensor(18589.6250, grad_fn=<MseLossBackward0>)\n",
      "5478 tensor(18589.3320, grad_fn=<MseLossBackward0>)\n",
      "5479 tensor(18589.0430, grad_fn=<MseLossBackward0>)\n",
      "5480 tensor(18588.7539, grad_fn=<MseLossBackward0>)\n",
      "5481 tensor(18588.4648, grad_fn=<MseLossBackward0>)\n",
      "5482 tensor(18588.1758, grad_fn=<MseLossBackward0>)\n",
      "5483 tensor(18587.8848, grad_fn=<MseLossBackward0>)\n",
      "5484 tensor(18587.5977, grad_fn=<MseLossBackward0>)\n",
      "5485 tensor(18587.3105, grad_fn=<MseLossBackward0>)\n",
      "5486 tensor(18587.0254, grad_fn=<MseLossBackward0>)\n",
      "5487 tensor(18586.7402, grad_fn=<MseLossBackward0>)\n",
      "5488 tensor(18586.4512, grad_fn=<MseLossBackward0>)\n",
      "5489 tensor(18586.1660, grad_fn=<MseLossBackward0>)\n",
      "5490 tensor(18585.8809, grad_fn=<MseLossBackward0>)\n",
      "5491 tensor(18585.5977, grad_fn=<MseLossBackward0>)\n",
      "5492 tensor(18585.3125, grad_fn=<MseLossBackward0>)\n",
      "5493 tensor(18585.0293, grad_fn=<MseLossBackward0>)\n",
      "5494 tensor(18584.7461, grad_fn=<MseLossBackward0>)\n",
      "5495 tensor(18584.4648, grad_fn=<MseLossBackward0>)\n",
      "5496 tensor(18584.1816, grad_fn=<MseLossBackward0>)\n",
      "5497 tensor(18583.9004, grad_fn=<MseLossBackward0>)\n",
      "5498 tensor(18583.6191, grad_fn=<MseLossBackward0>)\n",
      "5499 tensor(18583.3379, grad_fn=<MseLossBackward0>)\n",
      "5500 tensor(18583.0566, grad_fn=<MseLossBackward0>)\n",
      "5501 tensor(18582.7754, grad_fn=<MseLossBackward0>)\n",
      "5502 tensor(18582.4980, grad_fn=<MseLossBackward0>)\n",
      "5503 tensor(18582.2188, grad_fn=<MseLossBackward0>)\n",
      "5504 tensor(18581.9375, grad_fn=<MseLossBackward0>)\n",
      "5505 tensor(18581.6582, grad_fn=<MseLossBackward0>)\n",
      "5506 tensor(18581.3809, grad_fn=<MseLossBackward0>)\n",
      "5507 tensor(18581.1055, grad_fn=<MseLossBackward0>)\n",
      "5508 tensor(18580.8281, grad_fn=<MseLossBackward0>)\n",
      "5509 tensor(18580.5508, grad_fn=<MseLossBackward0>)\n",
      "5510 tensor(18580.2734, grad_fn=<MseLossBackward0>)\n",
      "5511 tensor(18579.9980, grad_fn=<MseLossBackward0>)\n",
      "5512 tensor(18579.7227, grad_fn=<MseLossBackward0>)\n",
      "5513 tensor(18579.4492, grad_fn=<MseLossBackward0>)\n",
      "5514 tensor(18579.1738, grad_fn=<MseLossBackward0>)\n",
      "5515 tensor(18578.9023, grad_fn=<MseLossBackward0>)\n",
      "5516 tensor(18578.6250, grad_fn=<MseLossBackward0>)\n",
      "5517 tensor(18578.3535, grad_fn=<MseLossBackward0>)\n",
      "5518 tensor(18578.0801, grad_fn=<MseLossBackward0>)\n",
      "5519 tensor(18577.8086, grad_fn=<MseLossBackward0>)\n",
      "5520 tensor(18577.5352, grad_fn=<MseLossBackward0>)\n",
      "5521 tensor(18577.2656, grad_fn=<MseLossBackward0>)\n",
      "5522 tensor(18576.9902, grad_fn=<MseLossBackward0>)\n",
      "5523 tensor(18576.7207, grad_fn=<MseLossBackward0>)\n",
      "5524 tensor(18576.4512, grad_fn=<MseLossBackward0>)\n",
      "5525 tensor(18576.1816, grad_fn=<MseLossBackward0>)\n",
      "5526 tensor(18575.9121, grad_fn=<MseLossBackward0>)\n",
      "5527 tensor(18575.6406, grad_fn=<MseLossBackward0>)\n",
      "5528 tensor(18575.3750, grad_fn=<MseLossBackward0>)\n",
      "5529 tensor(18575.1035, grad_fn=<MseLossBackward0>)\n",
      "5530 tensor(18574.8340, grad_fn=<MseLossBackward0>)\n",
      "5531 tensor(18574.5684, grad_fn=<MseLossBackward0>)\n",
      "5532 tensor(18574.3027, grad_fn=<MseLossBackward0>)\n",
      "5533 tensor(18574.0352, grad_fn=<MseLossBackward0>)\n",
      "5534 tensor(18573.7695, grad_fn=<MseLossBackward0>)\n",
      "5535 tensor(18573.5039, grad_fn=<MseLossBackward0>)\n",
      "5536 tensor(18573.2383, grad_fn=<MseLossBackward0>)\n",
      "5537 tensor(18572.9727, grad_fn=<MseLossBackward0>)\n",
      "5538 tensor(18572.7070, grad_fn=<MseLossBackward0>)\n",
      "5539 tensor(18572.4414, grad_fn=<MseLossBackward0>)\n",
      "5540 tensor(18572.1777, grad_fn=<MseLossBackward0>)\n",
      "5541 tensor(18571.9121, grad_fn=<MseLossBackward0>)\n",
      "5542 tensor(18571.6504, grad_fn=<MseLossBackward0>)\n",
      "5543 tensor(18571.3848, grad_fn=<MseLossBackward0>)\n",
      "5544 tensor(18571.1250, grad_fn=<MseLossBackward0>)\n",
      "5545 tensor(18570.8613, grad_fn=<MseLossBackward0>)\n",
      "5546 tensor(18570.6016, grad_fn=<MseLossBackward0>)\n",
      "5547 tensor(18570.3398, grad_fn=<MseLossBackward0>)\n",
      "5548 tensor(18570.0781, grad_fn=<MseLossBackward0>)\n",
      "5549 tensor(18569.8164, grad_fn=<MseLossBackward0>)\n",
      "5550 tensor(18569.5566, grad_fn=<MseLossBackward0>)\n",
      "5551 tensor(18569.2969, grad_fn=<MseLossBackward0>)\n",
      "5552 tensor(18569.0371, grad_fn=<MseLossBackward0>)\n",
      "5553 tensor(18568.7773, grad_fn=<MseLossBackward0>)\n",
      "5554 tensor(18568.5195, grad_fn=<MseLossBackward0>)\n",
      "5555 tensor(18568.2617, grad_fn=<MseLossBackward0>)\n",
      "5556 tensor(18568.0020, grad_fn=<MseLossBackward0>)\n",
      "5557 tensor(18567.7461, grad_fn=<MseLossBackward0>)\n",
      "5558 tensor(18567.4863, grad_fn=<MseLossBackward0>)\n",
      "5559 tensor(18567.2305, grad_fn=<MseLossBackward0>)\n",
      "5560 tensor(18566.9727, grad_fn=<MseLossBackward0>)\n",
      "5561 tensor(18566.7168, grad_fn=<MseLossBackward0>)\n",
      "5562 tensor(18566.4590, grad_fn=<MseLossBackward0>)\n",
      "5563 tensor(18566.2051, grad_fn=<MseLossBackward0>)\n",
      "5564 tensor(18565.9492, grad_fn=<MseLossBackward0>)\n",
      "5565 tensor(18565.6953, grad_fn=<MseLossBackward0>)\n",
      "5566 tensor(18565.4395, grad_fn=<MseLossBackward0>)\n",
      "5567 tensor(18565.1855, grad_fn=<MseLossBackward0>)\n",
      "5568 tensor(18564.9316, grad_fn=<MseLossBackward0>)\n",
      "5569 tensor(18564.6777, grad_fn=<MseLossBackward0>)\n",
      "5570 tensor(18564.4238, grad_fn=<MseLossBackward0>)\n",
      "5571 tensor(18564.1699, grad_fn=<MseLossBackward0>)\n",
      "5572 tensor(18563.9199, grad_fn=<MseLossBackward0>)\n",
      "5573 tensor(18563.6660, grad_fn=<MseLossBackward0>)\n",
      "5574 tensor(18563.4141, grad_fn=<MseLossBackward0>)\n",
      "5575 tensor(18563.1641, grad_fn=<MseLossBackward0>)\n",
      "5576 tensor(18562.9121, grad_fn=<MseLossBackward0>)\n",
      "5577 tensor(18562.6621, grad_fn=<MseLossBackward0>)\n",
      "5578 tensor(18562.4102, grad_fn=<MseLossBackward0>)\n",
      "5579 tensor(18562.1582, grad_fn=<MseLossBackward0>)\n",
      "5580 tensor(18561.9102, grad_fn=<MseLossBackward0>)\n",
      "5581 tensor(18561.6602, grad_fn=<MseLossBackward0>)\n",
      "5582 tensor(18561.4121, grad_fn=<MseLossBackward0>)\n",
      "5583 tensor(18561.1621, grad_fn=<MseLossBackward0>)\n",
      "5584 tensor(18560.9121, grad_fn=<MseLossBackward0>)\n",
      "5585 tensor(18560.6660, grad_fn=<MseLossBackward0>)\n",
      "5586 tensor(18560.4180, grad_fn=<MseLossBackward0>)\n",
      "5587 tensor(18560.1699, grad_fn=<MseLossBackward0>)\n",
      "5588 tensor(18559.9219, grad_fn=<MseLossBackward0>)\n",
      "5589 tensor(18559.6758, grad_fn=<MseLossBackward0>)\n",
      "5590 tensor(18559.4277, grad_fn=<MseLossBackward0>)\n",
      "5591 tensor(18559.1816, grad_fn=<MseLossBackward0>)\n",
      "5592 tensor(18558.9355, grad_fn=<MseLossBackward0>)\n",
      "5593 tensor(18558.6895, grad_fn=<MseLossBackward0>)\n",
      "5594 tensor(18558.4434, grad_fn=<MseLossBackward0>)\n",
      "5595 tensor(18558.1992, grad_fn=<MseLossBackward0>)\n",
      "5596 tensor(18557.9551, grad_fn=<MseLossBackward0>)\n",
      "5597 tensor(18557.7109, grad_fn=<MseLossBackward0>)\n",
      "5598 tensor(18557.4648, grad_fn=<MseLossBackward0>)\n",
      "5599 tensor(18557.2246, grad_fn=<MseLossBackward0>)\n",
      "5600 tensor(18556.9785, grad_fn=<MseLossBackward0>)\n",
      "5601 tensor(18556.7363, grad_fn=<MseLossBackward0>)\n",
      "5602 tensor(18556.4922, grad_fn=<MseLossBackward0>)\n",
      "5603 tensor(18556.2500, grad_fn=<MseLossBackward0>)\n",
      "5604 tensor(18556.0078, grad_fn=<MseLossBackward0>)\n",
      "5605 tensor(18555.7637, grad_fn=<MseLossBackward0>)\n",
      "5606 tensor(18555.5234, grad_fn=<MseLossBackward0>)\n",
      "5607 tensor(18555.2832, grad_fn=<MseLossBackward0>)\n",
      "5608 tensor(18555.0410, grad_fn=<MseLossBackward0>)\n",
      "5609 tensor(18554.7988, grad_fn=<MseLossBackward0>)\n",
      "5610 tensor(18554.5605, grad_fn=<MseLossBackward0>)\n",
      "5611 tensor(18554.3184, grad_fn=<MseLossBackward0>)\n",
      "5612 tensor(18554.0801, grad_fn=<MseLossBackward0>)\n",
      "5613 tensor(18553.8398, grad_fn=<MseLossBackward0>)\n",
      "5614 tensor(18553.5996, grad_fn=<MseLossBackward0>)\n",
      "5615 tensor(18553.3613, grad_fn=<MseLossBackward0>)\n",
      "5616 tensor(18553.1230, grad_fn=<MseLossBackward0>)\n",
      "5617 tensor(18552.8828, grad_fn=<MseLossBackward0>)\n",
      "5618 tensor(18552.6445, grad_fn=<MseLossBackward0>)\n",
      "5619 tensor(18552.4082, grad_fn=<MseLossBackward0>)\n",
      "5620 tensor(18552.1699, grad_fn=<MseLossBackward0>)\n",
      "5621 tensor(18551.9316, grad_fn=<MseLossBackward0>)\n",
      "5622 tensor(18551.6973, grad_fn=<MseLossBackward0>)\n",
      "5623 tensor(18551.4570, grad_fn=<MseLossBackward0>)\n",
      "5624 tensor(18551.2227, grad_fn=<MseLossBackward0>)\n",
      "5625 tensor(18550.9863, grad_fn=<MseLossBackward0>)\n",
      "5626 tensor(18550.7500, grad_fn=<MseLossBackward0>)\n",
      "5627 tensor(18550.5156, grad_fn=<MseLossBackward0>)\n",
      "5628 tensor(18550.2793, grad_fn=<MseLossBackward0>)\n",
      "5629 tensor(18550.0449, grad_fn=<MseLossBackward0>)\n",
      "5630 tensor(18549.8066, grad_fn=<MseLossBackward0>)\n",
      "5631 tensor(18549.5762, grad_fn=<MseLossBackward0>)\n",
      "5632 tensor(18549.3398, grad_fn=<MseLossBackward0>)\n",
      "5633 tensor(18549.1074, grad_fn=<MseLossBackward0>)\n",
      "5634 tensor(18548.8730, grad_fn=<MseLossBackward0>)\n",
      "5635 tensor(18548.6387, grad_fn=<MseLossBackward0>)\n",
      "5636 tensor(18548.4062, grad_fn=<MseLossBackward0>)\n",
      "5637 tensor(18548.1719, grad_fn=<MseLossBackward0>)\n",
      "5638 tensor(18547.9414, grad_fn=<MseLossBackward0>)\n",
      "5639 tensor(18547.7070, grad_fn=<MseLossBackward0>)\n",
      "5640 tensor(18547.4746, grad_fn=<MseLossBackward0>)\n",
      "5641 tensor(18547.2422, grad_fn=<MseLossBackward0>)\n",
      "5642 tensor(18547.0117, grad_fn=<MseLossBackward0>)\n",
      "5643 tensor(18546.7793, grad_fn=<MseLossBackward0>)\n",
      "5644 tensor(18546.5488, grad_fn=<MseLossBackward0>)\n",
      "5645 tensor(18546.3145, grad_fn=<MseLossBackward0>)\n",
      "5646 tensor(18546.0879, grad_fn=<MseLossBackward0>)\n",
      "5647 tensor(18545.8574, grad_fn=<MseLossBackward0>)\n",
      "5648 tensor(18545.6270, grad_fn=<MseLossBackward0>)\n",
      "5649 tensor(18545.3965, grad_fn=<MseLossBackward0>)\n",
      "5650 tensor(18545.1660, grad_fn=<MseLossBackward0>)\n",
      "5651 tensor(18544.9375, grad_fn=<MseLossBackward0>)\n",
      "5652 tensor(18544.7070, grad_fn=<MseLossBackward0>)\n",
      "5653 tensor(18544.4805, grad_fn=<MseLossBackward0>)\n",
      "5654 tensor(18544.2500, grad_fn=<MseLossBackward0>)\n",
      "5655 tensor(18544.0215, grad_fn=<MseLossBackward0>)\n",
      "5656 tensor(18543.7949, grad_fn=<MseLossBackward0>)\n",
      "5657 tensor(18543.5684, grad_fn=<MseLossBackward0>)\n",
      "5658 tensor(18543.3398, grad_fn=<MseLossBackward0>)\n",
      "5659 tensor(18543.1113, grad_fn=<MseLossBackward0>)\n",
      "5660 tensor(18542.8828, grad_fn=<MseLossBackward0>)\n",
      "5661 tensor(18542.6543, grad_fn=<MseLossBackward0>)\n",
      "5662 tensor(18542.4277, grad_fn=<MseLossBackward0>)\n",
      "5663 tensor(18542.2012, grad_fn=<MseLossBackward0>)\n",
      "5664 tensor(18541.9766, grad_fn=<MseLossBackward0>)\n",
      "5665 tensor(18541.7520, grad_fn=<MseLossBackward0>)\n",
      "5666 tensor(18541.5273, grad_fn=<MseLossBackward0>)\n",
      "5667 tensor(18541.3008, grad_fn=<MseLossBackward0>)\n",
      "5668 tensor(18541.0742, grad_fn=<MseLossBackward0>)\n",
      "5669 tensor(18540.8516, grad_fn=<MseLossBackward0>)\n",
      "5670 tensor(18540.6250, grad_fn=<MseLossBackward0>)\n",
      "5671 tensor(18540.4004, grad_fn=<MseLossBackward0>)\n",
      "5672 tensor(18540.1758, grad_fn=<MseLossBackward0>)\n",
      "5673 tensor(18539.9512, grad_fn=<MseLossBackward0>)\n",
      "5674 tensor(18539.7285, grad_fn=<MseLossBackward0>)\n",
      "5675 tensor(18539.5039, grad_fn=<MseLossBackward0>)\n",
      "5676 tensor(18539.2812, grad_fn=<MseLossBackward0>)\n",
      "5677 tensor(18539.0586, grad_fn=<MseLossBackward0>)\n",
      "5678 tensor(18538.8340, grad_fn=<MseLossBackward0>)\n",
      "5679 tensor(18538.6133, grad_fn=<MseLossBackward0>)\n",
      "5680 tensor(18538.3906, grad_fn=<MseLossBackward0>)\n",
      "5681 tensor(18538.1660, grad_fn=<MseLossBackward0>)\n",
      "5682 tensor(18537.9453, grad_fn=<MseLossBackward0>)\n",
      "5683 tensor(18537.7227, grad_fn=<MseLossBackward0>)\n",
      "5684 tensor(18537.5020, grad_fn=<MseLossBackward0>)\n",
      "5685 tensor(18537.2812, grad_fn=<MseLossBackward0>)\n",
      "5686 tensor(18537.0605, grad_fn=<MseLossBackward0>)\n",
      "5687 tensor(18536.8398, grad_fn=<MseLossBackward0>)\n",
      "5688 tensor(18536.6172, grad_fn=<MseLossBackward0>)\n",
      "5689 tensor(18536.3965, grad_fn=<MseLossBackward0>)\n",
      "5690 tensor(18536.1777, grad_fn=<MseLossBackward0>)\n",
      "5691 tensor(18535.9570, grad_fn=<MseLossBackward0>)\n",
      "5692 tensor(18535.7383, grad_fn=<MseLossBackward0>)\n",
      "5693 tensor(18535.5176, grad_fn=<MseLossBackward0>)\n",
      "5694 tensor(18535.2969, grad_fn=<MseLossBackward0>)\n",
      "5695 tensor(18535.0781, grad_fn=<MseLossBackward0>)\n",
      "5696 tensor(18534.8613, grad_fn=<MseLossBackward0>)\n",
      "5697 tensor(18534.6426, grad_fn=<MseLossBackward0>)\n",
      "5698 tensor(18534.4238, grad_fn=<MseLossBackward0>)\n",
      "5699 tensor(18534.2031, grad_fn=<MseLossBackward0>)\n",
      "5700 tensor(18533.9863, grad_fn=<MseLossBackward0>)\n",
      "5701 tensor(18533.7695, grad_fn=<MseLossBackward0>)\n",
      "5702 tensor(18533.5488, grad_fn=<MseLossBackward0>)\n",
      "5703 tensor(18533.3320, grad_fn=<MseLossBackward0>)\n",
      "5704 tensor(18533.1152, grad_fn=<MseLossBackward0>)\n",
      "5705 tensor(18532.9004, grad_fn=<MseLossBackward0>)\n",
      "5706 tensor(18532.6816, grad_fn=<MseLossBackward0>)\n",
      "5707 tensor(18532.4629, grad_fn=<MseLossBackward0>)\n",
      "5708 tensor(18532.2480, grad_fn=<MseLossBackward0>)\n",
      "5709 tensor(18532.0332, grad_fn=<MseLossBackward0>)\n",
      "5710 tensor(18531.8145, grad_fn=<MseLossBackward0>)\n",
      "5711 tensor(18531.5996, grad_fn=<MseLossBackward0>)\n",
      "5712 tensor(18531.3828, grad_fn=<MseLossBackward0>)\n",
      "5713 tensor(18531.1680, grad_fn=<MseLossBackward0>)\n",
      "5714 tensor(18530.9531, grad_fn=<MseLossBackward0>)\n",
      "5715 tensor(18530.7383, grad_fn=<MseLossBackward0>)\n",
      "5716 tensor(18530.5234, grad_fn=<MseLossBackward0>)\n",
      "5717 tensor(18530.3086, grad_fn=<MseLossBackward0>)\n",
      "5718 tensor(18530.0918, grad_fn=<MseLossBackward0>)\n",
      "5719 tensor(18529.8789, grad_fn=<MseLossBackward0>)\n",
      "5720 tensor(18529.6641, grad_fn=<MseLossBackward0>)\n",
      "5721 tensor(18529.4492, grad_fn=<MseLossBackward0>)\n",
      "5722 tensor(18529.2363, grad_fn=<MseLossBackward0>)\n",
      "5723 tensor(18529.0234, grad_fn=<MseLossBackward0>)\n",
      "5724 tensor(18528.8105, grad_fn=<MseLossBackward0>)\n",
      "5725 tensor(18528.5957, grad_fn=<MseLossBackward0>)\n",
      "5726 tensor(18528.3828, grad_fn=<MseLossBackward0>)\n",
      "5727 tensor(18528.1699, grad_fn=<MseLossBackward0>)\n",
      "5728 tensor(18527.9551, grad_fn=<MseLossBackward0>)\n",
      "5729 tensor(18527.7441, grad_fn=<MseLossBackward0>)\n",
      "5730 tensor(18527.5332, grad_fn=<MseLossBackward0>)\n",
      "5731 tensor(18527.3184, grad_fn=<MseLossBackward0>)\n",
      "5732 tensor(18527.1074, grad_fn=<MseLossBackward0>)\n",
      "5733 tensor(18526.8945, grad_fn=<MseLossBackward0>)\n",
      "5734 tensor(18526.6836, grad_fn=<MseLossBackward0>)\n",
      "5735 tensor(18526.4727, grad_fn=<MseLossBackward0>)\n",
      "5736 tensor(18526.2598, grad_fn=<MseLossBackward0>)\n",
      "5737 tensor(18526.0488, grad_fn=<MseLossBackward0>)\n",
      "5738 tensor(18525.8379, grad_fn=<MseLossBackward0>)\n",
      "5739 tensor(18525.6270, grad_fn=<MseLossBackward0>)\n",
      "5740 tensor(18525.4160, grad_fn=<MseLossBackward0>)\n",
      "5741 tensor(18525.2070, grad_fn=<MseLossBackward0>)\n",
      "5742 tensor(18524.9941, grad_fn=<MseLossBackward0>)\n",
      "5743 tensor(18524.7891, grad_fn=<MseLossBackward0>)\n",
      "5744 tensor(18524.5781, grad_fn=<MseLossBackward0>)\n",
      "5745 tensor(18524.3672, grad_fn=<MseLossBackward0>)\n",
      "5746 tensor(18524.1562, grad_fn=<MseLossBackward0>)\n",
      "5747 tensor(18523.9473, grad_fn=<MseLossBackward0>)\n",
      "5748 tensor(18523.7383, grad_fn=<MseLossBackward0>)\n",
      "5749 tensor(18523.5293, grad_fn=<MseLossBackward0>)\n",
      "5750 tensor(18523.3223, grad_fn=<MseLossBackward0>)\n",
      "5751 tensor(18523.1133, grad_fn=<MseLossBackward0>)\n",
      "5752 tensor(18522.9043, grad_fn=<MseLossBackward0>)\n",
      "5753 tensor(18522.6934, grad_fn=<MseLossBackward0>)\n",
      "5754 tensor(18522.4844, grad_fn=<MseLossBackward0>)\n",
      "5755 tensor(18522.2773, grad_fn=<MseLossBackward0>)\n",
      "5756 tensor(18522.0703, grad_fn=<MseLossBackward0>)\n",
      "5757 tensor(18521.8613, grad_fn=<MseLossBackward0>)\n",
      "5758 tensor(18521.6543, grad_fn=<MseLossBackward0>)\n",
      "5759 tensor(18521.4492, grad_fn=<MseLossBackward0>)\n",
      "5760 tensor(18521.2402, grad_fn=<MseLossBackward0>)\n",
      "5761 tensor(18521.0332, grad_fn=<MseLossBackward0>)\n",
      "5762 tensor(18520.8242, grad_fn=<MseLossBackward0>)\n",
      "5763 tensor(18520.6191, grad_fn=<MseLossBackward0>)\n",
      "5764 tensor(18520.4121, grad_fn=<MseLossBackward0>)\n",
      "5765 tensor(18520.2051, grad_fn=<MseLossBackward0>)\n",
      "5766 tensor(18520., grad_fn=<MseLossBackward0>)\n",
      "5767 tensor(18519.7969, grad_fn=<MseLossBackward0>)\n",
      "5768 tensor(18519.5879, grad_fn=<MseLossBackward0>)\n",
      "5769 tensor(18519.3809, grad_fn=<MseLossBackward0>)\n",
      "5770 tensor(18519.1758, grad_fn=<MseLossBackward0>)\n",
      "5771 tensor(18518.9707, grad_fn=<MseLossBackward0>)\n",
      "5772 tensor(18518.7656, grad_fn=<MseLossBackward0>)\n",
      "5773 tensor(18518.5625, grad_fn=<MseLossBackward0>)\n",
      "5774 tensor(18518.3555, grad_fn=<MseLossBackward0>)\n",
      "5775 tensor(18518.1484, grad_fn=<MseLossBackward0>)\n",
      "5776 tensor(18517.9453, grad_fn=<MseLossBackward0>)\n",
      "5777 tensor(18517.7402, grad_fn=<MseLossBackward0>)\n",
      "5778 tensor(18517.5371, grad_fn=<MseLossBackward0>)\n",
      "5779 tensor(18517.3320, grad_fn=<MseLossBackward0>)\n",
      "5780 tensor(18517.1270, grad_fn=<MseLossBackward0>)\n",
      "5781 tensor(18516.9238, grad_fn=<MseLossBackward0>)\n",
      "5782 tensor(18516.7168, grad_fn=<MseLossBackward0>)\n",
      "5783 tensor(18516.5137, grad_fn=<MseLossBackward0>)\n",
      "5784 tensor(18516.3125, grad_fn=<MseLossBackward0>)\n",
      "5785 tensor(18516.1074, grad_fn=<MseLossBackward0>)\n",
      "5786 tensor(18515.9043, grad_fn=<MseLossBackward0>)\n",
      "5787 tensor(18515.7012, grad_fn=<MseLossBackward0>)\n",
      "5788 tensor(18515.4980, grad_fn=<MseLossBackward0>)\n",
      "5789 tensor(18515.2949, grad_fn=<MseLossBackward0>)\n",
      "5790 tensor(18515.0918, grad_fn=<MseLossBackward0>)\n",
      "5791 tensor(18514.8887, grad_fn=<MseLossBackward0>)\n",
      "5792 tensor(18514.6875, grad_fn=<MseLossBackward0>)\n",
      "5793 tensor(18514.4863, grad_fn=<MseLossBackward0>)\n",
      "5794 tensor(18514.2832, grad_fn=<MseLossBackward0>)\n",
      "5795 tensor(18514.0801, grad_fn=<MseLossBackward0>)\n",
      "5796 tensor(18513.8770, grad_fn=<MseLossBackward0>)\n",
      "5797 tensor(18513.6738, grad_fn=<MseLossBackward0>)\n",
      "5798 tensor(18513.4746, grad_fn=<MseLossBackward0>)\n",
      "5799 tensor(18513.2734, grad_fn=<MseLossBackward0>)\n",
      "5800 tensor(18513.0723, grad_fn=<MseLossBackward0>)\n",
      "5801 tensor(18512.8691, grad_fn=<MseLossBackward0>)\n",
      "5802 tensor(18512.6680, grad_fn=<MseLossBackward0>)\n",
      "5803 tensor(18512.4688, grad_fn=<MseLossBackward0>)\n",
      "5804 tensor(18512.2676, grad_fn=<MseLossBackward0>)\n",
      "5805 tensor(18512.0645, grad_fn=<MseLossBackward0>)\n",
      "5806 tensor(18511.8652, grad_fn=<MseLossBackward0>)\n",
      "5807 tensor(18511.6641, grad_fn=<MseLossBackward0>)\n",
      "5808 tensor(18511.4668, grad_fn=<MseLossBackward0>)\n",
      "5809 tensor(18511.2656, grad_fn=<MseLossBackward0>)\n",
      "5810 tensor(18511.0664, grad_fn=<MseLossBackward0>)\n",
      "5811 tensor(18510.8652, grad_fn=<MseLossBackward0>)\n",
      "5812 tensor(18510.6621, grad_fn=<MseLossBackward0>)\n",
      "5813 tensor(18510.4668, grad_fn=<MseLossBackward0>)\n",
      "5814 tensor(18510.2637, grad_fn=<MseLossBackward0>)\n",
      "5815 tensor(18510.0625, grad_fn=<MseLossBackward0>)\n",
      "5816 tensor(18509.8652, grad_fn=<MseLossBackward0>)\n",
      "5817 tensor(18509.6660, grad_fn=<MseLossBackward0>)\n",
      "5818 tensor(18509.4688, grad_fn=<MseLossBackward0>)\n",
      "5819 tensor(18509.2695, grad_fn=<MseLossBackward0>)\n",
      "5820 tensor(18509.0703, grad_fn=<MseLossBackward0>)\n",
      "5821 tensor(18508.8711, grad_fn=<MseLossBackward0>)\n",
      "5822 tensor(18508.6719, grad_fn=<MseLossBackward0>)\n",
      "5823 tensor(18508.4746, grad_fn=<MseLossBackward0>)\n",
      "5824 tensor(18508.2754, grad_fn=<MseLossBackward0>)\n",
      "5825 tensor(18508.0781, grad_fn=<MseLossBackward0>)\n",
      "5826 tensor(18507.8809, grad_fn=<MseLossBackward0>)\n",
      "5827 tensor(18507.6797, grad_fn=<MseLossBackward0>)\n",
      "5828 tensor(18507.4805, grad_fn=<MseLossBackward0>)\n",
      "5829 tensor(18507.2852, grad_fn=<MseLossBackward0>)\n",
      "5830 tensor(18507.0879, grad_fn=<MseLossBackward0>)\n",
      "5831 tensor(18506.8887, grad_fn=<MseLossBackward0>)\n",
      "5832 tensor(18506.6914, grad_fn=<MseLossBackward0>)\n",
      "5833 tensor(18506.4941, grad_fn=<MseLossBackward0>)\n",
      "5834 tensor(18506.2969, grad_fn=<MseLossBackward0>)\n",
      "5835 tensor(18506.0996, grad_fn=<MseLossBackward0>)\n",
      "5836 tensor(18505.9023, grad_fn=<MseLossBackward0>)\n",
      "5837 tensor(18505.7051, grad_fn=<MseLossBackward0>)\n",
      "5838 tensor(18505.5078, grad_fn=<MseLossBackward0>)\n",
      "5839 tensor(18505.3125, grad_fn=<MseLossBackward0>)\n",
      "5840 tensor(18505.1152, grad_fn=<MseLossBackward0>)\n",
      "5841 tensor(18504.9219, grad_fn=<MseLossBackward0>)\n",
      "5842 tensor(18504.7246, grad_fn=<MseLossBackward0>)\n",
      "5843 tensor(18504.5273, grad_fn=<MseLossBackward0>)\n",
      "5844 tensor(18504.3301, grad_fn=<MseLossBackward0>)\n",
      "5845 tensor(18504.1328, grad_fn=<MseLossBackward0>)\n",
      "5846 tensor(18503.9395, grad_fn=<MseLossBackward0>)\n",
      "5847 tensor(18503.7422, grad_fn=<MseLossBackward0>)\n",
      "5848 tensor(18503.5449, grad_fn=<MseLossBackward0>)\n",
      "5849 tensor(18503.3496, grad_fn=<MseLossBackward0>)\n",
      "5850 tensor(18503.1543, grad_fn=<MseLossBackward0>)\n",
      "5851 tensor(18502.9590, grad_fn=<MseLossBackward0>)\n",
      "5852 tensor(18502.7656, grad_fn=<MseLossBackward0>)\n",
      "5853 tensor(18502.5703, grad_fn=<MseLossBackward0>)\n",
      "5854 tensor(18502.3750, grad_fn=<MseLossBackward0>)\n",
      "5855 tensor(18502.1797, grad_fn=<MseLossBackward0>)\n",
      "5856 tensor(18501.9844, grad_fn=<MseLossBackward0>)\n",
      "5857 tensor(18501.7930, grad_fn=<MseLossBackward0>)\n",
      "5858 tensor(18501.5957, grad_fn=<MseLossBackward0>)\n",
      "5859 tensor(18501.4023, grad_fn=<MseLossBackward0>)\n",
      "5860 tensor(18501.2051, grad_fn=<MseLossBackward0>)\n",
      "5861 tensor(18501.0098, grad_fn=<MseLossBackward0>)\n",
      "5862 tensor(18500.8164, grad_fn=<MseLossBackward0>)\n",
      "5863 tensor(18500.6230, grad_fn=<MseLossBackward0>)\n",
      "5864 tensor(18500.4297, grad_fn=<MseLossBackward0>)\n",
      "5865 tensor(18500.2363, grad_fn=<MseLossBackward0>)\n",
      "5866 tensor(18500.0410, grad_fn=<MseLossBackward0>)\n",
      "5867 tensor(18499.8477, grad_fn=<MseLossBackward0>)\n",
      "5868 tensor(18499.6543, grad_fn=<MseLossBackward0>)\n",
      "5869 tensor(18499.4609, grad_fn=<MseLossBackward0>)\n",
      "5870 tensor(18499.2656, grad_fn=<MseLossBackward0>)\n",
      "5871 tensor(18499.0723, grad_fn=<MseLossBackward0>)\n",
      "5872 tensor(18498.8789, grad_fn=<MseLossBackward0>)\n",
      "5873 tensor(18498.6875, grad_fn=<MseLossBackward0>)\n",
      "5874 tensor(18498.4922, grad_fn=<MseLossBackward0>)\n",
      "5875 tensor(18498.3008, grad_fn=<MseLossBackward0>)\n",
      "5876 tensor(18498.1074, grad_fn=<MseLossBackward0>)\n",
      "5877 tensor(18497.9160, grad_fn=<MseLossBackward0>)\n",
      "5878 tensor(18497.7227, grad_fn=<MseLossBackward0>)\n",
      "5879 tensor(18497.5293, grad_fn=<MseLossBackward0>)\n",
      "5880 tensor(18497.3340, grad_fn=<MseLossBackward0>)\n",
      "5881 tensor(18497.1445, grad_fn=<MseLossBackward0>)\n",
      "5882 tensor(18496.9512, grad_fn=<MseLossBackward0>)\n",
      "5883 tensor(18496.7598, grad_fn=<MseLossBackward0>)\n",
      "5884 tensor(18496.5664, grad_fn=<MseLossBackward0>)\n",
      "5885 tensor(18496.3730, grad_fn=<MseLossBackward0>)\n",
      "5886 tensor(18496.1816, grad_fn=<MseLossBackward0>)\n",
      "5887 tensor(18495.9902, grad_fn=<MseLossBackward0>)\n",
      "5888 tensor(18495.7988, grad_fn=<MseLossBackward0>)\n",
      "5889 tensor(18495.6055, grad_fn=<MseLossBackward0>)\n",
      "5890 tensor(18495.4160, grad_fn=<MseLossBackward0>)\n",
      "5891 tensor(18495.2246, grad_fn=<MseLossBackward0>)\n",
      "5892 tensor(18495.0312, grad_fn=<MseLossBackward0>)\n",
      "5893 tensor(18494.8398, grad_fn=<MseLossBackward0>)\n",
      "5894 tensor(18494.6504, grad_fn=<MseLossBackward0>)\n",
      "5895 tensor(18494.4570, grad_fn=<MseLossBackward0>)\n",
      "5896 tensor(18494.2676, grad_fn=<MseLossBackward0>)\n",
      "5897 tensor(18494.0742, grad_fn=<MseLossBackward0>)\n",
      "5898 tensor(18493.8848, grad_fn=<MseLossBackward0>)\n",
      "5899 tensor(18493.6934, grad_fn=<MseLossBackward0>)\n",
      "5900 tensor(18493.5020, grad_fn=<MseLossBackward0>)\n",
      "5901 tensor(18493.3125, grad_fn=<MseLossBackward0>)\n",
      "5902 tensor(18493.1211, grad_fn=<MseLossBackward0>)\n",
      "5903 tensor(18492.9297, grad_fn=<MseLossBackward0>)\n",
      "5904 tensor(18492.7402, grad_fn=<MseLossBackward0>)\n",
      "5905 tensor(18492.5488, grad_fn=<MseLossBackward0>)\n",
      "5906 tensor(18492.3594, grad_fn=<MseLossBackward0>)\n",
      "5907 tensor(18492.1680, grad_fn=<MseLossBackward0>)\n",
      "5908 tensor(18491.9766, grad_fn=<MseLossBackward0>)\n",
      "5909 tensor(18491.7871, grad_fn=<MseLossBackward0>)\n",
      "5910 tensor(18491.5977, grad_fn=<MseLossBackward0>)\n",
      "5911 tensor(18491.4082, grad_fn=<MseLossBackward0>)\n",
      "5912 tensor(18491.2168, grad_fn=<MseLossBackward0>)\n",
      "5913 tensor(18491.0273, grad_fn=<MseLossBackward0>)\n",
      "5914 tensor(18490.8398, grad_fn=<MseLossBackward0>)\n",
      "5915 tensor(18490.6484, grad_fn=<MseLossBackward0>)\n",
      "5916 tensor(18490.4590, grad_fn=<MseLossBackward0>)\n",
      "5917 tensor(18490.2695, grad_fn=<MseLossBackward0>)\n",
      "5918 tensor(18490.0801, grad_fn=<MseLossBackward0>)\n",
      "5919 tensor(18489.8926, grad_fn=<MseLossBackward0>)\n",
      "5920 tensor(18489.7012, grad_fn=<MseLossBackward0>)\n",
      "5921 tensor(18489.5117, grad_fn=<MseLossBackward0>)\n",
      "5922 tensor(18489.3242, grad_fn=<MseLossBackward0>)\n",
      "5923 tensor(18489.1328, grad_fn=<MseLossBackward0>)\n",
      "5924 tensor(18488.9453, grad_fn=<MseLossBackward0>)\n",
      "5925 tensor(18488.7539, grad_fn=<MseLossBackward0>)\n",
      "5926 tensor(18488.5684, grad_fn=<MseLossBackward0>)\n",
      "5927 tensor(18488.3770, grad_fn=<MseLossBackward0>)\n",
      "5928 tensor(18488.1895, grad_fn=<MseLossBackward0>)\n",
      "5929 tensor(18488.0020, grad_fn=<MseLossBackward0>)\n",
      "5930 tensor(18487.8145, grad_fn=<MseLossBackward0>)\n",
      "5931 tensor(18487.6230, grad_fn=<MseLossBackward0>)\n",
      "5932 tensor(18487.4355, grad_fn=<MseLossBackward0>)\n",
      "5933 tensor(18487.2480, grad_fn=<MseLossBackward0>)\n",
      "5934 tensor(18487.0605, grad_fn=<MseLossBackward0>)\n",
      "5935 tensor(18486.8691, grad_fn=<MseLossBackward0>)\n",
      "5936 tensor(18486.6816, grad_fn=<MseLossBackward0>)\n",
      "5937 tensor(18486.4941, grad_fn=<MseLossBackward0>)\n",
      "5938 tensor(18486.3047, grad_fn=<MseLossBackward0>)\n",
      "5939 tensor(18486.1172, grad_fn=<MseLossBackward0>)\n",
      "5940 tensor(18485.9297, grad_fn=<MseLossBackward0>)\n",
      "5941 tensor(18485.7422, grad_fn=<MseLossBackward0>)\n",
      "5942 tensor(18485.5527, grad_fn=<MseLossBackward0>)\n",
      "5943 tensor(18485.3652, grad_fn=<MseLossBackward0>)\n",
      "5944 tensor(18485.1777, grad_fn=<MseLossBackward0>)\n",
      "5945 tensor(18484.9902, grad_fn=<MseLossBackward0>)\n",
      "5946 tensor(18484.8027, grad_fn=<MseLossBackward0>)\n",
      "5947 tensor(18484.6152, grad_fn=<MseLossBackward0>)\n",
      "5948 tensor(18484.4277, grad_fn=<MseLossBackward0>)\n",
      "5949 tensor(18484.2402, grad_fn=<MseLossBackward0>)\n",
      "5950 tensor(18484.0527, grad_fn=<MseLossBackward0>)\n",
      "5951 tensor(18483.8633, grad_fn=<MseLossBackward0>)\n",
      "5952 tensor(18483.6777, grad_fn=<MseLossBackward0>)\n",
      "5953 tensor(18483.4902, grad_fn=<MseLossBackward0>)\n",
      "5954 tensor(18483.3027, grad_fn=<MseLossBackward0>)\n",
      "5955 tensor(18483.1172, grad_fn=<MseLossBackward0>)\n",
      "5956 tensor(18482.9297, grad_fn=<MseLossBackward0>)\n",
      "5957 tensor(18482.7441, grad_fn=<MseLossBackward0>)\n",
      "5958 tensor(18482.5566, grad_fn=<MseLossBackward0>)\n",
      "5959 tensor(18482.3691, grad_fn=<MseLossBackward0>)\n",
      "5960 tensor(18482.1816, grad_fn=<MseLossBackward0>)\n",
      "5961 tensor(18481.9961, grad_fn=<MseLossBackward0>)\n",
      "5962 tensor(18481.8086, grad_fn=<MseLossBackward0>)\n",
      "5963 tensor(18481.6230, grad_fn=<MseLossBackward0>)\n",
      "5964 tensor(18481.4375, grad_fn=<MseLossBackward0>)\n",
      "5965 tensor(18481.2500, grad_fn=<MseLossBackward0>)\n",
      "5966 tensor(18481.0625, grad_fn=<MseLossBackward0>)\n",
      "5967 tensor(18480.8770, grad_fn=<MseLossBackward0>)\n",
      "5968 tensor(18480.6914, grad_fn=<MseLossBackward0>)\n",
      "5969 tensor(18480.5059, grad_fn=<MseLossBackward0>)\n",
      "5970 tensor(18480.3203, grad_fn=<MseLossBackward0>)\n",
      "5971 tensor(18480.1328, grad_fn=<MseLossBackward0>)\n",
      "5972 tensor(18479.9453, grad_fn=<MseLossBackward0>)\n",
      "5973 tensor(18479.7617, grad_fn=<MseLossBackward0>)\n",
      "5974 tensor(18479.5742, grad_fn=<MseLossBackward0>)\n",
      "5975 tensor(18479.3906, grad_fn=<MseLossBackward0>)\n",
      "5976 tensor(18479.2031, grad_fn=<MseLossBackward0>)\n",
      "5977 tensor(18479.0176, grad_fn=<MseLossBackward0>)\n",
      "5978 tensor(18478.8301, grad_fn=<MseLossBackward0>)\n",
      "5979 tensor(18478.6465, grad_fn=<MseLossBackward0>)\n",
      "5980 tensor(18478.4609, grad_fn=<MseLossBackward0>)\n",
      "5981 tensor(18478.2754, grad_fn=<MseLossBackward0>)\n",
      "5982 tensor(18478.0898, grad_fn=<MseLossBackward0>)\n",
      "5983 tensor(18477.9043, grad_fn=<MseLossBackward0>)\n",
      "5984 tensor(18477.7168, grad_fn=<MseLossBackward0>)\n",
      "5985 tensor(18477.5332, grad_fn=<MseLossBackward0>)\n",
      "5986 tensor(18477.3477, grad_fn=<MseLossBackward0>)\n",
      "5987 tensor(18477.1602, grad_fn=<MseLossBackward0>)\n",
      "5988 tensor(18476.9766, grad_fn=<MseLossBackward0>)\n",
      "5989 tensor(18476.7930, grad_fn=<MseLossBackward0>)\n",
      "5990 tensor(18476.6055, grad_fn=<MseLossBackward0>)\n",
      "5991 tensor(18476.4219, grad_fn=<MseLossBackward0>)\n",
      "5992 tensor(18476.2383, grad_fn=<MseLossBackward0>)\n",
      "5993 tensor(18476.0527, grad_fn=<MseLossBackward0>)\n",
      "5994 tensor(18475.8672, grad_fn=<MseLossBackward0>)\n",
      "5995 tensor(18475.6816, grad_fn=<MseLossBackward0>)\n",
      "5996 tensor(18475.4980, grad_fn=<MseLossBackward0>)\n",
      "5997 tensor(18475.3145, grad_fn=<MseLossBackward0>)\n",
      "5998 tensor(18475.1270, grad_fn=<MseLossBackward0>)\n",
      "5999 tensor(18474.9414, grad_fn=<MseLossBackward0>)\n",
      "6000 tensor(18474.7598, grad_fn=<MseLossBackward0>)\n",
      "6001 tensor(18474.5762, grad_fn=<MseLossBackward0>)\n",
      "6002 tensor(18474.3906, grad_fn=<MseLossBackward0>)\n",
      "6003 tensor(18474.2051, grad_fn=<MseLossBackward0>)\n",
      "6004 tensor(18474.0234, grad_fn=<MseLossBackward0>)\n",
      "6005 tensor(18473.8359, grad_fn=<MseLossBackward0>)\n",
      "6006 tensor(18473.6523, grad_fn=<MseLossBackward0>)\n",
      "6007 tensor(18473.4688, grad_fn=<MseLossBackward0>)\n",
      "6008 tensor(18473.2812, grad_fn=<MseLossBackward0>)\n",
      "6009 tensor(18473.0977, grad_fn=<MseLossBackward0>)\n",
      "6010 tensor(18472.9141, grad_fn=<MseLossBackward0>)\n",
      "6011 tensor(18472.7324, grad_fn=<MseLossBackward0>)\n",
      "6012 tensor(18472.5469, grad_fn=<MseLossBackward0>)\n",
      "6013 tensor(18472.3633, grad_fn=<MseLossBackward0>)\n",
      "6014 tensor(18472.1777, grad_fn=<MseLossBackward0>)\n",
      "6015 tensor(18471.9980, grad_fn=<MseLossBackward0>)\n",
      "6016 tensor(18471.8105, grad_fn=<MseLossBackward0>)\n",
      "6017 tensor(18471.6289, grad_fn=<MseLossBackward0>)\n",
      "6018 tensor(18471.4434, grad_fn=<MseLossBackward0>)\n",
      "6019 tensor(18471.2617, grad_fn=<MseLossBackward0>)\n",
      "6020 tensor(18471.0742, grad_fn=<MseLossBackward0>)\n",
      "6021 tensor(18470.8926, grad_fn=<MseLossBackward0>)\n",
      "6022 tensor(18470.7070, grad_fn=<MseLossBackward0>)\n",
      "6023 tensor(18470.5254, grad_fn=<MseLossBackward0>)\n",
      "6024 tensor(18470.3438, grad_fn=<MseLossBackward0>)\n",
      "6025 tensor(18470.1602, grad_fn=<MseLossBackward0>)\n",
      "6026 tensor(18469.9727, grad_fn=<MseLossBackward0>)\n",
      "6027 tensor(18469.7930, grad_fn=<MseLossBackward0>)\n",
      "6028 tensor(18469.6094, grad_fn=<MseLossBackward0>)\n",
      "6029 tensor(18469.4238, grad_fn=<MseLossBackward0>)\n",
      "6030 tensor(18469.2402, grad_fn=<MseLossBackward0>)\n",
      "6031 tensor(18469.0566, grad_fn=<MseLossBackward0>)\n",
      "6032 tensor(18468.8750, grad_fn=<MseLossBackward0>)\n",
      "6033 tensor(18468.6895, grad_fn=<MseLossBackward0>)\n",
      "6034 tensor(18468.5078, grad_fn=<MseLossBackward0>)\n",
      "6035 tensor(18468.3242, grad_fn=<MseLossBackward0>)\n",
      "6036 tensor(18468.1426, grad_fn=<MseLossBackward0>)\n",
      "6037 tensor(18467.9590, grad_fn=<MseLossBackward0>)\n",
      "6038 tensor(18467.7734, grad_fn=<MseLossBackward0>)\n",
      "6039 tensor(18467.5938, grad_fn=<MseLossBackward0>)\n",
      "6040 tensor(18467.4082, grad_fn=<MseLossBackward0>)\n",
      "6041 tensor(18467.2246, grad_fn=<MseLossBackward0>)\n",
      "6042 tensor(18467.0430, grad_fn=<MseLossBackward0>)\n",
      "6043 tensor(18466.8594, grad_fn=<MseLossBackward0>)\n",
      "6044 tensor(18466.6777, grad_fn=<MseLossBackward0>)\n",
      "6045 tensor(18466.4941, grad_fn=<MseLossBackward0>)\n",
      "6046 tensor(18466.3125, grad_fn=<MseLossBackward0>)\n",
      "6047 tensor(18466.1289, grad_fn=<MseLossBackward0>)\n",
      "6048 tensor(18465.9492, grad_fn=<MseLossBackward0>)\n",
      "6049 tensor(18465.7637, grad_fn=<MseLossBackward0>)\n",
      "6050 tensor(18465.5820, grad_fn=<MseLossBackward0>)\n",
      "6051 tensor(18465.3984, grad_fn=<MseLossBackward0>)\n",
      "6052 tensor(18465.2168, grad_fn=<MseLossBackward0>)\n",
      "6053 tensor(18465.0332, grad_fn=<MseLossBackward0>)\n",
      "6054 tensor(18464.8496, grad_fn=<MseLossBackward0>)\n",
      "6055 tensor(18464.6680, grad_fn=<MseLossBackward0>)\n",
      "6056 tensor(18464.4863, grad_fn=<MseLossBackward0>)\n",
      "6057 tensor(18464.3047, grad_fn=<MseLossBackward0>)\n",
      "6058 tensor(18464.1211, grad_fn=<MseLossBackward0>)\n",
      "6059 tensor(18463.9395, grad_fn=<MseLossBackward0>)\n",
      "6060 tensor(18463.7559, grad_fn=<MseLossBackward0>)\n",
      "6061 tensor(18463.5742, grad_fn=<MseLossBackward0>)\n",
      "6062 tensor(18463.3926, grad_fn=<MseLossBackward0>)\n",
      "6063 tensor(18463.2109, grad_fn=<MseLossBackward0>)\n",
      "6064 tensor(18463.0293, grad_fn=<MseLossBackward0>)\n",
      "6065 tensor(18462.8457, grad_fn=<MseLossBackward0>)\n",
      "6066 tensor(18462.6641, grad_fn=<MseLossBackward0>)\n",
      "6067 tensor(18462.4824, grad_fn=<MseLossBackward0>)\n",
      "6068 tensor(18462.2988, grad_fn=<MseLossBackward0>)\n",
      "6069 tensor(18462.1191, grad_fn=<MseLossBackward0>)\n",
      "6070 tensor(18461.9375, grad_fn=<MseLossBackward0>)\n",
      "6071 tensor(18461.7520, grad_fn=<MseLossBackward0>)\n",
      "6072 tensor(18461.5723, grad_fn=<MseLossBackward0>)\n",
      "6073 tensor(18461.3906, grad_fn=<MseLossBackward0>)\n",
      "6074 tensor(18461.2070, grad_fn=<MseLossBackward0>)\n",
      "6075 tensor(18461.0254, grad_fn=<MseLossBackward0>)\n",
      "6076 tensor(18460.8438, grad_fn=<MseLossBackward0>)\n",
      "6077 tensor(18460.6641, grad_fn=<MseLossBackward0>)\n",
      "6078 tensor(18460.4785, grad_fn=<MseLossBackward0>)\n",
      "6079 tensor(18460.2988, grad_fn=<MseLossBackward0>)\n",
      "6080 tensor(18460.1172, grad_fn=<MseLossBackward0>)\n",
      "6081 tensor(18459.9355, grad_fn=<MseLossBackward0>)\n",
      "6082 tensor(18459.7520, grad_fn=<MseLossBackward0>)\n",
      "6083 tensor(18459.5723, grad_fn=<MseLossBackward0>)\n",
      "6084 tensor(18459.3906, grad_fn=<MseLossBackward0>)\n",
      "6085 tensor(18459.2090, grad_fn=<MseLossBackward0>)\n",
      "6086 tensor(18459.0293, grad_fn=<MseLossBackward0>)\n",
      "6087 tensor(18458.8438, grad_fn=<MseLossBackward0>)\n",
      "6088 tensor(18458.6660, grad_fn=<MseLossBackward0>)\n",
      "6089 tensor(18458.4824, grad_fn=<MseLossBackward0>)\n",
      "6090 tensor(18458.3008, grad_fn=<MseLossBackward0>)\n",
      "6091 tensor(18458.1211, grad_fn=<MseLossBackward0>)\n",
      "6092 tensor(18457.9375, grad_fn=<MseLossBackward0>)\n",
      "6093 tensor(18457.7598, grad_fn=<MseLossBackward0>)\n",
      "6094 tensor(18457.5742, grad_fn=<MseLossBackward0>)\n",
      "6095 tensor(18457.3945, grad_fn=<MseLossBackward0>)\n",
      "6096 tensor(18457.2148, grad_fn=<MseLossBackward0>)\n",
      "6097 tensor(18457.0332, grad_fn=<MseLossBackward0>)\n",
      "6098 tensor(18456.8496, grad_fn=<MseLossBackward0>)\n",
      "6099 tensor(18456.6699, grad_fn=<MseLossBackward0>)\n",
      "6100 tensor(18456.4863, grad_fn=<MseLossBackward0>)\n",
      "6101 tensor(18456.3066, grad_fn=<MseLossBackward0>)\n",
      "6102 tensor(18456.1270, grad_fn=<MseLossBackward0>)\n",
      "6103 tensor(18455.9453, grad_fn=<MseLossBackward0>)\n",
      "6104 tensor(18455.7656, grad_fn=<MseLossBackward0>)\n",
      "6105 tensor(18455.5840, grad_fn=<MseLossBackward0>)\n",
      "6106 tensor(18455.4023, grad_fn=<MseLossBackward0>)\n",
      "6107 tensor(18455.2207, grad_fn=<MseLossBackward0>)\n",
      "6108 tensor(18455.0391, grad_fn=<MseLossBackward0>)\n",
      "6109 tensor(18454.8594, grad_fn=<MseLossBackward0>)\n",
      "6110 tensor(18454.6777, grad_fn=<MseLossBackward0>)\n",
      "6111 tensor(18454.4980, grad_fn=<MseLossBackward0>)\n",
      "6112 tensor(18454.3164, grad_fn=<MseLossBackward0>)\n",
      "6113 tensor(18454.1348, grad_fn=<MseLossBackward0>)\n",
      "6114 tensor(18453.9531, grad_fn=<MseLossBackward0>)\n",
      "6115 tensor(18453.7734, grad_fn=<MseLossBackward0>)\n",
      "6116 tensor(18453.5918, grad_fn=<MseLossBackward0>)\n",
      "6117 tensor(18453.4121, grad_fn=<MseLossBackward0>)\n",
      "6118 tensor(18453.2305, grad_fn=<MseLossBackward0>)\n",
      "6119 tensor(18453.0488, grad_fn=<MseLossBackward0>)\n",
      "6120 tensor(18452.8691, grad_fn=<MseLossBackward0>)\n",
      "6121 tensor(18452.6875, grad_fn=<MseLossBackward0>)\n",
      "6122 tensor(18452.5078, grad_fn=<MseLossBackward0>)\n",
      "6123 tensor(18452.3262, grad_fn=<MseLossBackward0>)\n",
      "6124 tensor(18452.1465, grad_fn=<MseLossBackward0>)\n",
      "6125 tensor(18451.9648, grad_fn=<MseLossBackward0>)\n",
      "6126 tensor(18451.7852, grad_fn=<MseLossBackward0>)\n",
      "6127 tensor(18451.6055, grad_fn=<MseLossBackward0>)\n",
      "6128 tensor(18451.4238, grad_fn=<MseLossBackward0>)\n",
      "6129 tensor(18451.2441, grad_fn=<MseLossBackward0>)\n",
      "6130 tensor(18451.0645, grad_fn=<MseLossBackward0>)\n",
      "6131 tensor(18450.8828, grad_fn=<MseLossBackward0>)\n",
      "6132 tensor(18450.6992, grad_fn=<MseLossBackward0>)\n",
      "6133 tensor(18450.5195, grad_fn=<MseLossBackward0>)\n",
      "6134 tensor(18450.3379, grad_fn=<MseLossBackward0>)\n",
      "6135 tensor(18450.1582, grad_fn=<MseLossBackward0>)\n",
      "6136 tensor(18449.9785, grad_fn=<MseLossBackward0>)\n",
      "6137 tensor(18449.7988, grad_fn=<MseLossBackward0>)\n",
      "6138 tensor(18449.6172, grad_fn=<MseLossBackward0>)\n",
      "6139 tensor(18449.4395, grad_fn=<MseLossBackward0>)\n",
      "6140 tensor(18449.2598, grad_fn=<MseLossBackward0>)\n",
      "6141 tensor(18449.0781, grad_fn=<MseLossBackward0>)\n",
      "6142 tensor(18448.8965, grad_fn=<MseLossBackward0>)\n",
      "6143 tensor(18448.7168, grad_fn=<MseLossBackward0>)\n",
      "6144 tensor(18448.5352, grad_fn=<MseLossBackward0>)\n",
      "6145 tensor(18448.3555, grad_fn=<MseLossBackward0>)\n",
      "6146 tensor(18448.1758, grad_fn=<MseLossBackward0>)\n",
      "6147 tensor(18447.9941, grad_fn=<MseLossBackward0>)\n",
      "6148 tensor(18447.8164, grad_fn=<MseLossBackward0>)\n",
      "6149 tensor(18447.6348, grad_fn=<MseLossBackward0>)\n",
      "6150 tensor(18447.4531, grad_fn=<MseLossBackward0>)\n",
      "6151 tensor(18447.2754, grad_fn=<MseLossBackward0>)\n",
      "6152 tensor(18447.0938, grad_fn=<MseLossBackward0>)\n",
      "6153 tensor(18446.9121, grad_fn=<MseLossBackward0>)\n",
      "6154 tensor(18446.7344, grad_fn=<MseLossBackward0>)\n",
      "6155 tensor(18446.5527, grad_fn=<MseLossBackward0>)\n",
      "6156 tensor(18446.3730, grad_fn=<MseLossBackward0>)\n",
      "6157 tensor(18446.1934, grad_fn=<MseLossBackward0>)\n",
      "6158 tensor(18446.0137, grad_fn=<MseLossBackward0>)\n",
      "6159 tensor(18445.8340, grad_fn=<MseLossBackward0>)\n",
      "6160 tensor(18445.6543, grad_fn=<MseLossBackward0>)\n",
      "6161 tensor(18445.4727, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# test_out = model(test_data)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# test_loss = F.mse_loss(test_out, test_data.y)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# test_loss_l.append(test_loss.item())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(epoch, train_loss)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "train_data = ssp_obj.train_data.to(device)\n",
    "# test_data = ssp_obj.test_data.to(device)\n",
    "train_loss_l = []\n",
    "# test_loss_l = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(8000):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data, train_data.batch)\n",
    "    train_loss = F.mse_loss(out, train_data.y)\n",
    "    train_loss_l.append(train_loss.item())\n",
    "\n",
    "    # test_out = model(test_data)\n",
    "    # test_loss = F.mse_loss(test_out, test_data.y)\n",
    "    # test_loss_l.append(test_loss.item())\n",
    "\n",
    "    print(epoch, train_loss)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jqcla\\AppData\\Local\\Temp\\ipykernel_5360\\57309726.py:4: UserWarning: Using a target size (torch.Size([163800])) that is different to the input size (torch.Size([1, 163800])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  train_loss = F.mse_loss(out, train_data.y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2983.2488, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(2983.2231, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(2983.1978, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(2983.1721, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(2983.1467, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(2983.1211, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(2983.0957, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(2983.0698, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(2983.0442, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(2983.0188, grad_fn=<MseLossBackward0>)\n",
      "10 tensor(2982.9934, grad_fn=<MseLossBackward0>)\n",
      "11 tensor(2982.9678, grad_fn=<MseLossBackward0>)\n",
      "12 tensor(2982.9419, grad_fn=<MseLossBackward0>)\n",
      "13 tensor(2982.9167, grad_fn=<MseLossBackward0>)\n",
      "14 tensor(2982.8914, grad_fn=<MseLossBackward0>)\n",
      "15 tensor(2982.8657, grad_fn=<MseLossBackward0>)\n",
      "16 tensor(2982.8398, grad_fn=<MseLossBackward0>)\n",
      "17 tensor(2982.8149, grad_fn=<MseLossBackward0>)\n",
      "18 tensor(2982.7893, grad_fn=<MseLossBackward0>)\n",
      "19 tensor(2982.7639, grad_fn=<MseLossBackward0>)\n",
      "20 tensor(2982.7385, grad_fn=<MseLossBackward0>)\n",
      "21 tensor(2982.7129, grad_fn=<MseLossBackward0>)\n",
      "22 tensor(2982.6877, grad_fn=<MseLossBackward0>)\n",
      "23 tensor(2982.6621, grad_fn=<MseLossBackward0>)\n",
      "24 tensor(2982.6365, grad_fn=<MseLossBackward0>)\n",
      "25 tensor(2982.6116, grad_fn=<MseLossBackward0>)\n",
      "26 tensor(2982.5857, grad_fn=<MseLossBackward0>)\n",
      "27 tensor(2982.5608, grad_fn=<MseLossBackward0>)\n",
      "28 tensor(2982.5352, grad_fn=<MseLossBackward0>)\n",
      "29 tensor(2982.5098, grad_fn=<MseLossBackward0>)\n",
      "30 tensor(2982.4844, grad_fn=<MseLossBackward0>)\n",
      "31 tensor(2982.4595, grad_fn=<MseLossBackward0>)\n",
      "32 tensor(2982.4336, grad_fn=<MseLossBackward0>)\n",
      "33 tensor(2982.4084, grad_fn=<MseLossBackward0>)\n",
      "34 tensor(2982.3828, grad_fn=<MseLossBackward0>)\n",
      "35 tensor(2982.3579, grad_fn=<MseLossBackward0>)\n",
      "36 tensor(2982.3325, grad_fn=<MseLossBackward0>)\n",
      "37 tensor(2982.3071, grad_fn=<MseLossBackward0>)\n",
      "38 tensor(2982.2820, grad_fn=<MseLossBackward0>)\n",
      "39 tensor(2982.2566, grad_fn=<MseLossBackward0>)\n",
      "40 tensor(2982.2312, grad_fn=<MseLossBackward0>)\n",
      "41 tensor(2982.2063, grad_fn=<MseLossBackward0>)\n",
      "42 tensor(2982.1809, grad_fn=<MseLossBackward0>)\n",
      "43 tensor(2982.1558, grad_fn=<MseLossBackward0>)\n",
      "44 tensor(2982.1304, grad_fn=<MseLossBackward0>)\n",
      "45 tensor(2982.1050, grad_fn=<MseLossBackward0>)\n",
      "46 tensor(2982.0796, grad_fn=<MseLossBackward0>)\n",
      "47 tensor(2982.0547, grad_fn=<MseLossBackward0>)\n",
      "48 tensor(2982.0295, grad_fn=<MseLossBackward0>)\n",
      "49 tensor(2982.0042, grad_fn=<MseLossBackward0>)\n",
      "50 tensor(2981.9792, grad_fn=<MseLossBackward0>)\n",
      "51 tensor(2981.9541, grad_fn=<MseLossBackward0>)\n",
      "52 tensor(2981.9290, grad_fn=<MseLossBackward0>)\n",
      "53 tensor(2981.9038, grad_fn=<MseLossBackward0>)\n",
      "54 tensor(2981.8784, grad_fn=<MseLossBackward0>)\n",
      "55 tensor(2981.8533, grad_fn=<MseLossBackward0>)\n",
      "56 tensor(2981.8281, grad_fn=<MseLossBackward0>)\n",
      "57 tensor(2981.8030, grad_fn=<MseLossBackward0>)\n",
      "58 tensor(2981.7781, grad_fn=<MseLossBackward0>)\n",
      "59 tensor(2981.7529, grad_fn=<MseLossBackward0>)\n",
      "60 tensor(2981.7278, grad_fn=<MseLossBackward0>)\n",
      "61 tensor(2981.7029, grad_fn=<MseLossBackward0>)\n",
      "62 tensor(2981.6775, grad_fn=<MseLossBackward0>)\n",
      "63 tensor(2981.6526, grad_fn=<MseLossBackward0>)\n",
      "64 tensor(2981.6274, grad_fn=<MseLossBackward0>)\n",
      "65 tensor(2981.6021, grad_fn=<MseLossBackward0>)\n",
      "66 tensor(2981.5774, grad_fn=<MseLossBackward0>)\n",
      "67 tensor(2981.5522, grad_fn=<MseLossBackward0>)\n",
      "68 tensor(2981.5273, grad_fn=<MseLossBackward0>)\n",
      "69 tensor(2981.5020, grad_fn=<MseLossBackward0>)\n",
      "70 tensor(2981.4771, grad_fn=<MseLossBackward0>)\n",
      "71 tensor(2981.4524, grad_fn=<MseLossBackward0>)\n",
      "72 tensor(2981.4270, grad_fn=<MseLossBackward0>)\n",
      "73 tensor(2981.4023, grad_fn=<MseLossBackward0>)\n",
      "74 tensor(2981.3772, grad_fn=<MseLossBackward0>)\n",
      "75 tensor(2981.3521, grad_fn=<MseLossBackward0>)\n",
      "76 tensor(2981.3274, grad_fn=<MseLossBackward0>)\n",
      "77 tensor(2981.3025, grad_fn=<MseLossBackward0>)\n",
      "78 tensor(2981.2773, grad_fn=<MseLossBackward0>)\n",
      "79 tensor(2981.2524, grad_fn=<MseLossBackward0>)\n",
      "80 tensor(2981.2275, grad_fn=<MseLossBackward0>)\n",
      "81 tensor(2981.2024, grad_fn=<MseLossBackward0>)\n",
      "82 tensor(2981.1777, grad_fn=<MseLossBackward0>)\n",
      "83 tensor(2981.1528, grad_fn=<MseLossBackward0>)\n",
      "84 tensor(2981.1279, grad_fn=<MseLossBackward0>)\n",
      "85 tensor(2981.1030, grad_fn=<MseLossBackward0>)\n",
      "86 tensor(2981.0784, grad_fn=<MseLossBackward0>)\n",
      "87 tensor(2981.0532, grad_fn=<MseLossBackward0>)\n",
      "88 tensor(2981.0283, grad_fn=<MseLossBackward0>)\n",
      "89 tensor(2981.0037, grad_fn=<MseLossBackward0>)\n",
      "90 tensor(2980.9785, grad_fn=<MseLossBackward0>)\n",
      "91 tensor(2980.9539, grad_fn=<MseLossBackward0>)\n",
      "92 tensor(2980.9290, grad_fn=<MseLossBackward0>)\n",
      "93 tensor(2980.9043, grad_fn=<MseLossBackward0>)\n",
      "94 tensor(2980.8794, grad_fn=<MseLossBackward0>)\n",
      "95 tensor(2980.8545, grad_fn=<MseLossBackward0>)\n",
      "96 tensor(2980.8296, grad_fn=<MseLossBackward0>)\n",
      "97 tensor(2980.8052, grad_fn=<MseLossBackward0>)\n",
      "98 tensor(2980.7803, grad_fn=<MseLossBackward0>)\n",
      "99 tensor(2980.7554, grad_fn=<MseLossBackward0>)\n",
      "100 tensor(2980.7310, grad_fn=<MseLossBackward0>)\n",
      "101 tensor(2980.7061, grad_fn=<MseLossBackward0>)\n",
      "102 tensor(2980.6812, grad_fn=<MseLossBackward0>)\n",
      "103 tensor(2980.6565, grad_fn=<MseLossBackward0>)\n",
      "104 tensor(2980.6316, grad_fn=<MseLossBackward0>)\n",
      "105 tensor(2980.6069, grad_fn=<MseLossBackward0>)\n",
      "106 tensor(2980.5823, grad_fn=<MseLossBackward0>)\n",
      "107 tensor(2980.5579, grad_fn=<MseLossBackward0>)\n",
      "108 tensor(2980.5330, grad_fn=<MseLossBackward0>)\n",
      "109 tensor(2980.5083, grad_fn=<MseLossBackward0>)\n",
      "110 tensor(2980.4834, grad_fn=<MseLossBackward0>)\n",
      "111 tensor(2980.4590, grad_fn=<MseLossBackward0>)\n",
      "112 tensor(2980.4343, grad_fn=<MseLossBackward0>)\n",
      "113 tensor(2980.4094, grad_fn=<MseLossBackward0>)\n",
      "114 tensor(2980.3848, grad_fn=<MseLossBackward0>)\n",
      "115 tensor(2980.3604, grad_fn=<MseLossBackward0>)\n",
      "116 tensor(2980.3359, grad_fn=<MseLossBackward0>)\n",
      "117 tensor(2980.3110, grad_fn=<MseLossBackward0>)\n",
      "118 tensor(2980.2866, grad_fn=<MseLossBackward0>)\n",
      "119 tensor(2980.2617, grad_fn=<MseLossBackward0>)\n",
      "120 tensor(2980.2373, grad_fn=<MseLossBackward0>)\n",
      "121 tensor(2980.2129, grad_fn=<MseLossBackward0>)\n",
      "122 tensor(2980.1882, grad_fn=<MseLossBackward0>)\n",
      "123 tensor(2980.1636, grad_fn=<MseLossBackward0>)\n",
      "124 tensor(2980.1394, grad_fn=<MseLossBackward0>)\n",
      "125 tensor(2980.1145, grad_fn=<MseLossBackward0>)\n",
      "126 tensor(2980.0903, grad_fn=<MseLossBackward0>)\n",
      "127 tensor(2980.0659, grad_fn=<MseLossBackward0>)\n",
      "128 tensor(2980.0410, grad_fn=<MseLossBackward0>)\n",
      "129 tensor(2980.0166, grad_fn=<MseLossBackward0>)\n",
      "130 tensor(2979.9919, grad_fn=<MseLossBackward0>)\n",
      "131 tensor(2979.9675, grad_fn=<MseLossBackward0>)\n",
      "132 tensor(2979.9429, grad_fn=<MseLossBackward0>)\n",
      "133 tensor(2979.9187, grad_fn=<MseLossBackward0>)\n",
      "134 tensor(2979.8943, grad_fn=<MseLossBackward0>)\n",
      "135 tensor(2979.8699, grad_fn=<MseLossBackward0>)\n",
      "136 tensor(2979.8455, grad_fn=<MseLossBackward0>)\n",
      "137 tensor(2979.8210, grad_fn=<MseLossBackward0>)\n",
      "138 tensor(2979.7966, grad_fn=<MseLossBackward0>)\n",
      "139 tensor(2979.7720, grad_fn=<MseLossBackward0>)\n",
      "140 tensor(2979.7473, grad_fn=<MseLossBackward0>)\n",
      "141 tensor(2979.7231, grad_fn=<MseLossBackward0>)\n",
      "142 tensor(2979.6987, grad_fn=<MseLossBackward0>)\n",
      "143 tensor(2979.6748, grad_fn=<MseLossBackward0>)\n",
      "144 tensor(2979.6504, grad_fn=<MseLossBackward0>)\n",
      "145 tensor(2979.6257, grad_fn=<MseLossBackward0>)\n",
      "146 tensor(2979.6013, grad_fn=<MseLossBackward0>)\n",
      "147 tensor(2979.5771, grad_fn=<MseLossBackward0>)\n",
      "148 tensor(2979.5527, grad_fn=<MseLossBackward0>)\n",
      "149 tensor(2979.5283, grad_fn=<MseLossBackward0>)\n",
      "150 tensor(2979.5042, grad_fn=<MseLossBackward0>)\n",
      "151 tensor(2979.4800, grad_fn=<MseLossBackward0>)\n",
      "152 tensor(2979.4558, grad_fn=<MseLossBackward0>)\n",
      "153 tensor(2979.4314, grad_fn=<MseLossBackward0>)\n",
      "154 tensor(2979.4070, grad_fn=<MseLossBackward0>)\n",
      "155 tensor(2979.3826, grad_fn=<MseLossBackward0>)\n",
      "156 tensor(2979.3584, grad_fn=<MseLossBackward0>)\n",
      "157 tensor(2979.3340, grad_fn=<MseLossBackward0>)\n",
      "158 tensor(2979.3103, grad_fn=<MseLossBackward0>)\n",
      "159 tensor(2979.2856, grad_fn=<MseLossBackward0>)\n",
      "160 tensor(2979.2612, grad_fn=<MseLossBackward0>)\n",
      "161 tensor(2979.2371, grad_fn=<MseLossBackward0>)\n",
      "162 tensor(2979.2131, grad_fn=<MseLossBackward0>)\n",
      "163 tensor(2979.1890, grad_fn=<MseLossBackward0>)\n",
      "164 tensor(2979.1648, grad_fn=<MseLossBackward0>)\n",
      "165 tensor(2979.1406, grad_fn=<MseLossBackward0>)\n",
      "166 tensor(2979.1167, grad_fn=<MseLossBackward0>)\n",
      "167 tensor(2979.0920, grad_fn=<MseLossBackward0>)\n",
      "168 tensor(2979.0679, grad_fn=<MseLossBackward0>)\n",
      "169 tensor(2979.0439, grad_fn=<MseLossBackward0>)\n",
      "170 tensor(2979.0198, grad_fn=<MseLossBackward0>)\n",
      "171 tensor(2978.9958, grad_fn=<MseLossBackward0>)\n",
      "172 tensor(2978.9714, grad_fn=<MseLossBackward0>)\n",
      "173 tensor(2978.9475, grad_fn=<MseLossBackward0>)\n",
      "174 tensor(2978.9233, grad_fn=<MseLossBackward0>)\n",
      "175 tensor(2978.8992, grad_fn=<MseLossBackward0>)\n",
      "176 tensor(2978.8750, grad_fn=<MseLossBackward0>)\n",
      "177 tensor(2978.8511, grad_fn=<MseLossBackward0>)\n",
      "178 tensor(2978.8271, grad_fn=<MseLossBackward0>)\n",
      "179 tensor(2978.8032, grad_fn=<MseLossBackward0>)\n",
      "180 tensor(2978.7793, grad_fn=<MseLossBackward0>)\n",
      "181 tensor(2978.7549, grad_fn=<MseLossBackward0>)\n",
      "182 tensor(2978.7310, grad_fn=<MseLossBackward0>)\n",
      "183 tensor(2978.7070, grad_fn=<MseLossBackward0>)\n",
      "184 tensor(2978.6831, grad_fn=<MseLossBackward0>)\n",
      "185 tensor(2978.6589, grad_fn=<MseLossBackward0>)\n",
      "186 tensor(2978.6348, grad_fn=<MseLossBackward0>)\n",
      "187 tensor(2978.6111, grad_fn=<MseLossBackward0>)\n",
      "188 tensor(2978.5869, grad_fn=<MseLossBackward0>)\n",
      "189 tensor(2978.5632, grad_fn=<MseLossBackward0>)\n",
      "190 tensor(2978.5391, grad_fn=<MseLossBackward0>)\n",
      "191 tensor(2978.5151, grad_fn=<MseLossBackward0>)\n",
      "192 tensor(2978.4912, grad_fn=<MseLossBackward0>)\n",
      "193 tensor(2978.4673, grad_fn=<MseLossBackward0>)\n",
      "194 tensor(2978.4434, grad_fn=<MseLossBackward0>)\n",
      "195 tensor(2978.4194, grad_fn=<MseLossBackward0>)\n",
      "196 tensor(2978.3960, grad_fn=<MseLossBackward0>)\n",
      "197 tensor(2978.3716, grad_fn=<MseLossBackward0>)\n",
      "198 tensor(2978.3481, grad_fn=<MseLossBackward0>)\n",
      "199 tensor(2978.3240, grad_fn=<MseLossBackward0>)\n",
      "200 tensor(2978.3003, grad_fn=<MseLossBackward0>)\n",
      "201 tensor(2978.2766, grad_fn=<MseLossBackward0>)\n",
      "202 tensor(2978.2524, grad_fn=<MseLossBackward0>)\n",
      "203 tensor(2978.2288, grad_fn=<MseLossBackward0>)\n",
      "204 tensor(2978.2048, grad_fn=<MseLossBackward0>)\n",
      "205 tensor(2978.1809, grad_fn=<MseLossBackward0>)\n",
      "206 tensor(2978.1575, grad_fn=<MseLossBackward0>)\n",
      "207 tensor(2978.1335, grad_fn=<MseLossBackward0>)\n",
      "208 tensor(2978.1099, grad_fn=<MseLossBackward0>)\n",
      "209 tensor(2978.0859, grad_fn=<MseLossBackward0>)\n",
      "210 tensor(2978.0623, grad_fn=<MseLossBackward0>)\n",
      "211 tensor(2978.0386, grad_fn=<MseLossBackward0>)\n",
      "212 tensor(2978.0149, grad_fn=<MseLossBackward0>)\n",
      "213 tensor(2977.9912, grad_fn=<MseLossBackward0>)\n",
      "214 tensor(2977.9673, grad_fn=<MseLossBackward0>)\n",
      "215 tensor(2977.9438, grad_fn=<MseLossBackward0>)\n",
      "216 tensor(2977.9199, grad_fn=<MseLossBackward0>)\n",
      "217 tensor(2977.8962, grad_fn=<MseLossBackward0>)\n",
      "218 tensor(2977.8726, grad_fn=<MseLossBackward0>)\n",
      "219 tensor(2977.8486, grad_fn=<MseLossBackward0>)\n",
      "220 tensor(2977.8250, grad_fn=<MseLossBackward0>)\n",
      "221 tensor(2977.8015, grad_fn=<MseLossBackward0>)\n",
      "222 tensor(2977.7778, grad_fn=<MseLossBackward0>)\n",
      "223 tensor(2977.7544, grad_fn=<MseLossBackward0>)\n",
      "224 tensor(2977.7307, grad_fn=<MseLossBackward0>)\n",
      "225 tensor(2977.7068, grad_fn=<MseLossBackward0>)\n",
      "226 tensor(2977.6833, grad_fn=<MseLossBackward0>)\n",
      "227 tensor(2977.6597, grad_fn=<MseLossBackward0>)\n",
      "228 tensor(2977.6362, grad_fn=<MseLossBackward0>)\n",
      "229 tensor(2977.6125, grad_fn=<MseLossBackward0>)\n",
      "230 tensor(2977.5891, grad_fn=<MseLossBackward0>)\n",
      "231 tensor(2977.5657, grad_fn=<MseLossBackward0>)\n",
      "232 tensor(2977.5422, grad_fn=<MseLossBackward0>)\n",
      "233 tensor(2977.5183, grad_fn=<MseLossBackward0>)\n",
      "234 tensor(2977.4951, grad_fn=<MseLossBackward0>)\n",
      "235 tensor(2977.4714, grad_fn=<MseLossBackward0>)\n",
      "236 tensor(2977.4480, grad_fn=<MseLossBackward0>)\n",
      "237 tensor(2977.4243, grad_fn=<MseLossBackward0>)\n",
      "238 tensor(2977.4006, grad_fn=<MseLossBackward0>)\n",
      "239 tensor(2977.3774, grad_fn=<MseLossBackward0>)\n",
      "240 tensor(2977.3540, grad_fn=<MseLossBackward0>)\n",
      "241 tensor(2977.3303, grad_fn=<MseLossBackward0>)\n",
      "242 tensor(2977.3071, grad_fn=<MseLossBackward0>)\n",
      "243 tensor(2977.2834, grad_fn=<MseLossBackward0>)\n",
      "244 tensor(2977.2605, grad_fn=<MseLossBackward0>)\n",
      "245 tensor(2977.2368, grad_fn=<MseLossBackward0>)\n",
      "246 tensor(2977.2136, grad_fn=<MseLossBackward0>)\n",
      "247 tensor(2977.1897, grad_fn=<MseLossBackward0>)\n",
      "248 tensor(2977.1663, grad_fn=<MseLossBackward0>)\n",
      "249 tensor(2977.1433, grad_fn=<MseLossBackward0>)\n",
      "250 tensor(2977.1201, grad_fn=<MseLossBackward0>)\n",
      "251 tensor(2977.0964, grad_fn=<MseLossBackward0>)\n",
      "252 tensor(2977.0732, grad_fn=<MseLossBackward0>)\n",
      "253 tensor(2977.0496, grad_fn=<MseLossBackward0>)\n",
      "254 tensor(2977.0264, grad_fn=<MseLossBackward0>)\n",
      "255 tensor(2977.0029, grad_fn=<MseLossBackward0>)\n",
      "256 tensor(2976.9795, grad_fn=<MseLossBackward0>)\n",
      "257 tensor(2976.9565, grad_fn=<MseLossBackward0>)\n",
      "258 tensor(2976.9333, grad_fn=<MseLossBackward0>)\n",
      "259 tensor(2976.9099, grad_fn=<MseLossBackward0>)\n",
      "260 tensor(2976.8870, grad_fn=<MseLossBackward0>)\n",
      "261 tensor(2976.8633, grad_fn=<MseLossBackward0>)\n",
      "262 tensor(2976.8401, grad_fn=<MseLossBackward0>)\n",
      "263 tensor(2976.8169, grad_fn=<MseLossBackward0>)\n",
      "264 tensor(2976.7935, grad_fn=<MseLossBackward0>)\n",
      "265 tensor(2976.7705, grad_fn=<MseLossBackward0>)\n",
      "266 tensor(2976.7471, grad_fn=<MseLossBackward0>)\n",
      "267 tensor(2976.7241, grad_fn=<MseLossBackward0>)\n",
      "268 tensor(2976.7007, grad_fn=<MseLossBackward0>)\n",
      "269 tensor(2976.6775, grad_fn=<MseLossBackward0>)\n",
      "270 tensor(2976.6545, grad_fn=<MseLossBackward0>)\n",
      "271 tensor(2976.6313, grad_fn=<MseLossBackward0>)\n",
      "272 tensor(2976.6079, grad_fn=<MseLossBackward0>)\n",
      "273 tensor(2976.5847, grad_fn=<MseLossBackward0>)\n",
      "274 tensor(2976.5618, grad_fn=<MseLossBackward0>)\n",
      "275 tensor(2976.5383, grad_fn=<MseLossBackward0>)\n",
      "276 tensor(2976.5154, grad_fn=<MseLossBackward0>)\n",
      "277 tensor(2976.4924, grad_fn=<MseLossBackward0>)\n",
      "278 tensor(2976.4695, grad_fn=<MseLossBackward0>)\n",
      "279 tensor(2976.4463, grad_fn=<MseLossBackward0>)\n",
      "280 tensor(2976.4231, grad_fn=<MseLossBackward0>)\n",
      "281 tensor(2976.3999, grad_fn=<MseLossBackward0>)\n",
      "282 tensor(2976.3770, grad_fn=<MseLossBackward0>)\n",
      "283 tensor(2976.3542, grad_fn=<MseLossBackward0>)\n",
      "284 tensor(2976.3308, grad_fn=<MseLossBackward0>)\n",
      "285 tensor(2976.3079, grad_fn=<MseLossBackward0>)\n",
      "286 tensor(2976.2849, grad_fn=<MseLossBackward0>)\n",
      "287 tensor(2976.2617, grad_fn=<MseLossBackward0>)\n",
      "288 tensor(2976.2388, grad_fn=<MseLossBackward0>)\n",
      "289 tensor(2976.2158, grad_fn=<MseLossBackward0>)\n",
      "290 tensor(2976.1926, grad_fn=<MseLossBackward0>)\n",
      "291 tensor(2976.1697, grad_fn=<MseLossBackward0>)\n",
      "292 tensor(2976.1470, grad_fn=<MseLossBackward0>)\n",
      "293 tensor(2976.1240, grad_fn=<MseLossBackward0>)\n",
      "294 tensor(2976.1011, grad_fn=<MseLossBackward0>)\n",
      "295 tensor(2976.0776, grad_fn=<MseLossBackward0>)\n",
      "296 tensor(2976.0552, grad_fn=<MseLossBackward0>)\n",
      "297 tensor(2976.0320, grad_fn=<MseLossBackward0>)\n",
      "298 tensor(2976.0095, grad_fn=<MseLossBackward0>)\n",
      "299 tensor(2975.9866, grad_fn=<MseLossBackward0>)\n",
      "300 tensor(2975.9634, grad_fn=<MseLossBackward0>)\n",
      "301 tensor(2975.9407, grad_fn=<MseLossBackward0>)\n",
      "302 tensor(2975.9177, grad_fn=<MseLossBackward0>)\n",
      "303 tensor(2975.8948, grad_fn=<MseLossBackward0>)\n",
      "304 tensor(2975.8723, grad_fn=<MseLossBackward0>)\n",
      "305 tensor(2975.8491, grad_fn=<MseLossBackward0>)\n",
      "306 tensor(2975.8264, grad_fn=<MseLossBackward0>)\n",
      "307 tensor(2975.8035, grad_fn=<MseLossBackward0>)\n",
      "308 tensor(2975.7808, grad_fn=<MseLossBackward0>)\n",
      "309 tensor(2975.7581, grad_fn=<MseLossBackward0>)\n",
      "310 tensor(2975.7356, grad_fn=<MseLossBackward0>)\n",
      "311 tensor(2975.7126, grad_fn=<MseLossBackward0>)\n",
      "312 tensor(2975.6897, grad_fn=<MseLossBackward0>)\n",
      "313 tensor(2975.6670, grad_fn=<MseLossBackward0>)\n",
      "314 tensor(2975.6445, grad_fn=<MseLossBackward0>)\n",
      "315 tensor(2975.6213, grad_fn=<MseLossBackward0>)\n",
      "316 tensor(2975.5986, grad_fn=<MseLossBackward0>)\n",
      "317 tensor(2975.5759, grad_fn=<MseLossBackward0>)\n",
      "318 tensor(2975.5535, grad_fn=<MseLossBackward0>)\n",
      "319 tensor(2975.5308, grad_fn=<MseLossBackward0>)\n",
      "320 tensor(2975.5081, grad_fn=<MseLossBackward0>)\n",
      "321 tensor(2975.4851, grad_fn=<MseLossBackward0>)\n",
      "322 tensor(2975.4626, grad_fn=<MseLossBackward0>)\n",
      "323 tensor(2975.4402, grad_fn=<MseLossBackward0>)\n",
      "324 tensor(2975.4175, grad_fn=<MseLossBackward0>)\n",
      "325 tensor(2975.3945, grad_fn=<MseLossBackward0>)\n",
      "326 tensor(2975.3721, grad_fn=<MseLossBackward0>)\n",
      "327 tensor(2975.3496, grad_fn=<MseLossBackward0>)\n",
      "328 tensor(2975.3269, grad_fn=<MseLossBackward0>)\n",
      "329 tensor(2975.3042, grad_fn=<MseLossBackward0>)\n",
      "330 tensor(2975.2815, grad_fn=<MseLossBackward0>)\n",
      "331 tensor(2975.2593, grad_fn=<MseLossBackward0>)\n",
      "332 tensor(2975.2368, grad_fn=<MseLossBackward0>)\n",
      "333 tensor(2975.2141, grad_fn=<MseLossBackward0>)\n",
      "334 tensor(2975.1914, grad_fn=<MseLossBackward0>)\n",
      "335 tensor(2975.1689, grad_fn=<MseLossBackward0>)\n",
      "336 tensor(2975.1462, grad_fn=<MseLossBackward0>)\n",
      "337 tensor(2975.1238, grad_fn=<MseLossBackward0>)\n",
      "338 tensor(2975.1013, grad_fn=<MseLossBackward0>)\n",
      "339 tensor(2975.0791, grad_fn=<MseLossBackward0>)\n",
      "340 tensor(2975.0562, grad_fn=<MseLossBackward0>)\n",
      "341 tensor(2975.0339, grad_fn=<MseLossBackward0>)\n",
      "342 tensor(2975.0112, grad_fn=<MseLossBackward0>)\n",
      "343 tensor(2974.9893, grad_fn=<MseLossBackward0>)\n",
      "344 tensor(2974.9666, grad_fn=<MseLossBackward0>)\n",
      "345 tensor(2974.9441, grad_fn=<MseLossBackward0>)\n",
      "346 tensor(2974.9216, grad_fn=<MseLossBackward0>)\n",
      "347 tensor(2974.8994, grad_fn=<MseLossBackward0>)\n",
      "348 tensor(2974.8770, grad_fn=<MseLossBackward0>)\n",
      "349 tensor(2974.8547, grad_fn=<MseLossBackward0>)\n",
      "350 tensor(2974.8323, grad_fn=<MseLossBackward0>)\n",
      "351 tensor(2974.8098, grad_fn=<MseLossBackward0>)\n",
      "352 tensor(2974.7876, grad_fn=<MseLossBackward0>)\n",
      "353 tensor(2974.7649, grad_fn=<MseLossBackward0>)\n",
      "354 tensor(2974.7432, grad_fn=<MseLossBackward0>)\n",
      "355 tensor(2974.7205, grad_fn=<MseLossBackward0>)\n",
      "356 tensor(2974.6982, grad_fn=<MseLossBackward0>)\n",
      "357 tensor(2974.6758, grad_fn=<MseLossBackward0>)\n",
      "358 tensor(2974.6536, grad_fn=<MseLossBackward0>)\n",
      "359 tensor(2974.6313, grad_fn=<MseLossBackward0>)\n",
      "360 tensor(2974.6091, grad_fn=<MseLossBackward0>)\n",
      "361 tensor(2974.5867, grad_fn=<MseLossBackward0>)\n",
      "362 tensor(2974.5642, grad_fn=<MseLossBackward0>)\n",
      "363 tensor(2974.5422, grad_fn=<MseLossBackward0>)\n",
      "364 tensor(2974.5203, grad_fn=<MseLossBackward0>)\n",
      "365 tensor(2974.4976, grad_fn=<MseLossBackward0>)\n",
      "366 tensor(2974.4753, grad_fn=<MseLossBackward0>)\n",
      "367 tensor(2974.4534, grad_fn=<MseLossBackward0>)\n",
      "368 tensor(2974.4312, grad_fn=<MseLossBackward0>)\n",
      "369 tensor(2974.4089, grad_fn=<MseLossBackward0>)\n",
      "370 tensor(2974.3865, grad_fn=<MseLossBackward0>)\n",
      "371 tensor(2974.3645, grad_fn=<MseLossBackward0>)\n",
      "372 tensor(2974.3423, grad_fn=<MseLossBackward0>)\n",
      "373 tensor(2974.3206, grad_fn=<MseLossBackward0>)\n",
      "374 tensor(2974.2981, grad_fn=<MseLossBackward0>)\n",
      "375 tensor(2974.2756, grad_fn=<MseLossBackward0>)\n",
      "376 tensor(2974.2539, grad_fn=<MseLossBackward0>)\n",
      "377 tensor(2974.2317, grad_fn=<MseLossBackward0>)\n",
      "378 tensor(2974.2100, grad_fn=<MseLossBackward0>)\n",
      "379 tensor(2974.1877, grad_fn=<MseLossBackward0>)\n",
      "380 tensor(2974.1655, grad_fn=<MseLossBackward0>)\n",
      "381 tensor(2974.1436, grad_fn=<MseLossBackward0>)\n",
      "382 tensor(2974.1213, grad_fn=<MseLossBackward0>)\n",
      "383 tensor(2974.0994, grad_fn=<MseLossBackward0>)\n",
      "384 tensor(2974.0771, grad_fn=<MseLossBackward0>)\n",
      "385 tensor(2974.0554, grad_fn=<MseLossBackward0>)\n",
      "386 tensor(2974.0334, grad_fn=<MseLossBackward0>)\n",
      "387 tensor(2974.0115, grad_fn=<MseLossBackward0>)\n",
      "388 tensor(2973.9893, grad_fn=<MseLossBackward0>)\n",
      "389 tensor(2973.9673, grad_fn=<MseLossBackward0>)\n",
      "390 tensor(2973.9456, grad_fn=<MseLossBackward0>)\n",
      "391 tensor(2973.9233, grad_fn=<MseLossBackward0>)\n",
      "392 tensor(2973.9014, grad_fn=<MseLossBackward0>)\n",
      "393 tensor(2973.8796, grad_fn=<MseLossBackward0>)\n",
      "394 tensor(2973.8577, grad_fn=<MseLossBackward0>)\n",
      "395 tensor(2973.8357, grad_fn=<MseLossBackward0>)\n",
      "396 tensor(2973.8140, grad_fn=<MseLossBackward0>)\n",
      "397 tensor(2973.7917, grad_fn=<MseLossBackward0>)\n",
      "398 tensor(2973.7700, grad_fn=<MseLossBackward0>)\n",
      "399 tensor(2973.7480, grad_fn=<MseLossBackward0>)\n",
      "400 tensor(2973.7263, grad_fn=<MseLossBackward0>)\n",
      "401 tensor(2973.7046, grad_fn=<MseLossBackward0>)\n",
      "402 tensor(2973.6826, grad_fn=<MseLossBackward0>)\n",
      "403 tensor(2973.6606, grad_fn=<MseLossBackward0>)\n",
      "404 tensor(2973.6392, grad_fn=<MseLossBackward0>)\n",
      "405 tensor(2973.6172, grad_fn=<MseLossBackward0>)\n",
      "406 tensor(2973.5952, grad_fn=<MseLossBackward0>)\n",
      "407 tensor(2973.5735, grad_fn=<MseLossBackward0>)\n",
      "408 tensor(2973.5518, grad_fn=<MseLossBackward0>)\n",
      "409 tensor(2973.5303, grad_fn=<MseLossBackward0>)\n",
      "410 tensor(2973.5083, grad_fn=<MseLossBackward0>)\n",
      "411 tensor(2973.4866, grad_fn=<MseLossBackward0>)\n",
      "412 tensor(2973.4651, grad_fn=<MseLossBackward0>)\n",
      "413 tensor(2973.4431, grad_fn=<MseLossBackward0>)\n",
      "414 tensor(2973.4211, grad_fn=<MseLossBackward0>)\n",
      "415 tensor(2973.3997, grad_fn=<MseLossBackward0>)\n",
      "416 tensor(2973.3782, grad_fn=<MseLossBackward0>)\n",
      "417 tensor(2973.3560, grad_fn=<MseLossBackward0>)\n",
      "418 tensor(2973.3345, grad_fn=<MseLossBackward0>)\n",
      "419 tensor(2973.3130, grad_fn=<MseLossBackward0>)\n",
      "420 tensor(2973.2913, grad_fn=<MseLossBackward0>)\n",
      "421 tensor(2973.2695, grad_fn=<MseLossBackward0>)\n",
      "422 tensor(2973.2480, grad_fn=<MseLossBackward0>)\n",
      "423 tensor(2973.2263, grad_fn=<MseLossBackward0>)\n",
      "424 tensor(2973.2051, grad_fn=<MseLossBackward0>)\n",
      "425 tensor(2973.1833, grad_fn=<MseLossBackward0>)\n",
      "426 tensor(2973.1616, grad_fn=<MseLossBackward0>)\n",
      "427 tensor(2973.1399, grad_fn=<MseLossBackward0>)\n",
      "428 tensor(2973.1184, grad_fn=<MseLossBackward0>)\n",
      "429 tensor(2973.0969, grad_fn=<MseLossBackward0>)\n",
      "430 tensor(2973.0754, grad_fn=<MseLossBackward0>)\n",
      "431 tensor(2973.0540, grad_fn=<MseLossBackward0>)\n",
      "432 tensor(2973.0322, grad_fn=<MseLossBackward0>)\n",
      "433 tensor(2973.0105, grad_fn=<MseLossBackward0>)\n",
      "434 tensor(2972.9890, grad_fn=<MseLossBackward0>)\n",
      "435 tensor(2972.9678, grad_fn=<MseLossBackward0>)\n",
      "436 tensor(2972.9463, grad_fn=<MseLossBackward0>)\n",
      "437 tensor(2972.9248, grad_fn=<MseLossBackward0>)\n",
      "438 tensor(2972.9031, grad_fn=<MseLossBackward0>)\n",
      "439 tensor(2972.8816, grad_fn=<MseLossBackward0>)\n",
      "440 tensor(2972.8601, grad_fn=<MseLossBackward0>)\n",
      "441 tensor(2972.8389, grad_fn=<MseLossBackward0>)\n",
      "442 tensor(2972.8179, grad_fn=<MseLossBackward0>)\n",
      "443 tensor(2972.7961, grad_fn=<MseLossBackward0>)\n",
      "444 tensor(2972.7747, grad_fn=<MseLossBackward0>)\n",
      "445 tensor(2972.7534, grad_fn=<MseLossBackward0>)\n",
      "446 tensor(2972.7319, grad_fn=<MseLossBackward0>)\n",
      "447 tensor(2972.7104, grad_fn=<MseLossBackward0>)\n",
      "448 tensor(2972.6890, grad_fn=<MseLossBackward0>)\n",
      "449 tensor(2972.6682, grad_fn=<MseLossBackward0>)\n",
      "450 tensor(2972.6465, grad_fn=<MseLossBackward0>)\n",
      "451 tensor(2972.6252, grad_fn=<MseLossBackward0>)\n",
      "452 tensor(2972.6038, grad_fn=<MseLossBackward0>)\n",
      "453 tensor(2972.5825, grad_fn=<MseLossBackward0>)\n",
      "454 tensor(2972.5613, grad_fn=<MseLossBackward0>)\n",
      "455 tensor(2972.5403, grad_fn=<MseLossBackward0>)\n",
      "456 tensor(2972.5188, grad_fn=<MseLossBackward0>)\n",
      "457 tensor(2972.4976, grad_fn=<MseLossBackward0>)\n",
      "458 tensor(2972.4766, grad_fn=<MseLossBackward0>)\n",
      "459 tensor(2972.4551, grad_fn=<MseLossBackward0>)\n",
      "460 tensor(2972.4336, grad_fn=<MseLossBackward0>)\n",
      "461 tensor(2972.4124, grad_fn=<MseLossBackward0>)\n",
      "462 tensor(2972.3914, grad_fn=<MseLossBackward0>)\n",
      "463 tensor(2972.3701, grad_fn=<MseLossBackward0>)\n",
      "464 tensor(2972.3486, grad_fn=<MseLossBackward0>)\n",
      "465 tensor(2972.3281, grad_fn=<MseLossBackward0>)\n",
      "466 tensor(2972.3066, grad_fn=<MseLossBackward0>)\n",
      "467 tensor(2972.2854, grad_fn=<MseLossBackward0>)\n",
      "468 tensor(2972.2644, grad_fn=<MseLossBackward0>)\n",
      "469 tensor(2972.2432, grad_fn=<MseLossBackward0>)\n",
      "470 tensor(2972.2222, grad_fn=<MseLossBackward0>)\n",
      "471 tensor(2972.2009, grad_fn=<MseLossBackward0>)\n",
      "472 tensor(2972.1799, grad_fn=<MseLossBackward0>)\n",
      "473 tensor(2972.1589, grad_fn=<MseLossBackward0>)\n",
      "474 tensor(2972.1377, grad_fn=<MseLossBackward0>)\n",
      "475 tensor(2972.1165, grad_fn=<MseLossBackward0>)\n",
      "476 tensor(2972.0955, grad_fn=<MseLossBackward0>)\n",
      "477 tensor(2972.0747, grad_fn=<MseLossBackward0>)\n",
      "478 tensor(2972.0535, grad_fn=<MseLossBackward0>)\n",
      "479 tensor(2972.0327, grad_fn=<MseLossBackward0>)\n",
      "480 tensor(2972.0115, grad_fn=<MseLossBackward0>)\n",
      "481 tensor(2971.9905, grad_fn=<MseLossBackward0>)\n",
      "482 tensor(2971.9692, grad_fn=<MseLossBackward0>)\n",
      "483 tensor(2971.9485, grad_fn=<MseLossBackward0>)\n",
      "484 tensor(2971.9275, grad_fn=<MseLossBackward0>)\n",
      "485 tensor(2971.9067, grad_fn=<MseLossBackward0>)\n",
      "486 tensor(2971.8857, grad_fn=<MseLossBackward0>)\n",
      "487 tensor(2971.8647, grad_fn=<MseLossBackward0>)\n",
      "488 tensor(2971.8440, grad_fn=<MseLossBackward0>)\n",
      "489 tensor(2971.8228, grad_fn=<MseLossBackward0>)\n",
      "490 tensor(2971.8020, grad_fn=<MseLossBackward0>)\n",
      "491 tensor(2971.7812, grad_fn=<MseLossBackward0>)\n",
      "492 tensor(2971.7600, grad_fn=<MseLossBackward0>)\n",
      "493 tensor(2971.7393, grad_fn=<MseLossBackward0>)\n",
      "494 tensor(2971.7185, grad_fn=<MseLossBackward0>)\n",
      "495 tensor(2971.6975, grad_fn=<MseLossBackward0>)\n",
      "496 tensor(2971.6768, grad_fn=<MseLossBackward0>)\n",
      "497 tensor(2971.6560, grad_fn=<MseLossBackward0>)\n",
      "498 tensor(2971.6350, grad_fn=<MseLossBackward0>)\n",
      "499 tensor(2971.6143, grad_fn=<MseLossBackward0>)\n",
      "500 tensor(2971.5935, grad_fn=<MseLossBackward0>)\n",
      "501 tensor(2971.5725, grad_fn=<MseLossBackward0>)\n",
      "502 tensor(2971.5520, grad_fn=<MseLossBackward0>)\n",
      "503 tensor(2971.5312, grad_fn=<MseLossBackward0>)\n",
      "504 tensor(2971.5105, grad_fn=<MseLossBackward0>)\n",
      "505 tensor(2971.4895, grad_fn=<MseLossBackward0>)\n",
      "506 tensor(2971.4688, grad_fn=<MseLossBackward0>)\n",
      "507 tensor(2971.4480, grad_fn=<MseLossBackward0>)\n",
      "508 tensor(2971.4275, grad_fn=<MseLossBackward0>)\n",
      "509 tensor(2971.4067, grad_fn=<MseLossBackward0>)\n",
      "510 tensor(2971.3865, grad_fn=<MseLossBackward0>)\n",
      "511 tensor(2971.3657, grad_fn=<MseLossBackward0>)\n",
      "512 tensor(2971.3450, grad_fn=<MseLossBackward0>)\n",
      "513 tensor(2971.3240, grad_fn=<MseLossBackward0>)\n",
      "514 tensor(2971.3037, grad_fn=<MseLossBackward0>)\n",
      "515 tensor(2971.2827, grad_fn=<MseLossBackward0>)\n",
      "516 tensor(2971.2625, grad_fn=<MseLossBackward0>)\n",
      "517 tensor(2971.2417, grad_fn=<MseLossBackward0>)\n",
      "518 tensor(2971.2212, grad_fn=<MseLossBackward0>)\n",
      "519 tensor(2971.2004, grad_fn=<MseLossBackward0>)\n",
      "520 tensor(2971.1799, grad_fn=<MseLossBackward0>)\n",
      "521 tensor(2971.1594, grad_fn=<MseLossBackward0>)\n",
      "522 tensor(2971.1387, grad_fn=<MseLossBackward0>)\n",
      "523 tensor(2971.1184, grad_fn=<MseLossBackward0>)\n",
      "524 tensor(2971.0977, grad_fn=<MseLossBackward0>)\n",
      "525 tensor(2971.0771, grad_fn=<MseLossBackward0>)\n",
      "526 tensor(2971.0566, grad_fn=<MseLossBackward0>)\n",
      "527 tensor(2971.0364, grad_fn=<MseLossBackward0>)\n",
      "528 tensor(2971.0156, grad_fn=<MseLossBackward0>)\n",
      "529 tensor(2970.9954, grad_fn=<MseLossBackward0>)\n",
      "530 tensor(2970.9749, grad_fn=<MseLossBackward0>)\n",
      "531 tensor(2970.9543, grad_fn=<MseLossBackward0>)\n",
      "532 tensor(2970.9338, grad_fn=<MseLossBackward0>)\n",
      "533 tensor(2970.9133, grad_fn=<MseLossBackward0>)\n",
      "534 tensor(2970.8928, grad_fn=<MseLossBackward0>)\n",
      "535 tensor(2970.8726, grad_fn=<MseLossBackward0>)\n",
      "536 tensor(2970.8518, grad_fn=<MseLossBackward0>)\n",
      "537 tensor(2970.8320, grad_fn=<MseLossBackward0>)\n",
      "538 tensor(2970.8115, grad_fn=<MseLossBackward0>)\n",
      "539 tensor(2970.7910, grad_fn=<MseLossBackward0>)\n",
      "540 tensor(2970.7705, grad_fn=<MseLossBackward0>)\n",
      "541 tensor(2970.7500, grad_fn=<MseLossBackward0>)\n",
      "542 tensor(2970.7300, grad_fn=<MseLossBackward0>)\n",
      "543 tensor(2970.7097, grad_fn=<MseLossBackward0>)\n",
      "544 tensor(2970.6890, grad_fn=<MseLossBackward0>)\n",
      "545 tensor(2970.6687, grad_fn=<MseLossBackward0>)\n",
      "546 tensor(2970.6487, grad_fn=<MseLossBackward0>)\n",
      "547 tensor(2970.6287, grad_fn=<MseLossBackward0>)\n",
      "548 tensor(2970.6082, grad_fn=<MseLossBackward0>)\n",
      "549 tensor(2970.5876, grad_fn=<MseLossBackward0>)\n",
      "550 tensor(2970.5676, grad_fn=<MseLossBackward0>)\n",
      "551 tensor(2970.5476, grad_fn=<MseLossBackward0>)\n",
      "552 tensor(2970.5271, grad_fn=<MseLossBackward0>)\n",
      "553 tensor(2970.5068, grad_fn=<MseLossBackward0>)\n",
      "554 tensor(2970.4866, grad_fn=<MseLossBackward0>)\n",
      "555 tensor(2970.4666, grad_fn=<MseLossBackward0>)\n",
      "556 tensor(2970.4463, grad_fn=<MseLossBackward0>)\n",
      "557 tensor(2970.4260, grad_fn=<MseLossBackward0>)\n",
      "558 tensor(2970.4060, grad_fn=<MseLossBackward0>)\n",
      "559 tensor(2970.3860, grad_fn=<MseLossBackward0>)\n",
      "560 tensor(2970.3657, grad_fn=<MseLossBackward0>)\n",
      "561 tensor(2970.3455, grad_fn=<MseLossBackward0>)\n",
      "562 tensor(2970.3257, grad_fn=<MseLossBackward0>)\n",
      "563 tensor(2970.3052, grad_fn=<MseLossBackward0>)\n",
      "564 tensor(2970.2854, grad_fn=<MseLossBackward0>)\n",
      "565 tensor(2970.2649, grad_fn=<MseLossBackward0>)\n",
      "566 tensor(2970.2451, grad_fn=<MseLossBackward0>)\n",
      "567 tensor(2970.2249, grad_fn=<MseLossBackward0>)\n",
      "568 tensor(2970.2051, grad_fn=<MseLossBackward0>)\n",
      "569 tensor(2970.1851, grad_fn=<MseLossBackward0>)\n",
      "570 tensor(2970.1650, grad_fn=<MseLossBackward0>)\n",
      "571 tensor(2970.1448, grad_fn=<MseLossBackward0>)\n",
      "572 tensor(2970.1250, grad_fn=<MseLossBackward0>)\n",
      "573 tensor(2970.1047, grad_fn=<MseLossBackward0>)\n",
      "574 tensor(2970.0850, grad_fn=<MseLossBackward0>)\n",
      "575 tensor(2970.0649, grad_fn=<MseLossBackward0>)\n",
      "576 tensor(2970.0449, grad_fn=<MseLossBackward0>)\n",
      "577 tensor(2970.0251, grad_fn=<MseLossBackward0>)\n",
      "578 tensor(2970.0049, grad_fn=<MseLossBackward0>)\n",
      "579 tensor(2969.9851, grad_fn=<MseLossBackward0>)\n",
      "580 tensor(2969.9653, grad_fn=<MseLossBackward0>)\n",
      "581 tensor(2969.9453, grad_fn=<MseLossBackward0>)\n",
      "582 tensor(2969.9255, grad_fn=<MseLossBackward0>)\n",
      "583 tensor(2969.9055, grad_fn=<MseLossBackward0>)\n",
      "584 tensor(2969.8857, grad_fn=<MseLossBackward0>)\n",
      "585 tensor(2969.8660, grad_fn=<MseLossBackward0>)\n",
      "586 tensor(2969.8459, grad_fn=<MseLossBackward0>)\n",
      "587 tensor(2969.8262, grad_fn=<MseLossBackward0>)\n",
      "588 tensor(2969.8064, grad_fn=<MseLossBackward0>)\n",
      "589 tensor(2969.7864, grad_fn=<MseLossBackward0>)\n",
      "590 tensor(2969.7668, grad_fn=<MseLossBackward0>)\n",
      "591 tensor(2969.7468, grad_fn=<MseLossBackward0>)\n",
      "592 tensor(2969.7273, grad_fn=<MseLossBackward0>)\n",
      "593 tensor(2969.7075, grad_fn=<MseLossBackward0>)\n",
      "594 tensor(2969.6877, grad_fn=<MseLossBackward0>)\n",
      "595 tensor(2969.6677, grad_fn=<MseLossBackward0>)\n",
      "596 tensor(2969.6479, grad_fn=<MseLossBackward0>)\n",
      "597 tensor(2969.6284, grad_fn=<MseLossBackward0>)\n",
      "598 tensor(2969.6089, grad_fn=<MseLossBackward0>)\n",
      "599 tensor(2969.5891, grad_fn=<MseLossBackward0>)\n",
      "600 tensor(2969.5696, grad_fn=<MseLossBackward0>)\n",
      "601 tensor(2969.5496, grad_fn=<MseLossBackward0>)\n",
      "602 tensor(2969.5300, grad_fn=<MseLossBackward0>)\n",
      "603 tensor(2969.5103, grad_fn=<MseLossBackward0>)\n",
      "604 tensor(2969.4907, grad_fn=<MseLossBackward0>)\n",
      "605 tensor(2969.4709, grad_fn=<MseLossBackward0>)\n",
      "606 tensor(2969.4514, grad_fn=<MseLossBackward0>)\n",
      "607 tensor(2969.4316, grad_fn=<MseLossBackward0>)\n",
      "608 tensor(2969.4121, grad_fn=<MseLossBackward0>)\n",
      "609 tensor(2969.3928, grad_fn=<MseLossBackward0>)\n",
      "610 tensor(2969.3733, grad_fn=<MseLossBackward0>)\n",
      "611 tensor(2969.3535, grad_fn=<MseLossBackward0>)\n",
      "612 tensor(2969.3340, grad_fn=<MseLossBackward0>)\n",
      "613 tensor(2969.3145, grad_fn=<MseLossBackward0>)\n",
      "614 tensor(2969.2949, grad_fn=<MseLossBackward0>)\n",
      "615 tensor(2969.2754, grad_fn=<MseLossBackward0>)\n",
      "616 tensor(2969.2559, grad_fn=<MseLossBackward0>)\n",
      "617 tensor(2969.2363, grad_fn=<MseLossBackward0>)\n",
      "618 tensor(2969.2168, grad_fn=<MseLossBackward0>)\n",
      "619 tensor(2969.1975, grad_fn=<MseLossBackward0>)\n",
      "620 tensor(2969.1777, grad_fn=<MseLossBackward0>)\n",
      "621 tensor(2969.1584, grad_fn=<MseLossBackward0>)\n",
      "622 tensor(2969.1392, grad_fn=<MseLossBackward0>)\n",
      "623 tensor(2969.1199, grad_fn=<MseLossBackward0>)\n",
      "624 tensor(2969.1003, grad_fn=<MseLossBackward0>)\n",
      "625 tensor(2969.0808, grad_fn=<MseLossBackward0>)\n",
      "626 tensor(2969.0613, grad_fn=<MseLossBackward0>)\n",
      "627 tensor(2969.0417, grad_fn=<MseLossBackward0>)\n",
      "628 tensor(2969.0227, grad_fn=<MseLossBackward0>)\n",
      "629 tensor(2969.0034, grad_fn=<MseLossBackward0>)\n",
      "630 tensor(2968.9839, grad_fn=<MseLossBackward0>)\n",
      "631 tensor(2968.9644, grad_fn=<MseLossBackward0>)\n",
      "632 tensor(2968.9451, grad_fn=<MseLossBackward0>)\n",
      "633 tensor(2968.9260, grad_fn=<MseLossBackward0>)\n",
      "634 tensor(2968.9062, grad_fn=<MseLossBackward0>)\n",
      "635 tensor(2968.8872, grad_fn=<MseLossBackward0>)\n",
      "636 tensor(2968.8682, grad_fn=<MseLossBackward0>)\n",
      "637 tensor(2968.8486, grad_fn=<MseLossBackward0>)\n",
      "638 tensor(2968.8296, grad_fn=<MseLossBackward0>)\n",
      "639 tensor(2968.8101, grad_fn=<MseLossBackward0>)\n",
      "640 tensor(2968.7913, grad_fn=<MseLossBackward0>)\n",
      "641 tensor(2968.7717, grad_fn=<MseLossBackward0>)\n",
      "642 tensor(2968.7524, grad_fn=<MseLossBackward0>)\n",
      "643 tensor(2968.7334, grad_fn=<MseLossBackward0>)\n",
      "644 tensor(2968.7141, grad_fn=<MseLossBackward0>)\n",
      "645 tensor(2968.6946, grad_fn=<MseLossBackward0>)\n",
      "646 tensor(2968.6760, grad_fn=<MseLossBackward0>)\n",
      "647 tensor(2968.6567, grad_fn=<MseLossBackward0>)\n",
      "648 tensor(2968.6375, grad_fn=<MseLossBackward0>)\n",
      "649 tensor(2968.6184, grad_fn=<MseLossBackward0>)\n",
      "650 tensor(2968.5991, grad_fn=<MseLossBackward0>)\n",
      "651 tensor(2968.5798, grad_fn=<MseLossBackward0>)\n",
      "652 tensor(2968.5608, grad_fn=<MseLossBackward0>)\n",
      "653 tensor(2968.5420, grad_fn=<MseLossBackward0>)\n",
      "654 tensor(2968.5227, grad_fn=<MseLossBackward0>)\n",
      "655 tensor(2968.5037, grad_fn=<MseLossBackward0>)\n",
      "656 tensor(2968.4844, grad_fn=<MseLossBackward0>)\n",
      "657 tensor(2968.4658, grad_fn=<MseLossBackward0>)\n",
      "658 tensor(2968.4465, grad_fn=<MseLossBackward0>)\n",
      "659 tensor(2968.4277, grad_fn=<MseLossBackward0>)\n",
      "660 tensor(2968.4087, grad_fn=<MseLossBackward0>)\n",
      "661 tensor(2968.3896, grad_fn=<MseLossBackward0>)\n",
      "662 tensor(2968.3706, grad_fn=<MseLossBackward0>)\n",
      "663 tensor(2968.3516, grad_fn=<MseLossBackward0>)\n",
      "664 tensor(2968.3325, grad_fn=<MseLossBackward0>)\n",
      "665 tensor(2968.3137, grad_fn=<MseLossBackward0>)\n",
      "666 tensor(2968.2949, grad_fn=<MseLossBackward0>)\n",
      "667 tensor(2968.2761, grad_fn=<MseLossBackward0>)\n",
      "668 tensor(2968.2568, grad_fn=<MseLossBackward0>)\n",
      "669 tensor(2968.2380, grad_fn=<MseLossBackward0>)\n",
      "670 tensor(2968.2192, grad_fn=<MseLossBackward0>)\n",
      "671 tensor(2968.2004, grad_fn=<MseLossBackward0>)\n",
      "672 tensor(2968.1814, grad_fn=<MseLossBackward0>)\n",
      "673 tensor(2968.1624, grad_fn=<MseLossBackward0>)\n",
      "674 tensor(2968.1438, grad_fn=<MseLossBackward0>)\n",
      "675 tensor(2968.1248, grad_fn=<MseLossBackward0>)\n",
      "676 tensor(2968.1062, grad_fn=<MseLossBackward0>)\n",
      "677 tensor(2968.0872, grad_fn=<MseLossBackward0>)\n",
      "678 tensor(2968.0686, grad_fn=<MseLossBackward0>)\n",
      "679 tensor(2968.0493, grad_fn=<MseLossBackward0>)\n",
      "680 tensor(2968.0308, grad_fn=<MseLossBackward0>)\n",
      "681 tensor(2968.0120, grad_fn=<MseLossBackward0>)\n",
      "682 tensor(2967.9934, grad_fn=<MseLossBackward0>)\n",
      "683 tensor(2967.9741, grad_fn=<MseLossBackward0>)\n",
      "684 tensor(2967.9558, grad_fn=<MseLossBackward0>)\n",
      "685 tensor(2967.9370, grad_fn=<MseLossBackward0>)\n",
      "686 tensor(2967.9185, grad_fn=<MseLossBackward0>)\n",
      "687 tensor(2967.8997, grad_fn=<MseLossBackward0>)\n",
      "688 tensor(2967.8811, grad_fn=<MseLossBackward0>)\n",
      "689 tensor(2967.8623, grad_fn=<MseLossBackward0>)\n",
      "690 tensor(2967.8433, grad_fn=<MseLossBackward0>)\n",
      "691 tensor(2967.8250, grad_fn=<MseLossBackward0>)\n",
      "692 tensor(2967.8062, grad_fn=<MseLossBackward0>)\n",
      "693 tensor(2967.7878, grad_fn=<MseLossBackward0>)\n",
      "694 tensor(2967.7688, grad_fn=<MseLossBackward0>)\n",
      "695 tensor(2967.7505, grad_fn=<MseLossBackward0>)\n",
      "696 tensor(2967.7322, grad_fn=<MseLossBackward0>)\n",
      "697 tensor(2967.7131, grad_fn=<MseLossBackward0>)\n",
      "698 tensor(2967.6943, grad_fn=<MseLossBackward0>)\n",
      "699 tensor(2967.6760, grad_fn=<MseLossBackward0>)\n",
      "700 tensor(2967.6577, grad_fn=<MseLossBackward0>)\n",
      "701 tensor(2967.6387, grad_fn=<MseLossBackward0>)\n",
      "702 tensor(2967.6204, grad_fn=<MseLossBackward0>)\n",
      "703 tensor(2967.6021, grad_fn=<MseLossBackward0>)\n",
      "704 tensor(2967.5830, grad_fn=<MseLossBackward0>)\n",
      "705 tensor(2967.5647, grad_fn=<MseLossBackward0>)\n",
      "706 tensor(2967.5464, grad_fn=<MseLossBackward0>)\n",
      "707 tensor(2967.5281, grad_fn=<MseLossBackward0>)\n",
      "708 tensor(2967.5098, grad_fn=<MseLossBackward0>)\n",
      "709 tensor(2967.4910, grad_fn=<MseLossBackward0>)\n",
      "710 tensor(2967.4724, grad_fn=<MseLossBackward0>)\n",
      "711 tensor(2967.4541, grad_fn=<MseLossBackward0>)\n",
      "712 tensor(2967.4358, grad_fn=<MseLossBackward0>)\n",
      "713 tensor(2967.4175, grad_fn=<MseLossBackward0>)\n",
      "714 tensor(2967.3992, grad_fn=<MseLossBackward0>)\n",
      "715 tensor(2967.3806, grad_fn=<MseLossBackward0>)\n",
      "716 tensor(2967.3621, grad_fn=<MseLossBackward0>)\n",
      "717 tensor(2967.3438, grad_fn=<MseLossBackward0>)\n",
      "718 tensor(2967.3257, grad_fn=<MseLossBackward0>)\n",
      "719 tensor(2967.3071, grad_fn=<MseLossBackward0>)\n",
      "720 tensor(2967.2888, grad_fn=<MseLossBackward0>)\n",
      "721 tensor(2967.2705, grad_fn=<MseLossBackward0>)\n",
      "722 tensor(2967.2522, grad_fn=<MseLossBackward0>)\n",
      "723 tensor(2967.2339, grad_fn=<MseLossBackward0>)\n",
      "724 tensor(2967.2158, grad_fn=<MseLossBackward0>)\n",
      "725 tensor(2967.1975, grad_fn=<MseLossBackward0>)\n",
      "726 tensor(2967.1792, grad_fn=<MseLossBackward0>)\n",
      "727 tensor(2967.1606, grad_fn=<MseLossBackward0>)\n",
      "728 tensor(2967.1426, grad_fn=<MseLossBackward0>)\n",
      "729 tensor(2967.1245, grad_fn=<MseLossBackward0>)\n",
      "730 tensor(2967.1060, grad_fn=<MseLossBackward0>)\n",
      "731 tensor(2967.0881, grad_fn=<MseLossBackward0>)\n",
      "732 tensor(2967.0698, grad_fn=<MseLossBackward0>)\n",
      "733 tensor(2967.0515, grad_fn=<MseLossBackward0>)\n",
      "734 tensor(2967.0334, grad_fn=<MseLossBackward0>)\n",
      "735 tensor(2967.0154, grad_fn=<MseLossBackward0>)\n",
      "736 tensor(2966.9973, grad_fn=<MseLossBackward0>)\n",
      "737 tensor(2966.9790, grad_fn=<MseLossBackward0>)\n",
      "738 tensor(2966.9607, grad_fn=<MseLossBackward0>)\n",
      "739 tensor(2966.9426, grad_fn=<MseLossBackward0>)\n",
      "740 tensor(2966.9248, grad_fn=<MseLossBackward0>)\n",
      "741 tensor(2966.9065, grad_fn=<MseLossBackward0>)\n",
      "742 tensor(2966.8887, grad_fn=<MseLossBackward0>)\n",
      "743 tensor(2966.8706, grad_fn=<MseLossBackward0>)\n",
      "744 tensor(2966.8525, grad_fn=<MseLossBackward0>)\n",
      "745 tensor(2966.8345, grad_fn=<MseLossBackward0>)\n",
      "746 tensor(2966.8164, grad_fn=<MseLossBackward0>)\n",
      "747 tensor(2966.7986, grad_fn=<MseLossBackward0>)\n",
      "748 tensor(2966.7803, grad_fn=<MseLossBackward0>)\n",
      "749 tensor(2966.7625, grad_fn=<MseLossBackward0>)\n",
      "750 tensor(2966.7446, grad_fn=<MseLossBackward0>)\n",
      "751 tensor(2966.7266, grad_fn=<MseLossBackward0>)\n",
      "752 tensor(2966.7087, grad_fn=<MseLossBackward0>)\n",
      "753 tensor(2966.6904, grad_fn=<MseLossBackward0>)\n",
      "754 tensor(2966.6726, grad_fn=<MseLossBackward0>)\n",
      "755 tensor(2966.6548, grad_fn=<MseLossBackward0>)\n",
      "756 tensor(2966.6367, grad_fn=<MseLossBackward0>)\n",
      "757 tensor(2966.6189, grad_fn=<MseLossBackward0>)\n",
      "758 tensor(2966.6011, grad_fn=<MseLossBackward0>)\n",
      "759 tensor(2966.5835, grad_fn=<MseLossBackward0>)\n",
      "760 tensor(2966.5657, grad_fn=<MseLossBackward0>)\n",
      "761 tensor(2966.5476, grad_fn=<MseLossBackward0>)\n",
      "762 tensor(2966.5298, grad_fn=<MseLossBackward0>)\n",
      "763 tensor(2966.5117, grad_fn=<MseLossBackward0>)\n",
      "764 tensor(2966.4941, grad_fn=<MseLossBackward0>)\n",
      "765 tensor(2966.4761, grad_fn=<MseLossBackward0>)\n",
      "766 tensor(2966.4585, grad_fn=<MseLossBackward0>)\n",
      "767 tensor(2966.4407, grad_fn=<MseLossBackward0>)\n",
      "768 tensor(2966.4226, grad_fn=<MseLossBackward0>)\n",
      "769 tensor(2966.4055, grad_fn=<MseLossBackward0>)\n",
      "770 tensor(2966.3875, grad_fn=<MseLossBackward0>)\n",
      "771 tensor(2966.3696, grad_fn=<MseLossBackward0>)\n",
      "772 tensor(2966.3518, grad_fn=<MseLossBackward0>)\n",
      "773 tensor(2966.3345, grad_fn=<MseLossBackward0>)\n",
      "774 tensor(2966.3167, grad_fn=<MseLossBackward0>)\n",
      "775 tensor(2966.2988, grad_fn=<MseLossBackward0>)\n",
      "776 tensor(2966.2812, grad_fn=<MseLossBackward0>)\n",
      "777 tensor(2966.2637, grad_fn=<MseLossBackward0>)\n",
      "778 tensor(2966.2461, grad_fn=<MseLossBackward0>)\n",
      "779 tensor(2966.2285, grad_fn=<MseLossBackward0>)\n",
      "780 tensor(2966.2104, grad_fn=<MseLossBackward0>)\n",
      "781 tensor(2966.1931, grad_fn=<MseLossBackward0>)\n",
      "782 tensor(2966.1753, grad_fn=<MseLossBackward0>)\n",
      "783 tensor(2966.1580, grad_fn=<MseLossBackward0>)\n",
      "784 tensor(2966.1404, grad_fn=<MseLossBackward0>)\n",
      "785 tensor(2966.1228, grad_fn=<MseLossBackward0>)\n",
      "786 tensor(2966.1052, grad_fn=<MseLossBackward0>)\n",
      "787 tensor(2966.0874, grad_fn=<MseLossBackward0>)\n",
      "788 tensor(2966.0698, grad_fn=<MseLossBackward0>)\n",
      "789 tensor(2966.0527, grad_fn=<MseLossBackward0>)\n",
      "790 tensor(2966.0349, grad_fn=<MseLossBackward0>)\n",
      "791 tensor(2966.0176, grad_fn=<MseLossBackward0>)\n",
      "792 tensor(2966., grad_fn=<MseLossBackward0>)\n",
      "793 tensor(2965.9824, grad_fn=<MseLossBackward0>)\n",
      "794 tensor(2965.9651, grad_fn=<MseLossBackward0>)\n",
      "795 tensor(2965.9475, grad_fn=<MseLossBackward0>)\n",
      "796 tensor(2965.9302, grad_fn=<MseLossBackward0>)\n",
      "797 tensor(2965.9128, grad_fn=<MseLossBackward0>)\n",
      "798 tensor(2965.8955, grad_fn=<MseLossBackward0>)\n",
      "799 tensor(2965.8777, grad_fn=<MseLossBackward0>)\n",
      "800 tensor(2965.8606, grad_fn=<MseLossBackward0>)\n",
      "801 tensor(2965.8433, grad_fn=<MseLossBackward0>)\n",
      "802 tensor(2965.8259, grad_fn=<MseLossBackward0>)\n",
      "803 tensor(2965.8083, grad_fn=<MseLossBackward0>)\n",
      "804 tensor(2965.7910, grad_fn=<MseLossBackward0>)\n",
      "805 tensor(2965.7737, grad_fn=<MseLossBackward0>)\n",
      "806 tensor(2965.7563, grad_fn=<MseLossBackward0>)\n",
      "807 tensor(2965.7393, grad_fn=<MseLossBackward0>)\n",
      "808 tensor(2965.7217, grad_fn=<MseLossBackward0>)\n",
      "809 tensor(2965.7043, grad_fn=<MseLossBackward0>)\n",
      "810 tensor(2965.6873, grad_fn=<MseLossBackward0>)\n",
      "811 tensor(2965.6702, grad_fn=<MseLossBackward0>)\n",
      "812 tensor(2965.6528, grad_fn=<MseLossBackward0>)\n",
      "813 tensor(2965.6353, grad_fn=<MseLossBackward0>)\n",
      "814 tensor(2965.6184, grad_fn=<MseLossBackward0>)\n",
      "815 tensor(2965.6011, grad_fn=<MseLossBackward0>)\n",
      "816 tensor(2965.5840, grad_fn=<MseLossBackward0>)\n",
      "817 tensor(2965.5669, grad_fn=<MseLossBackward0>)\n",
      "818 tensor(2965.5496, grad_fn=<MseLossBackward0>)\n",
      "819 tensor(2965.5320, grad_fn=<MseLossBackward0>)\n",
      "820 tensor(2965.5151, grad_fn=<MseLossBackward0>)\n",
      "821 tensor(2965.4978, grad_fn=<MseLossBackward0>)\n",
      "822 tensor(2965.4810, grad_fn=<MseLossBackward0>)\n",
      "823 tensor(2965.4634, grad_fn=<MseLossBackward0>)\n",
      "824 tensor(2965.4465, grad_fn=<MseLossBackward0>)\n",
      "825 tensor(2965.4294, grad_fn=<MseLossBackward0>)\n",
      "826 tensor(2965.4124, grad_fn=<MseLossBackward0>)\n",
      "827 tensor(2965.3953, grad_fn=<MseLossBackward0>)\n",
      "828 tensor(2965.3782, grad_fn=<MseLossBackward0>)\n",
      "829 tensor(2965.3616, grad_fn=<MseLossBackward0>)\n",
      "830 tensor(2965.3440, grad_fn=<MseLossBackward0>)\n",
      "831 tensor(2965.3271, grad_fn=<MseLossBackward0>)\n",
      "832 tensor(2965.3098, grad_fn=<MseLossBackward0>)\n",
      "833 tensor(2965.2930, grad_fn=<MseLossBackward0>)\n",
      "834 tensor(2965.2759, grad_fn=<MseLossBackward0>)\n",
      "835 tensor(2965.2590, grad_fn=<MseLossBackward0>)\n",
      "836 tensor(2965.2422, grad_fn=<MseLossBackward0>)\n",
      "837 tensor(2965.2251, grad_fn=<MseLossBackward0>)\n",
      "838 tensor(2965.2080, grad_fn=<MseLossBackward0>)\n",
      "839 tensor(2965.1914, grad_fn=<MseLossBackward0>)\n",
      "840 tensor(2965.1743, grad_fn=<MseLossBackward0>)\n",
      "841 tensor(2965.1575, grad_fn=<MseLossBackward0>)\n",
      "842 tensor(2965.1404, grad_fn=<MseLossBackward0>)\n",
      "843 tensor(2965.1238, grad_fn=<MseLossBackward0>)\n",
      "844 tensor(2965.1069, grad_fn=<MseLossBackward0>)\n",
      "845 tensor(2965.0898, grad_fn=<MseLossBackward0>)\n",
      "846 tensor(2965.0730, grad_fn=<MseLossBackward0>)\n",
      "847 tensor(2965.0562, grad_fn=<MseLossBackward0>)\n",
      "848 tensor(2965.0396, grad_fn=<MseLossBackward0>)\n",
      "849 tensor(2965.0222, grad_fn=<MseLossBackward0>)\n",
      "850 tensor(2965.0059, grad_fn=<MseLossBackward0>)\n",
      "851 tensor(2964.9890, grad_fn=<MseLossBackward0>)\n",
      "852 tensor(2964.9719, grad_fn=<MseLossBackward0>)\n",
      "853 tensor(2964.9556, grad_fn=<MseLossBackward0>)\n",
      "854 tensor(2964.9387, grad_fn=<MseLossBackward0>)\n",
      "855 tensor(2964.9219, grad_fn=<MseLossBackward0>)\n",
      "856 tensor(2964.9050, grad_fn=<MseLossBackward0>)\n",
      "857 tensor(2964.8884, grad_fn=<MseLossBackward0>)\n",
      "858 tensor(2964.8716, grad_fn=<MseLossBackward0>)\n",
      "859 tensor(2964.8550, grad_fn=<MseLossBackward0>)\n",
      "860 tensor(2964.8384, grad_fn=<MseLossBackward0>)\n",
      "861 tensor(2964.8215, grad_fn=<MseLossBackward0>)\n",
      "862 tensor(2964.8047, grad_fn=<MseLossBackward0>)\n",
      "863 tensor(2964.7883, grad_fn=<MseLossBackward0>)\n",
      "864 tensor(2964.7715, grad_fn=<MseLossBackward0>)\n",
      "865 tensor(2964.7549, grad_fn=<MseLossBackward0>)\n",
      "866 tensor(2964.7383, grad_fn=<MseLossBackward0>)\n",
      "867 tensor(2964.7217, grad_fn=<MseLossBackward0>)\n",
      "868 tensor(2964.7053, grad_fn=<MseLossBackward0>)\n",
      "869 tensor(2964.6882, grad_fn=<MseLossBackward0>)\n",
      "870 tensor(2964.6721, grad_fn=<MseLossBackward0>)\n",
      "871 tensor(2964.6553, grad_fn=<MseLossBackward0>)\n",
      "872 tensor(2964.6389, grad_fn=<MseLossBackward0>)\n",
      "873 tensor(2964.6223, grad_fn=<MseLossBackward0>)\n",
      "874 tensor(2964.6057, grad_fn=<MseLossBackward0>)\n",
      "875 tensor(2964.5891, grad_fn=<MseLossBackward0>)\n",
      "876 tensor(2964.5728, grad_fn=<MseLossBackward0>)\n",
      "877 tensor(2964.5564, grad_fn=<MseLossBackward0>)\n",
      "878 tensor(2964.5398, grad_fn=<MseLossBackward0>)\n",
      "879 tensor(2964.5234, grad_fn=<MseLossBackward0>)\n",
      "880 tensor(2964.5068, grad_fn=<MseLossBackward0>)\n",
      "881 tensor(2964.4905, grad_fn=<MseLossBackward0>)\n",
      "882 tensor(2964.4739, grad_fn=<MseLossBackward0>)\n",
      "883 tensor(2964.4578, grad_fn=<MseLossBackward0>)\n",
      "884 tensor(2964.4409, grad_fn=<MseLossBackward0>)\n",
      "885 tensor(2964.4248, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8000\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     out \u001b[39m=\u001b[39m model(train_data, train_data\u001b[39m.\u001b[39;49mbatch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(out, train_data\u001b[39m.\u001b[39my)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     train_loss_l\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index, torch\u001b[39m.\u001b[39mminimum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_weight\u001b[39m.\u001b[39mabs(),torch\u001b[39m.\u001b[39mones(data\u001b[39m.\u001b[39mnum_edges)))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m global_mean_pool(x, batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X50sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(8000):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data, train_data.batch)\n",
    "    train_loss = F.mse_loss(out, train_data.y)\n",
    "    train_loss_l.append(train_loss.item())\n",
    "\n",
    "    # test_out = model(test_data)\n",
    "    # test_loss = F.mse_loss(test_out, test_data.y)\n",
    "    # test_loss_l.append(test_loss.item())\n",
    "\n",
    "    print(epoch, train_loss)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([4.6932e-20, 4.9169e-20, 3.3778e-20,  ..., 7.4521e-25, 6.3317e-25,\n",
       "        7.8895e-25], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('model_final4', 'wb')\n",
    "pickle.dump(model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('model_final', 'rb')\n",
    "model_final = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "GCN                                      1,482\n",
       "GCNConv: 1-1                           36\n",
       "    SumAggregation: 2-1               --\n",
       "    Linear: 2-2                       5,896,800\n",
       "GCNConv: 1-2                           1\n",
       "    SumAggregation: 2-3               --\n",
       "    Linear: 2-4                       36\n",
       "=================================================================\n",
       "Total params: 5,898,355\n",
       "Trainable params: 5,898,355\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsf = summary(model_final)\n",
    "dsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jqcla\\AppData\\Local\\Temp\\ipykernel_12920\\938135185.py:3: UserWarning: Using a target size (torch.Size([163800])) that is different to the input size (torch.Size([39, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  train_loss = F.mse_loss(out, train_data.y)\n"
     ]
    }
   ],
   "source": [
    "train_data = ssp_obj.test_data\n",
    "out = model_final(train_data)\n",
    "train_loss = F.mse_loss(out, train_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "GCN                                      1,482\n",
       "GCNConv: 1-1                           36\n",
       "    SumAggregation: 2-1               --\n",
       "    Linear: 2-2                       5,896,800\n",
       "GCNConv: 1-2                           64\n",
       "    SumAggregation: 2-3               --\n",
       "    Linear: 2-4                       2,304\n",
       "Linear: 1-3                            10,647,000\n",
       "=================================================================\n",
       "Total params: 16,547,686\n",
       "Trainable params: 16,547,686\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_l[8:])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error(MSE)')\n",
    "plt.title('Loss vs Epoches')\n",
    "plt.grid(linestyle='--')\n",
    "# plt.plot(test_loss_l[50:], label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'GCN' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_final3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m file\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'GCN' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "file = open('model_final3', 'rb')\n",
    "model = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_final3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model3 \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m file\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m edge_to_adj(model3\u001b[39m.\u001b[39medge_weight)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "file = open('model_final3', 'rb')\n",
    "model3 = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "edge_to_adj(model3.edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_undirected.edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(38**2 + 38) /2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.9519e-41, 1.9519e-41, 1.9519e-41,  ..., 1.9519e-41, 1.9519e-41,\n",
       "        1.9519e-41], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = edge_to_adj(model.edge_weight)\n",
    "save.to_csv('model4_edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCESS-ESM1-5',\n",
       " 'AWI-CM-1-1-MR',\n",
       " 'BCC-CSM2-MR',\n",
       " 'CAMS-CSM1-0',\n",
       " 'CanESM5 p1',\n",
       " 'CanESM5 p2',\n",
       " 'CanESM5-CanOE p2',\n",
       " 'CESM2',\n",
       " 'CESM2-WACCM',\n",
       " 'CIESM',\n",
       " 'CMCC-CM2-SR5',\n",
       " 'CNRM-CM6-1 f2',\n",
       " 'CNRM-CM6-1-HR f2',\n",
       " 'CNRM-ESM2-1 f2',\n",
       " 'EC-Earth3',\n",
       " 'EC-Earth3-Veg',\n",
       " 'FGOALS-f3-L',\n",
       " 'FGOALS-g3',\n",
       " 'FIO-ESM-2-0',\n",
       " 'GFDL-CM4',\n",
       " 'GFDL-ESM4',\n",
       " 'GISS-E2-1-G p1',\n",
       " 'GISS-E2-1-G p3',\n",
       " 'HadGEM3-GC31-LL f3',\n",
       " 'HadGEM3-GC31-MM f3',\n",
       " 'INM-CM4-8',\n",
       " 'INM-CM5-0',\n",
       " 'IPSL-CM6A-LR',\n",
       " 'KACE-1-0-G',\n",
       " 'MCM-UA-1-0',\n",
       " 'MIROC6',\n",
       " 'MIROC-ES2L f2',\n",
       " 'MPI-ESM1-2-HR',\n",
       " 'MPI-ESM1-2-LR',\n",
       " 'MRI-ESM2-0',\n",
       " 'NESM3',\n",
       " 'NorESM2-LM',\n",
       " 'NorESM2-MM',\n",
       " 'UKESM1-0-LL f2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def edge_to_adj(edge_weight):\n",
    "    x = edge_weight\n",
    "    m = torch.zeros((39, 39))\n",
    "\n",
    "\n",
    "    triu_indices = torch.triu_indices(row=39, col=39, offset=1)\n",
    "    m[triu_indices[0], triu_indices[1]] = x[:741]\n",
    "    if x.size()[0] > 741: \n",
    "        tril_indices = torch.tril_indices(row=39, col=39, offset=-1)\n",
    "        m[tril_indices[0], tril_indices[1]] = x[741:]\n",
    "\n",
    "    t_np = m.detach().numpy() #convert to Numpy array\n",
    "    df = pd.DataFrame(t_np) #convert to a dataframe\n",
    "    return df\n",
    "\n",
    "file = open(\"model_names.txt\", \"r\") \n",
    "data = file.read() \n",
    "data_into_list = data.split(\"\\n\")\n",
    "file.close() \n",
    "names = data_into_list\n",
    "names.pop(0)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     file\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     edge_df \u001b[39m=\u001b[39m edge_to_adj(model\u001b[39m.\u001b[39medge_weight)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "models = ['model_final3', 'model_final2', 'model_undirected', 'model']\n",
    "for model in models:\n",
    "    file = open(model, 'rb')\n",
    "    model = pickle.load(file)\n",
    "    file.close()\n",
    "    edge_df = edge_to_adj(model.edge_weight)\n",
    "    edge_df.columns = names\n",
    "    edge_df.index = names\n",
    "    plt.matshow(edge_df)\n",
    "    cb = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23b01e561f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2394aac0c10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAGkCAYAAACckEpMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgBElEQVR4nO3df2xV9eH/8dfl16HKvTcybO+949J0St0AYZkwKFMpLFQ614CYBTUxJduYKPD9kmpwYPa1300pY5FgwmQ6DYNMVv5QxM8HBLovtsywZoUPxAaNYihSY2sngd5S4ULh/f3DcOelpb233Mv73NvnIzmRe8675744V/vy3Z4fHmOMEQAAFg2yHQAAAMoIAGAdZQQAsI4yAgBYRxkBAKyjjAAA1lFGAADrKCMAgHWUEQDAOsoIAGBdRpTRSy+9pIKCAg0fPlx33XWX/vGPf9iOlJDKykp5PJ64JRAI2I7Vo/3796usrEyhUEgej0dvvfVW3HZjjCorKxUKhZSTk6Pi4mIdPXrUTtge9JV/4cKF3T6LadOm2Ql7laqqKk2ZMkVer1e5ubmaN2+ePvroo7gxbj7+ieR38/HfuHGjJk6cKJ/PJ5/Pp6KiIr3zzjux7W4+9n1ld/Nxv5rry2jbtm1avny5nnnmGR0+fFj33HOPSktLdfLkSdvREjJ+/Hi1tLTElsbGRtuRetTZ2alJkyZpw4YNPW5fu3at1q1bpw0bNqihoUGBQECzZ89WR0fHDU7as77yS9KcOXPiPotdu3bdwITXVldXpyVLlqi+vl41NTXq6upSSUmJOjs7Y2PcfPwTyS+59/iPHj1aa9as0cGDB3Xw4EHNmjVLc+fOjRWOm499X9kl9x73bozL/fCHPzSLFy+OW/fd737X/PrXv7aUKHHPPvusmTRpku0YSZNktm/fHnt9+fJlEwgEzJo1a2Lrzp8/b/x+v/nTn/5kIWHvrs5vjDHl5eVm7ty5VvIkq62tzUgydXV1xpjMO/5X5zcms46/Mcbccsst5tVXX824Y2/Mf7Ibk1nH3dUzowsXLujQoUMqKSmJW19SUqIDBw5YSpWcY8eOKRQKqaCgQA899JCOHz9uO1LSmpqa1NraGvc5OI6jGTNmZMznIEm1tbXKzc1VYWGhFi1apLa2NtuRetTe3i5JGjlypKTMO/5X578iE47/pUuXVF1drc7OThUVFWXUsb86+xWZcNwlaYjtAL358ssvdenSJeXl5cWtz8vLU2trq6VUiZs6daq2bNmiwsJCffHFF3ruuec0ffp0HT16VN/61rdsx0vYlWPd0+fw6aef2oiUtNLSUv3sZz9Tfn6+mpqa9Jvf/EazZs3SoUOH5DiO7XgxxhhVVFTo7rvv1oQJEyRl1vHvKb/k/uPf2NiooqIinT9/XiNGjND27ds1bty4WOG4+dhfK7vk/uP+Ta4uoys8Hk/ca2NMt3VuVFpaGvvznXfeqaKiIt12223avHmzKioqLCbrn0z9HCRpwYIFsT9PmDBBkydPVn5+vnbu3Kn58+dbTBZv6dKlev/99/Xee+9125YJx/9a+d1+/O+44w4dOXJEZ86c0RtvvKHy8nLV1dXFtrv52F8r+7hx41x/3L/J1T+mGzVqlAYPHtxtFtTW1tbt/1Qywc0336w777xTx44dsx0lKVfOAMyWz0GSgsGg8vPzXfVZLFu2TG+//bbeffddjR49OrY+U47/tfL3xG3Hf9iwYbr99ts1efJkVVVVadKkSXrxxRcz4thfK3tP3Hbcv8nVZTRs2DDdddddqqmpiVtfU1Oj6dOnW0rVf9FoVB9++KGCwaDtKEkpKChQIBCI+xwuXLigurq6jPwcJOnUqVNqbm52xWdhjNHSpUv15ptvat++fSooKIjb7vbj31f+nrjp+PfEGKNoNOr6Y9+TK9l74urjbuvMiURVV1eboUOHmtdee8188MEHZvny5ebmm282J06csB2tT08++aSpra01x48fN/X19eanP/2p8Xq9rsze0dFhDh8+bA4fPmwkmXXr1pnDhw+bTz/91BhjzJo1a4zf7zdvvvmmaWxsNA8//LAJBoMmEolYTv613vJ3dHSYJ5980hw4cMA0NTWZd9991xQVFZlvf/vbrsj/+OOPG7/fb2pra01LS0ts+eqrr2Jj3Hz8+8rv9uO/cuVKs3//ftPU1GTef/99s2rVKjNo0CCzd+9eY4y7j31v2d1+3K/m+jIyxpg//vGPJj8/3wwbNsz84Ac/iDtl1M0WLFhggsGgGTp0qAmFQmb+/Pnm6NGjtmP16N133zWSui3l5eXGmK9PL3722WdNIBAwjuOYe++91zQ2NtoN/Q295f/qq69MSUmJufXWW83QoUPNmDFjTHl5uTl58qTt2MYY02NuSWbTpk2xMW4+/n3ld/vx//nPfx77/nLrrbeaH//4x7EiMsbdx7637G4/7lfzGGPMjZuHAQDQnat/ZwQAGBgoIwCAdZQRAMA6yggAYB1lBACwjjICAFiXEWUUjUZVWVl5zauK3Y789mRydon8NmVydinz8mfEdUaRSER+v1/t7e3y+Xy24ySN/PZkcnaJ/DZlcnYp8/JnxMwIAJDdKCMAgHWue57R5cuX9fnnn8vr9caeFxKJROL+mWnIb08mZ5fIb1MmZ5fckd8Yo46ODoVCIQ0a1Pvcx3W/M/rss88UDodtxwAApEhzc3Ofz7hK28zopZde0h/+8Ae1tLRo/PjxWr9+ve65554+v87r9UqS7tZPNERDex0bve8HKckKAEi9rq7zavh/VbHv671JSxlt27ZNy5cv10svvaQf/ehHevnll1VaWqoPPvhAY8aM6fVrr/xoboiGaoin9zK6NHR4yjIDANIjkUe0p+UEhnXr1ukXv/iFfvnLX+p73/ue1q9fr3A4rI0bN6bj7QAAGS7lZXThwgUdOnRIJSUlcetLSkp04MCBbuOj0agikUjcAgAYWFJeRl9++aUuXbqkvLy8uPV5eXlqbW3tNr6qqkp+vz+2cPICAAw8abvO6OqfERpjevy54cqVK9Xe3h5bmpub0xUJAOBSKT+BYdSoURo8eHC3WVBbW1u32ZIkOY4jx3FSHQMAkEFSPjMaNmyY7rrrLtXU1MStr6mp0fTp01P9dgCALJCWU7srKir06KOPavLkySoqKtIrr7yikydPavHixel4OwBAhktLGS1YsECnTp3Sb3/7W7W0tGjChAnatWuX8vPzE97H9o8b5fP2PnG7L3S9SQEA6TLYXEx4rOtuB3TltuenP/5OAmX0/RsTCgCQtC5zUbXakdBjLLhrNwDAOsoIAGAdZQQAsI4yAgBYRxkBAKyjjAAA1lFGAADr0vak1xthz+dH+hzDtUgA4H7MjAAA1lFGAADrKCMAgHWUEQDAOsoIAGAdZQQAsI4yAgBYRxkBAKyjjAAA1mX0HRgSkchdGiT33qkh0fwA4DaRjsu6pTCxscyMAADWUUYAAOsoIwCAdZQRAMA6yggAYB1lBACwjjICAFhHGQEArMv6i14TlcpHmA+EC1XdepEwAPfoMhclHU9oLDMjAIB1lBEAwDrKCABgHWUEALCOMgIAWEcZAQCso4wAANZRRgAA6ygjAIB13IEhCZn+CHMAcKuUz4wqKyvl8XjilkAgkOq3AQBkkbTMjMaPH6+///3vsdeDBw9Ox9sAALJEWspoyJAhzIYAAAlLywkMx44dUygUUkFBgR566CEdP37tu7ZGo1FFIpG4BQAwsKS8jKZOnaotW7Zoz549+vOf/6zW1lZNnz5dp06d6nF8VVWV/H5/bAmHw6mOBABwOY8xxqTzDTo7O3XbbbdpxYoVqqio6LY9Go0qGo3GXkciEYXDYZ3++DvyeTPzzHPOpgOAr59nVKsdam9vl8/n63Vs2k/tvvnmm3XnnXfq2LFjPW53HEeO46Q7BgDAxdI+9YhGo/rwww8VDAbT/VYAgAyV8pnRU089pbKyMo0ZM0ZtbW167rnnFIlEVF5enuq3uuFmPParxAaW9T1k+H/96/rCAH2I3j/FdgQMcF0Xz0t7diQ0NuVl9Nlnn+nhhx/Wl19+qVtvvVXTpk1TfX298vPzU/1WAIAskfIyqq6uTvUuAQBZLjNPVwMAZBXKCABgHWUEALCOMgIAWEcZAQCso4wAANZRRgAA61z72PHS/71QQ4YOtx0jbc6X/TChcYncqYEr7YHeOTsbbEcYkAabiwmPZWYEALCOMgIAWEcZAQCso4wAANZRRgAA6ygjAIB1lBEAwDrKCABgnWsvesXXErk4lkeYp8epXxQlNO5br/0zzUmA7MfMCABgHWUEALCOMgIAWEcZAQCso4wAANZRRgAA6ygjAIB1lBEAwDrKCABgHXdgyAKpfIQ5/oM7KwA3DjMjAIB1lBEAwDrKCABgHWUEALCOMgIAWEcZAQCso4wAANZRRgAA67jodQDhEeYA3CrpmdH+/ftVVlamUCgkj8ejt956K267MUaVlZUKhULKyclRcXGxjh49mqq8AIAslHQZdXZ2atKkSdqwYUOP29euXat169Zpw4YNamhoUCAQ0OzZs9XR0XHdYQEA2SnpH9OVlpaqtLS0x23GGK1fv17PPPOM5s+fL0navHmz8vLytHXrVj322GPXlxYAkJVSegJDU1OTWltbVVJSElvnOI5mzJihAwcO9Pg10WhUkUgkbgEADCwpLaPW1lZJUl5eXtz6vLy82LarVVVVye/3x5ZwOJzKSACADJCWU7s9Hk/ca2NMt3VXrFy5Uu3t7bGlubk5HZEAAC6W0lO7A4GApK9nSMFgMLa+ra2t22zpCsdx5DhOKmMAADJMSmdGBQUFCgQCqqmpia27cOGC6urqNH369FS+FQAgiyQ9Mzp79qw++eST2OumpiYdOXJEI0eO1JgxY7R8+XKtXr1aY8eO1dixY7V69WrddNNNeuSRR1IaHACQPZIuo4MHD2rmzJmx1xUVFZKk8vJy/eUvf9GKFSt07tw5PfHEEzp9+rSmTp2qvXv3yuv1pi51EupefsXK+2aslxMbdl/o+2mNAWBgSbqMiouLZYy55naPx6PKykpVVlZeTy4AwADCjVIBANZRRgAA6ygjAIB1lBEAwDrKCABgHWUEALCOMgIAWOcxvV00ZEEkEpHf79fpj78jn5euzGRcGAsMbF3momq1Q+3t7fL5fL2O5bs9AMA6yggAYB1lBACwjjICAFhHGQEArKOMAADWUUYAAOsoIwCAdZQRAMC6pJ/0CiRqz+dHEhrHnRoAMDMCAFhHGQEArKOMAADWUUYAAOsoIwCAdZQRAMA6yggAYB1lBACwjjICAFjHHRiSULxoke0I2en+xIY5OxvSmwMDWqJ3DEHiIh2XdUthYmOZGQEArKOMAADWUUYAAOsoIwCAdZQRAMA6yggAYB1lBACwjjICAFjn2otef7KsXEOGDrcdAy4SvX9Kn2MSvTCWCxxT75K5nNC4wR7+HzgdEjn+bj72SSfbv3+/ysrKFAqF5PF49NZbb8VtX7hwoTweT9wybdq0VOUFAGShpMuos7NTkyZN0oYNG645Zs6cOWppaYktu3btuq6QAIDslvSP6UpLS1VaWtrrGMdxFAgE+h0KADCwpOUHiLW1tcrNzVVhYaEWLVqktra2a46NRqOKRCJxCwBgYEl5GZWWlur111/Xvn379MILL6ihoUGzZs1SNBrtcXxVVZX8fn9sCYfDqY4EAHC5lJ9Nt2DBgtifJ0yYoMmTJys/P187d+7U/Pnzu41fuXKlKioqYq8jkQiFBAADTNpP7Q4Gg8rPz9exY8d63O44jhzHSXcMAICLpf2k81OnTqm5uVnBYDDdbwUAyFBJz4zOnj2rTz75JPa6qalJR44c0ciRIzVy5EhVVlbqwQcfVDAY1IkTJ7Rq1SqNGjVKDzzwQEqDAwCyR9JldPDgQc2cOTP2+srve8rLy7Vx40Y1NjZqy5YtOnPmjILBoGbOnKlt27bJ6/WmLjWyio3Hid8X+n6fY1J5lwa33p2geNGiG/p+GFi6Lp6X9GxCY5Muo+LiYhljrrl9z549ye4SADDAufdGRQCAAYMyAgBYRxkBAKyjjAAA1lFGAADrKCMAgHWUEQDAOtc+dtzZ8z8a4hlqOwYGsEQujJUSexw6gN4xMwIAWEcZAQCso4wAANZRRgAA6ygjAIB1lBEAwDrKCABgHWUEALCOMgIAWOfaOzAAmSKRx6YnepcGG49gB9JlsLmY8FhmRgAA6ygjAIB1lBEAwDrKCABgHWUEALCOMgIAWEcZAQCso4wAANa59qLX86V3acjQ4b2OMRlcpZ7LiY3L5L9jomwci0TeM9H3S2Rfw//rX4ntDBigBsC3OgCA21FGAADrKCMAgHWUEQDAOsoIAGAdZQQAsI4yAgBYRxkBAKyjjAAA1rn2DgxmUN9XwN/oq+hTeQcAt95ZIdG7ISQi0b+jjWMxED5LIJMk9Z9RVVWVpkyZIq/Xq9zcXM2bN08fffRR3BhjjCorKxUKhZSTk6Pi4mIdPXo0paEBANklqTKqq6vTkiVLVF9fr5qaGnV1damkpESdnZ2xMWvXrtW6deu0YcMGNTQ0KBAIaPbs2ero6Eh5eABAdvAYY0x/v/jf//63cnNzVVdXp3vvvVfGGIVCIS1fvlxPP/20JCkajSovL0+///3v9dhjj/W5z0gkIr/fr6n3/7bPG6Vm8o/p3MrGj+kGgpwd3CgVA0+Xuaha7VB7e7t8Pl+vY6/r20V7e7skaeTIkZKkpqYmtba2qqSkJDbGcRzNmDFDBw4c6HEf0WhUkUgkbgEADCz9LiNjjCoqKnT33XdrwoQJkqTW1lZJUl5eXtzYvLy82LarVVVVye/3x5ZwONzfSACADNXvMlq6dKnef/99/e1vf+u2zePxxL02xnRbd8XKlSvV3t4eW5qbm/sbCQCQofp1aveyZcv09ttva//+/Ro9enRsfSAQkPT1DCkYDMbWt7W1dZstXeE4jhzH6U8MAECWSGpmZIzR0qVL9eabb2rfvn0qKCiI215QUKBAIKCamprYugsXLqiurk7Tp09PTWIAQNZJama0ZMkSbd26VTt27JDX6439Hsjv9ysnJ0cej0fLly/X6tWrNXbsWI0dO1arV6/WTTfdpEceeSSpYDk7D2mIZ2hSXwMAyExJldHGjRslScXFxXHrN23apIULF0qSVqxYoXPnzumJJ57Q6dOnNXXqVO3du1derzclgQEA2ee6rjNKhyvXGRVrLjMjAMhgN+w6IwAAUoEyAgBYRxkBAKyjjAAA1lFGAADrKCMAgHWUEQDAOtc+dhxAdoreP8V2BNwgXRfPS3t2JDSWmREAwDrKCABgHWUEALCOMgIAWEcZAQCso4wAANZRRgAA6ygjAIB1XPQKoE9cqIp0Y2YEALCOMgIAWEcZAQCso4wAANZRRgAA6ygjAIB1lBEAwDrKCABgHWUEALCOOzAA6JOzs6HPMdylwa5EPqMbbbC5mPBYZkYAAOsoIwCAdZQRAMA6yggAYB1lBACwjjICAFhHGQEArKOMAADWcdErMIDt+fxIyvZ1Xyhlu8IAlNTMqKqqSlOmTJHX61Vubq7mzZunjz76KG7MwoUL5fF44pZp06alNDQAILskVUZ1dXVasmSJ6uvrVVNTo66uLpWUlKizszNu3Jw5c9TS0hJbdu3aldLQAIDsktSP6Xbv3h33etOmTcrNzdWhQ4d07733xtY7jqNAIJCahACArHddJzC0t7dLkkaOHBm3vra2Vrm5uSosLNSiRYvU1tZ2zX1Eo1FFIpG4BQAwsPS7jIwxqqio0N13360JEybE1peWlur111/Xvn379MILL6ihoUGzZs1SNBrtcT9VVVXy+/2xJRwO9zcSACBDeYwxpj9fuGTJEu3cuVPvvfeeRo8efc1xLS0tys/PV3V1tebPn99tezQajSuqSCSicDisYs3VEM/Q/kQDkKDUnk33/ZTtC9mhy1xUrXaovb1dPp+v17H9OrV72bJlevvtt7V///5ei0iSgsGg8vPzdezYsR63O44jx3H6EwMAkCWSKiNjjJYtW6bt27ertrZWBQUFfX7NqVOn1NzcrGAw2O+QAIDsltTvjJYsWaK//vWv2rp1q7xer1pbW9Xa2qpz585Jks6ePaunnnpK//znP3XixAnV1taqrKxMo0aN0gMPPJCWvwAAIPMl9Tsjj8fT4/pNmzZp4cKFOnfunObNm6fDhw/rzJkzCgaDmjlzpn73u98lfGJCJBKR3+/X6Y+/I5+XuxUB2YbfLQ0cafudUV+9lZOToz179iSzSwAAuFEqAMA+yggAYB1lBACwjjICAFhHGQEArKOMAADWUUYAAOsoIwCAdf26USoA9FeidwrnTg32nP/pD1Oyn66L56XdOxIay8wIAGAdZQQAsI4yAgBYRxkBAKyjjAAA1lFGAADrKCMAgHWUEQDAOi56BTLMjF/9ynaEG+OnfQ8Z/t//SmhXqbqIE+nDzAgAYB1lBACwjjICAFhHGQEArKOMAADWUUYAAOsoIwCAdZQRAMA6yggAYJ1r78BQ+r8WasjQ4bZjAHCxRO+skMidGrhLg13MjAAA1lFGAADrKCMAgHWUEQDAOsoIAGAdZQQAsI4yAgBYRxkBAKxz7UWvqeIxxnYEAJZF75/S5xgeYW5XUjOjjRs3auLEifL5fPL5fCoqKtI777wT226MUWVlpUKhkHJyclRcXKyjR4+mPDQAILskVUajR4/WmjVrdPDgQR08eFCzZs3S3LlzY4Wzdu1arVu3Ths2bFBDQ4MCgYBmz56tjo6OtIQHAGSHpMqorKxMP/nJT1RYWKjCwkI9//zzGjFihOrr62WM0fr16/XMM89o/vz5mjBhgjZv3qyvvvpKW7duTVd+AEAW6PcJDJcuXVJ1dbU6OztVVFSkpqYmtba2qqSkJDbGcRzNmDFDBw4cuOZ+otGoIpFI3AIAGFiSLqPGxkaNGDFCjuNo8eLF2r59u8aNG6fW1lZJUl5eXtz4vLy82LaeVFVVye/3x5ZwOJxsJABAhku6jO644w4dOXJE9fX1evzxx1VeXq4PPvggtt3j8cSNN8Z0W/dNK1euVHt7e2xpbm5ONhIAIMMlfWr3sGHDdPvtt0uSJk+erIaGBr344ot6+umnJUmtra0KBoOx8W1tbd1mS9/kOI4cx0k2BgAgi1z3Ra/GGEWjURUUFCgQCKimpia27cKFC6qrq9P06dOv920AAFksqZnRqlWrVFpaqnA4rI6ODlVXV6u2tla7d++Wx+PR8uXLtXr1ao0dO1Zjx47V6tWrddNNN+mRRx5JV34AQBZIqoy++OILPfroo2ppaZHf79fEiRO1e/duzZ49W5K0YsUKnTt3Tk888YROnz6tqVOnau/evfJ6vUkH8xjD3RMscHY22I4AuBqPME8PjzHu+o4fiUTk9/tVdN//1ZChw23HGXAoI+D6UUZf67p4XvW7/4/a29vl8/l6HcuNUgEA1lFGAADrKCMAgHWUEQDAOsoIAGAdZQQAsI4yAgBY59rHjjt7/kdDPENtxwCApNl4hHndK6+kbF+pEum4rFsKExvLzAgAYB1lBACwjjICAFhHGQEArKOMAADWUUYAAOsoIwCAdZQRAMA6yggAYJ1r78AAIP32fH7EdoQB7b5Q32MGymfEzAgAYB1lBACwjjICAFhHGQEArKOMAADWUUYAAOsoIwCAdZQRAMA61170uv3jRvm8dCWA7JXIBa33hb6fsn25Gd/tAQDWUUYAAOsoIwCAdZQRAMA6yggAYB1lBACwjjICAFhHGQEArKOMAADWufYODACAxO+skMidGtx8l4akZkYbN27UxIkT5fP55PP5VFRUpHfeeSe2feHChfJ4PHHLtGnTUh4aAJBdkpoZjR49WmvWrNHtt98uSdq8ebPmzp2rw4cPa/z48ZKkOXPmaNOmTbGvGTZsWArjAgCyUVJlVFZWFvf6+eef18aNG1VfXx8rI8dxFAgEUpcQAJD1+n0Cw6VLl1RdXa3Ozk4VFRXF1tfW1io3N1eFhYVatGiR2traet1PNBpVJBKJWwAAA0vSZdTY2KgRI0bIcRwtXrxY27dv17hx4yRJpaWlev3117Vv3z698MILamho0KxZsxSNRq+5v6qqKvn9/tgSDof7/7cBAGQkjzHGJPMFFy5c0MmTJ3XmzBm98cYbevXVV1VXVxcrpG9qaWlRfn6+qqurNX/+/B73F41G48oqEokoHA7r9Mff4XlGAJAgN55NF+m4rFsKj6u9vV0+n6/XsUmf2j1s2LDYCQyTJ09WQ0ODXnzxRb388svdxgaDQeXn5+vYsWPX3J/jOHIcJ9kYAIAsct1TD2PMNX8Md+rUKTU3NysYDF7v2wAAslhSM6NVq1aptLRU4XBYHR0dqq6uVm1trXbv3q2zZ8+qsrJSDz74oILBoE6cOKFVq1Zp1KhReuCBB9KVHwCQIDc/wjypMvriiy/06KOPqqWlRX6/XxMnTtTu3bs1e/ZsnTt3To2NjdqyZYvOnDmjYDComTNnatu2bfJ6venKDwDIAkmV0WuvvXbNbTk5OdqzZ891BwIADDycrgYAsI4yAgBYRxkBAKyjjAAA1lFGAADrKCMAgHWUEQDAOtc+dvyBwjs1xDPUdgwAGHBs3HSVmREAwDrKCABgHWUEALCOMgIAWEcZAQCso4wAANZRRgAA6ygjAIB1rr3oFQDgXolcGNtlLko6ntD+mBkBAKyjjAAA1lFGAADrKCMAgHWUEQDAOsoIAGAdZQQAsI4yAgBY57qLXo0xkqQuXZSM5TAAgH7r0kVJ//m+3hvXlVFHR4ck6T3tspwEAJAKHR0d8vv9vY7xmEQq6wa6fPmyPv/8c3m9Xnk8HklSJBJROBxWc3OzfD6f5YTJI789mZxdIr9NmZxdckd+Y4w6OjoUCoU0aFDvvxVy3cxo0KBBGj16dI/bfD5fRv5LcQX57cnk7BL5bcrk7JL9/H3NiK7gBAYAgHWUEQDAuowoI8dx9Oyzz8pxHNtR+oX89mRydon8NmVydinz8rvuBAYAwMCTETMjAEB2o4wAANZRRgAA6ygjAIB1lBEAwDrKCABgHWUEALCOMgIAWPf/ASyw3cschBm2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f9891b1dc0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20fb46bac40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('model_undirected', 'wb')\n",
    "pickle.dump(model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\shrinkage_prior.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m edge_weights \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mtestfile.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/shrinkage_prior.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m edge_weights\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jqcla\\AppData\\Local\\Temp\\ipykernel_5360\\392092164.py:6: UserWarning: Using a target size (torch.Size([163800])) that is different to the input size (torch.Size([1, 163800])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  train_loss = F.mse_loss(out, train_data.y)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import R2Score\n",
    "\n",
    "r2 = R2Score()\n",
    "out = model(train_data, None)\n",
    "# r2 = R2Score(out,train_data.y)\n",
    "train_loss = F.mse_loss(out, train_data.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2964.4084, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[284.5283, 287.5394, 284.8813,  ..., 295.2670, 295.7874, 296.4720],\n",
       "        [282.4641, 280.2446, 280.3983,  ..., 296.2043, 296.9356, 298.2597],\n",
       "        [284.2912, 284.0540, 284.1302,  ..., 297.9766, 298.9336, 299.5635],\n",
       "        ...,\n",
       "        [283.6683, 283.9431, 283.8918,  ..., 294.2794, 295.5863, 296.7030],\n",
       "        [284.8356, 285.7901, 287.1679,  ..., 297.3830, 297.6404, 297.9521],\n",
       "        [284.8099, 284.1621, 282.4539,  ..., 296.9257, 297.4362, 298.1982]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 288.5294,  286.4150,  291.2878,  ...,  289.3131, -281.8930,\n",
       "        -280.8668], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.flatten()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-98.2444, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2(out,train_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([285.1003, 285.9076, 282.4460,  ..., 296.6862, 297.5055, 298.2751])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
