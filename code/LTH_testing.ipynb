{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import ssp_data\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import pdb\n",
    "import net\n",
    "import pruning\n",
    "import copy\n",
    "from scipy.sparse import coo_matrix\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data_pickle', 'rb')\n",
    "ssp_obj = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pdb\n",
    "import copy\n",
    "\n",
    "def torch_normalize_adj(adj):\n",
    "    adj = adj + torch.eye(adj.shape[0]).cpu()\n",
    "    rowsum = adj.sum(1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0\n",
    "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt).cpu()\n",
    "    return adj.mm(d_mat_inv_sqrt).t().mm(d_mat_inv_sqrt)\n",
    "\n",
    "class net_gcn(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, adj):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = len(embedding_dim) - 1\n",
    "        self.net_layer = nn.ModuleList([nn.Linear(embedding_dim[ln], embedding_dim[ln+1], bias=False) for ln in range(self.layer_num)])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.adj_nonzero = torch.nonzero(adj, as_tuple=False).shape[0]\n",
    "        self.adj_mask1_train = nn.Parameter(self.generate_adj_mask(adj))\n",
    "        self.adj_mask2_fixed = nn.Parameter(self.generate_adj_mask(adj), requires_grad=False)\n",
    "        self.normalize = torch_normalize_adj\n",
    "    \n",
    "    def forward(self, x, adj, val_test=False):\n",
    "        \n",
    "        adj = torch.mul(adj, self.adj_mask1_train)\n",
    "        adj = torch.mul(adj, self.adj_mask2_fixed)\n",
    "        adj = self.normalize(adj)\n",
    "        #adj = torch.mul(adj, self.adj_mask2_fixed)\n",
    "        for ln in range(self.layer_num):\n",
    "            x = torch.mm(adj, x)\n",
    "            x = self.net_layer[ln](x)\n",
    "            if ln == self.layer_num - 1:\n",
    "                break\n",
    "            x = self.relu(x)\n",
    "            if val_test:\n",
    "                continue\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def generate_adj_mask(self, input_adj):\n",
    "        \n",
    "        sparse_adj = input_adj\n",
    "        zeros = torch.zeros_like(sparse_adj)\n",
    "        ones = torch.ones_like(sparse_adj)\n",
    "        mask = torch.where(sparse_adj != 0, ones, zeros)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning.setup_seed(seed)\n",
    "\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = load_data(args['dataset'])\n",
    "\n",
    "args = {\n",
    "    'embedding_dim': [74100, 512, 74100],\n",
    "    'lr': 0.01,\n",
    "    'weight_decay': 5e-4,\n",
    "    'pruning_percent_wei': 0.2,\n",
    "    'pruning_percent_adj': 0.05,\n",
    "    'total_epoch': 2000,\n",
    "    's1': 1e-6,\n",
    "    's2': 1e-3,\n",
    "    'init_soft_mask_type' : 'all_one',\n",
    "    'weight_dir': False,\n",
    "}\n",
    "\n",
    "def run_fix_mask(args, seed, rewind_weight_mask):\n",
    "\n",
    "    pruning.setup_seed(seed)\n",
    "\n",
    "    adj = ssp_obj.train_data.edge_index\n",
    "    features = ssp_obj.train_data.x.float()\n",
    "    labels = ssp_obj.train_data.y.float()\n",
    "\n",
    "    node_num = features.size()[0]\n",
    "    class_num = labels.numpy().max() + 1\n",
    "\n",
    "    adj = adj.cpu()\n",
    "    features = features.cpu()\n",
    "    labels = labels.cpu()\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    net_gcn = net.net_gcn(embedding_dim=args['embedding_dim'], adj=adj)\n",
    "    pruning.add_mask(net_gcn)\n",
    "    net_gcn = net_gcn.cpu()\n",
    "    net_gcn.load_state_dict(rewind_weight_mask)\n",
    "    adj_spar, wei_spar = pruning.print_sparsity(net_gcn)\n",
    "\n",
    "    for name, param in net_gcn.named_parameters():\n",
    "        if 'mask' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    optimizer = torch.optim.Adam(net_gcn.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    acc_test = 0.0\n",
    "    best_val_acc = {'val_acc': 0, 'epoch' : 0, 'test_acc': 0}\n",
    "\n",
    "    for epoch in range(200):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net_gcn(features, adj)\n",
    "        loss = loss_func(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            output = net_gcn(features, adj, val_test=True)\n",
    "            acc_val = f1_score(labels.cpu().numpy(), output.cpu().numpy().argmax(axis=1), average='micro')\n",
    "            # acc_test = f1_score(labels[idx_test].cpu().numpy(), output[idx_test].cpu().numpy().argmax(axis=1), average='micro')\n",
    "            if acc_val > best_val_acc['val_acc']:\n",
    "                best_val_acc['val_acc'] = acc_val\n",
    "                best_val_acc['test_acc'] = acc_test\n",
    "                best_val_acc['epoch'] = epoch\n",
    "\n",
    "        print(\"(Fix Mask) Epoch:[{}] Val:[{:.2f}] Test:[{:.2f}] | Final Val:[{:.2f}] Test:[{:.2f}] at Epoch:[{}]\"\n",
    "                    .format(epoch, acc_val * 100, acc_test * 100, \n",
    "                                best_val_acc['val_acc'] * 100, \n",
    "                                best_val_acc['test_acc'] * 100, \n",
    "                                best_val_acc['epoch']))\n",
    "        return best_val_acc['val_acc'], best_val_acc['test_acc'], best_val_acc['epoch'], adj_spar, wei_spar\n",
    "    \n",
    "def run_get_mask(args, seed, imp_num, rewind_weight_mask=None):\n",
    "\n",
    "    pruning.setup_seed(seed)\n",
    "    # adj, features, labels, idx_train, idx_val, idx_test = load_data(args['dataset'])\n",
    "\n",
    "    adj = ssp_obj.train_data.edge_index\n",
    "    features = ssp_obj.train_data.x\n",
    "    labels = ssp_obj.train_data.y\n",
    "    \n",
    "    node_num = features.size()[0]\n",
    "    class_num = labels.numpy().max() + 1\n",
    "\n",
    "    adj = adj.cpu()\n",
    "    features = features.cpu()\n",
    "    labels = labels.cpu()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    net_gcn = net.net_gcn(embedding_dim=args['embedding_dim'], adj=adj)\n",
    "    pruning.add_mask(net_gcn)\n",
    "    net_gcn = net_gcn.cpu()\n",
    "\n",
    "    if args['weight_dir']:\n",
    "        print(\"load : {}\".format(args['weight_dir']))\n",
    "        encoder_weight = {}\n",
    "        cl_ckpt = torch.load(args['weight_dir'], map_location='cpu')\n",
    "        encoder_weight['weight_orig_weight'] = cl_ckpt['gcn.fc.weight']\n",
    "        ori_state_dict = net_gcn.net_layer[0].state_dict()\n",
    "        ori_state_dict.update(encoder_weight)\n",
    "        net_gcn.net_layer[0].load_state_dict(ori_state_dict)\n",
    "\n",
    "    if rewind_weight_mask:\n",
    "        net_gcn.load_state_dict(rewind_weight_mask)\n",
    "        if not args['rewind_soft_mask'] or args['init_soft_mask_type'] == 'all_one':\n",
    "            pruning.soft_mask_init(net_gcn, args['init_soft_mask_type'], seed)\n",
    "        adj_spar, wei_spar = pruning.print_sparsity(net_gcn)\n",
    "    else:\n",
    "        pruning.soft_mask_init(net_gcn, args['init_soft_mask_type'], seed)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net_gcn.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "    acc_test = 0.0\n",
    "    best_val_acc = {'val_acc': 0, 'epoch' : 0, 'test_acc':0}\n",
    "    rewind_weight = copy.deepcopy(net_gcn.state_dict())\n",
    "    for epoch in range(args['total_epoch']):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net_gcn(features, adj)\n",
    "        loss = loss_func(output, labels.float())\n",
    "        loss.backward()\n",
    "        pruning.subgradient_update_mask(net_gcn, args) # l1 norm\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            output = net_gcn(features, adj, val_test=True)\n",
    "            acc_val = f1_score(labels.cpu().numpy(), output.cpu().numpy().argmax(axis=1), average='micro')\n",
    "            # acc_test = f1_score(labels[idx_test].cpu().numpy(), output[idx_test].cpu().numpy().argmax(axis=1), average='micro')\n",
    "            if acc_val > best_val_acc['val_acc']:\n",
    "                best_val_acc['test_acc'] = acc_test\n",
    "                best_val_acc['val_acc'] = acc_val\n",
    "                best_val_acc['epoch'] = epoch\n",
    "                best_epoch_mask = pruning.get_final_mask_epoch(net_gcn, adj_percent=args['pruning_percent_adj'], \n",
    "                                                                        wei_percent=args['pruning_percent_wei'])\n",
    "\n",
    "            print(\"(Get Mask) Epoch:[{}] Val:[{:.2f}] Test:[{:.2f}] | Best Val:[{:.2f}] Test:[{:.2f}] at Epoch:[{}]\"\n",
    "                 .format(epoch, acc_val * 100, acc_test * 100, \n",
    "                                best_val_acc['val_acc'] * 100,  \n",
    "                                best_val_acc['test_acc'] * 100,\n",
    "                                best_val_acc['epoch']))\n",
    "\n",
    "    return best_epoch_mask, rewind_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\LTH_testing.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     final_mask_dict, rewind_weight \u001b[39m=\u001b[39m run_get_mask(args, seed, p, rewind_weight)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     rewind_weight[\u001b[39m'\u001b[39m\u001b[39madj_mask1_train\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m final_mask_dict[\u001b[39m'\u001b[39m\u001b[39madj_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     rewind_weight[\u001b[39m'\u001b[39m\u001b[39madj_mask2_fixed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m final_mask_dict[\u001b[39m'\u001b[39m\u001b[39madj_mask\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\LTH_testing.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     adj_spar, wei_spar \u001b[39m=\u001b[39m pruning\u001b[39m.\u001b[39mprint_sparsity(net_gcn)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     pruning\u001b[39m.\u001b[39;49msoft_mask_init(net_gcn, args[\u001b[39m'\u001b[39;49m\u001b[39minit_soft_mask_type\u001b[39;49m\u001b[39m'\u001b[39;49m], seed)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(net_gcn\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m], weight_decay\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/LTH_testing.ipynb#W4sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m acc_test \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\pruning.py:379\u001b[0m, in \u001b[0;36msoft_mask_init\u001b[1;34m(model, init_type, seed)\u001b[0m\n\u001b[0;32m    377\u001b[0m setup_seed(seed)\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m init_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall_one\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 379\u001b[0m     add_trainable_mask_noise(model, c\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m)\n\u001b[0;32m    380\u001b[0m \u001b[39melif\u001b[39;00m init_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mkaiming\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    382\u001b[0m     init\u001b[39m.\u001b[39mkaiming_uniform_(model\u001b[39m.\u001b[39madj_mask1_train, a\u001b[39m=\u001b[39mmath\u001b[39m.\u001b[39msqrt(\u001b[39m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\pruning.py:358\u001b[0m, in \u001b[0;36madd_trainable_mask_noise\u001b[1;34m(model, c)\u001b[0m\n\u001b[0;32m    356\u001b[0m rand1 \u001b[39m=\u001b[39m rand1 \u001b[39m*\u001b[39m model\u001b[39m.\u001b[39madj_mask1_train\n\u001b[0;32m    357\u001b[0m \u001b[39mprint\u001b[39m(rand1\u001b[39m.\u001b[39mdtype, model\u001b[39m.\u001b[39madj_mask1_train\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 358\u001b[0m model\u001b[39m.\u001b[39;49madj_mask1_train\u001b[39m.\u001b[39;49madd_(rand1\u001b[39m.\u001b[39;49mfloat())\u001b[39m#yeet\u001b[39;00m\n\u001b[0;32m    360\u001b[0m rand2 \u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrand(model\u001b[39m.\u001b[39mnet_layer[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight_mask_train\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m c\n\u001b[0;32m    361\u001b[0m rand2 \u001b[39m=\u001b[39m rand2\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mnet_layer[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight_mask_train\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "rewind_weight = None\n",
    "seed = 1\n",
    "for p in range(20):\n",
    "    \n",
    "    final_mask_dict, rewind_weight = run_get_mask(args, seed, p, rewind_weight)\n",
    "\n",
    "    rewind_weight['adj_mask1_train'] = final_mask_dict['adj_mask']\n",
    "    rewind_weight['adj_mask2_fixed'] = final_mask_dict['adj_mask']\n",
    "    rewind_weight['net_layer.0.weight_mask_train'] = final_mask_dict['weight1_mask']\n",
    "    rewind_weight['net_layer.0.weight_mask_fixed'] = final_mask_dict['weight1_mask']\n",
    "    rewind_weight['net_layer.1.weight_mask_train'] = final_mask_dict['weight2_mask']\n",
    "    rewind_weight['net_layer.1.weight_mask_fixed'] = final_mask_dict['weight2_mask']\n",
    "\n",
    "    best_acc_val, final_acc_test, final_epoch_list, adj_spar, wei_spar = run_fix_mask(args, seed, rewind_weight)\n",
    "    print(\"=\" * 120)\n",
    "    print(\"syd : Sparsity:[{}], Best Val:[{:.2f}] at epoch:[{}] | Final Test Acc:[{:.2f}] Adj:[{:.2f}%] Wei:[{:.2f}%]\"\n",
    "        .format(p + 1, best_acc_val * 100, final_epoch_list, final_acc_test * 100, adj_spar, wei_spar))\n",
    "    print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
