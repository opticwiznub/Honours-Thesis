{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "\n",
    "# import numpy as np\n",
    "from utils import *\n",
    "from modules import *\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_type='synthetic', data_filename='alarm', data_dir='data/', data_sample_size=50, data_variable_size=39, graph_type='erdos-renyi', graph_degree=2, graph_sem_type='linear-gauss', graph_linear_type='nonlinear_2', edge_types=2, x_dims=1, z_dims=1, optimizer='Adam', graph_threshold=0.3, tau_A=0.0, lambda_A=0.0, c_A=1, use_A_connect_loss=0, use_A_positiver_loss=0, no_cuda=True, seed=42, epochs=300, batch_size=100, lr=0.003, encoder_hidden=64, decoder_hidden=64, temp=0.5, k_max_iter=100.0, encoder='mlp', decoder='mlp', no_factor=False, suffix='_springs5', encoder_dropout=0.0, decoder_dropout=0.0, save_folder='logs', load_folder='', h_tol=1e-08, prediction_steps=10, lr_decay=200, gamma=1.0, skip_first=False, var=5e-05, hard=False, prior=False, dynamic_graph=False, cuda=False, factor=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# -----------data parameters ------\n",
    "# configurations\n",
    "parser.add_argument('--data_type', type=str, default= 'synthetic',\n",
    "                    choices=['synthetic', 'discrete', 'real'],\n",
    "                    help='choosing which experiment to do.')\n",
    "parser.add_argument('--data_filename', type=str, default= 'alarm',\n",
    "                    help='data file name containing the discrete files.')\n",
    "parser.add_argument('--data_dir', type=str, default= 'data/',\n",
    "                    help='data file name containing the discrete files.')\n",
    "parser.add_argument('--data_sample_size', type=int, default=50,\n",
    "                    help='the number of samples of data')\n",
    "parser.add_argument('--data_variable_size', type=int, default=39,\n",
    "                    help='the number of variables in synthetic generated data')\n",
    "parser.add_argument('--graph_type', type=str, default='erdos-renyi',\n",
    "                    help='the type of DAG graph by generation method')\n",
    "parser.add_argument('--graph_degree', type=int, default=2,\n",
    "                    help='the number of degree in generated DAG graph')\n",
    "parser.add_argument('--graph_sem_type', type=str, default='linear-gauss',\n",
    "                    help='the structure equation model (SEM) parameter type')\n",
    "parser.add_argument('--graph_linear_type', type=str, default='nonlinear_2',\n",
    "                    help='the synthetic data type: linear -> linear SEM, nonlinear_1 -> x=Acos(x+1)+z, nonlinear_2 -> x=2sin(A(x+0.5))+A(x+0.5)+z')\n",
    "parser.add_argument('--edge-types', type=int, default=2,\n",
    "                    help='The number of edge types to infer.')\n",
    "parser.add_argument('--x_dims', type=int, default=1, #changed here\n",
    "                    help='The number of input dimensions: default 1.')\n",
    "parser.add_argument('--z_dims', type=int, default=1,\n",
    "                    help='The number of latent variable dimensions: default the same as variable size.')\n",
    "\n",
    "# -----------training hyperparameters\n",
    "parser.add_argument('--optimizer', type = str, default = 'Adam',\n",
    "                    help = 'the choice of optimizer used')\n",
    "parser.add_argument('--graph_threshold', type=  float, default = 0.3,  # 0.3 is good, 0.2 is error prune\n",
    "                    help = 'threshold for learned adjacency matrix binarization')\n",
    "parser.add_argument('--tau_A', type = float, default=0.0,\n",
    "                    help='coefficient for L-1 norm of A.')\n",
    "parser.add_argument('--lambda_A',  type = float, default= 0.,\n",
    "                    help='coefficient for DAG constraint h(A).')\n",
    "parser.add_argument('--c_A',  type = float, default= 1,\n",
    "                    help='coefficient for absolute value h(A).')\n",
    "parser.add_argument('--use_A_connect_loss',  type = int, default= 0,\n",
    "                    help='flag to use A connect loss')\n",
    "parser.add_argument('--use_A_positiver_loss', type = int, default = 0,\n",
    "                    help = 'flag to enforce A must have positive values')\n",
    "\n",
    "\n",
    "parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default= 300,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--batch-size', type=int, default = 100, # note: should be divisible by sample size, otherwise throw an error\n",
    "                    help='Number of samples per batch.')\n",
    "parser.add_argument('--lr', type=float, default=3e-3,  # basline rate = 1e-3\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--encoder-hidden', type=int, default=64,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--decoder-hidden', type=int, default=64,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--temp', type=float, default=0.5,\n",
    "                    help='Temperature for Gumbel softmax.')\n",
    "parser.add_argument('--k_max_iter', type = int, default = 1e2,\n",
    "                    help ='the max iteration number for searching lambda and c')\n",
    "\n",
    "parser.add_argument('--encoder', type=str, default='mlp',\n",
    "                    help='Type of path encoder model (mlp, or sem).')\n",
    "parser.add_argument('--decoder', type=str, default='mlp',\n",
    "                    help='Type of decoder model (mlp, or sim).')\n",
    "parser.add_argument('--no-factor', action='store_true', default=False,\n",
    "                    help='Disables factor graph model.')\n",
    "parser.add_argument('--suffix', type=str, default='_springs5',\n",
    "                    help='Suffix for training data (e.g. \"_charged\".')\n",
    "parser.add_argument('--encoder-dropout', type=float, default=0.0,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--decoder-dropout', type=float, default=0.0,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--save-folder', type=str, default='logs',\n",
    "                    help='Where to save the trained model, leave empty to not save anything.')\n",
    "parser.add_argument('--load-folder', type=str, default='',\n",
    "                    help='Where to load the trained model if finetunning. ' +\n",
    "                         'Leave empty to train from scratch')\n",
    "\n",
    "\n",
    "parser.add_argument('--h_tol', type=float, default = 1e-8,\n",
    "                    help='the tolerance of error of h(A) to zero')\n",
    "parser.add_argument('--prediction-steps', type=int, default=10, metavar='N',\n",
    "                    help='Num steps to predict before re-using teacher forcing.')\n",
    "parser.add_argument('--lr-decay', type=int, default=200,\n",
    "                    help='After how epochs to decay LR by a factor of gamma.')\n",
    "parser.add_argument('--gamma', type=float, default= 1.0,\n",
    "                    help='LR decay factor.')\n",
    "parser.add_argument('--skip-first', action='store_true', default=False,\n",
    "                    help='Skip first edge type in decoder, i.e. it represents no-edge.')\n",
    "parser.add_argument('--var', type=float, default=5e-5,\n",
    "                    help='Output variance.')\n",
    "parser.add_argument('--hard', action='store_true', default=False,\n",
    "                    help='Uses discrete samples in training forward pass.')\n",
    "parser.add_argument('--prior', action='store_true', default=False,\n",
    "                    help='Whether to use sparsity prior.')\n",
    "parser.add_argument('--dynamic-graph', action='store_true', default=False,\n",
    "                    help='Whether test with dynamically re-computed graph.')\n",
    "\n",
    "# class Args(argparse.Namespace):\n",
    "#     parser\n",
    "\n",
    "# args=Args().parse_args()\n",
    "\n",
    "args = parser.parse_args('')\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.factor = True\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, ground_truth_G = load_data( args, args.batch_size, args.suffix)\n",
    "\n",
    "off_diag = np.ones([args.data_variable_size, args.data_variable_size]) - np.eye(args.data_variable_size)\n",
    "\n",
    "rel_rec = np.array(encode_onehot(np.where(off_diag)[1]), dtype=np.float64)\n",
    "rel_send = np.array(encode_onehot(np.where(off_diag)[0]), dtype=np.float64)\n",
    "rel_rec = torch.DoubleTensor(rel_rec)\n",
    "rel_send = torch.DoubleTensor(rel_send)\n",
    "\n",
    "num_nodes = args.data_variable_size\n",
    "adj_A = np.zeros((num_nodes, num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MLPEncoder(   \n",
    "    args.data_variable_size * args.x_dims, \n",
    "    args.x_dims, \n",
    "    args.encoder_hidden,\n",
    "    int(args.z_dims), \n",
    "    adj_A,\n",
    "    batch_size = args.batch_size,\n",
    "    do_prob = args.encoder_dropout, \n",
    "    factor = args.factor\n",
    "    ).double()\n",
    "\n",
    "decoder = MLPDecoder(\n",
    "    args.data_variable_size * args.x_dims,\n",
    "    args.z_dims, args.x_dims, encoder,\n",
    "    data_variable_size = args.data_variable_size,\n",
    "    batch_size = args.batch_size,\n",
    "    n_hid=args.decoder_hidden,\n",
    "    do_prob=args.decoder_dropout\n",
    "    ).double()\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=args.lr)\n",
    "\n",
    "# Linear indices of an upper triangular mx, used for acc calculation\n",
    "triu_indices = get_triu_offdiag_indices(args.data_variable_size)\n",
    "tril_indices = get_tril_offdiag_indices(args.data_variable_size)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_decay, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\.conda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inverse: LAPACK library not found in compilation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\dag_learning.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m \u001b[39mwhile\u001b[39;00m c_A \u001b[39m<\u001b[39m \u001b[39m1e+20\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m         ELBO_loss, NLL_loss, MSE_loss, graph, origin_A \u001b[39m=\u001b[39m train(epoch, best_ELBO_loss, ground_truth_G, lambda_A, c_A, optimizer)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m         \u001b[39mif\u001b[39;00m ELBO_loss \u001b[39m<\u001b[39m best_ELBO_loss:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m             best_ELBO_loss \u001b[39m=\u001b[39m ELBO_loss\n",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\dag_learning.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m enc_x, logits, origin_A, adj_A_tilt_encoder, z_gap, z_positive, myA, Wa \u001b[39m=\u001b[39m encoder(data, rel_rec, rel_send)  \u001b[39m# logits is of size: [num_sims, z_dims]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m edges \u001b[39m=\u001b[39m logits\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m dec_x, output, adj_A_tilt_decoder \u001b[39m=\u001b[39m decoder(data, edges, args\u001b[39m.\u001b[39;49mdata_variable_size \u001b[39m*\u001b[39;49m args\u001b[39m.\u001b[39;49mx_dims, rel_rec, rel_send, origin_A, adj_A_tilt_encoder, Wa)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39msum(output \u001b[39m!=\u001b[39m output):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnan error\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\modules.py:318\u001b[0m, in \u001b[0;36mMLPDecoder.forward\u001b[1;34m(self, inputs, input_z, n_in_node, rel_rec, rel_send, origin_A, adj_A_tilt, Wa)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, input_z, n_in_node, rel_rec, rel_send, origin_A, adj_A_tilt, Wa):\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m     \u001b[39m#adj_A_new1 = (I-A^T)^(-1)\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     adj_A_new1 \u001b[39m=\u001b[39m preprocess_adj_new1(origin_A)\n\u001b[0;32m    319\u001b[0m     mat_z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(adj_A_new1, input_z\u001b[39m+\u001b[39mWa)\u001b[39m-\u001b[39mWa\n\u001b[0;32m    321\u001b[0m     H3 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_fc1((mat_z)))\n",
      "File \u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\utils.py:624\u001b[0m, in \u001b[0;36mpreprocess_adj_new1\u001b[1;34m(adj)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_adj_new1\u001b[39m(adj):\n\u001b[1;32m--> 624\u001b[0m     adj_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49minverse(torch\u001b[39m.\u001b[39;49meye(adj\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49mdouble()\u001b[39m-\u001b[39;49madj\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m))\n\u001b[0;32m    625\u001b[0m     \u001b[39mreturn\u001b[39;00m adj_normalized\n",
      "\u001b[1;31mRuntimeError\u001b[0m: inverse: LAPACK library not found in compilation"
     ]
    }
   ],
   "source": [
    "rel_rec = Variable(rel_rec)\n",
    "rel_send = Variable(rel_send)\n",
    "\n",
    "\n",
    "# compute constraint h(A) value\n",
    "def _h_A(A, m):\n",
    "    expm_A = matrix_poly(A*A, m)\n",
    "    h_A = torch.trace(expm_A) - m\n",
    "    return h_A\n",
    "\n",
    "prox_plus = torch.nn.Threshold(0.,0.)\n",
    "\n",
    "def stau(w, tau):\n",
    "    w1 = prox_plus(torch.abs(w)-tau)\n",
    "    return torch.sign(w)*w1\n",
    "\n",
    "\n",
    "def update_optimizer(optimizer, original_lr, c_A):\n",
    "    '''related LR to c_A, whenever c_A gets big, reduce LR proportionally'''\n",
    "    MAX_LR = 1e-2\n",
    "    MIN_LR = 1e-4\n",
    "\n",
    "    estimated_lr = original_lr / (math.log10(c_A) + 1e-10)\n",
    "    if estimated_lr > MAX_LR:\n",
    "        lr = MAX_LR\n",
    "    elif estimated_lr < MIN_LR:\n",
    "        lr = MIN_LR\n",
    "    else:\n",
    "        lr = estimated_lr\n",
    "\n",
    "    # set LR\n",
    "    for parame_group in optimizer.param_groups:\n",
    "        parame_group['lr'] = lr\n",
    "\n",
    "    return optimizer, lr\n",
    "\n",
    "#===================================\n",
    "# training:\n",
    "#===================================\n",
    "\n",
    "def train(epoch, best_val_loss, ground_truth_G, lambda_A, c_A, optimizer):\n",
    "    t = time.time()\n",
    "    nll_train = []\n",
    "    kl_train = []\n",
    "    mse_train = []\n",
    "    shd_trian = []\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    # update optimizer\n",
    "    optimizer, lr = update_optimizer(optimizer, args.lr, c_A)\n",
    "\n",
    "\n",
    "    for batch_idx, (data, relations) in enumerate(train_loader):\n",
    "\n",
    "        if args.cuda:\n",
    "            data, relations = data.cuda(), relations.cuda()\n",
    "        data, relations = Variable(data).double(), Variable(relations).double()\n",
    "\n",
    "        # reshape data\n",
    "        relations = relations.unsqueeze(2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc_x, logits, origin_A, adj_A_tilt_encoder, z_gap, z_positive, myA, Wa = encoder(data, rel_rec, rel_send)  # logits is of size: [num_sims, z_dims]\n",
    "        edges = logits\n",
    "\n",
    "        dec_x, output, adj_A_tilt_decoder = decoder(data, edges, args.data_variable_size * args.x_dims, rel_rec, rel_send, origin_A, adj_A_tilt_encoder, Wa)\n",
    "\n",
    "        if torch.sum(output != output):\n",
    "            print('nan error\\n')\n",
    "\n",
    "        target = data\n",
    "        preds = output\n",
    "        variance = 0.\n",
    "\n",
    "        # reconstruction accuracy loss\n",
    "        loss_nll = nll_gaussian(preds, target, variance)\n",
    "\n",
    "        # KL loss\n",
    "        loss_kl = kl_gaussian_sem(logits)\n",
    "\n",
    "        # ELBO loss:\n",
    "        loss = loss_kl + loss_nll\n",
    "\n",
    "        # add A loss\n",
    "        one_adj_A = origin_A # torch.mean(adj_A_tilt_decoder, dim =0)\n",
    "        sparse_loss = args.tau_A * torch.sum(torch.abs(one_adj_A))\n",
    "\n",
    "        # other loss term\n",
    "        if args.use_A_connect_loss:\n",
    "            connect_gap = A_connect_loss(one_adj_A, args.graph_threshold, z_gap)\n",
    "            loss += lambda_A * connect_gap + 0.5 * c_A * connect_gap * connect_gap\n",
    "\n",
    "        if args.use_A_positiver_loss:\n",
    "            positive_gap = A_positive_loss(one_adj_A, z_positive)\n",
    "            loss += .1 * (lambda_A * positive_gap + 0.5 * c_A * positive_gap * positive_gap)\n",
    "\n",
    "        # compute h(A)\n",
    "        h_A = _h_A(origin_A, args.data_variable_size)\n",
    "        loss += lambda_A * h_A + 0.5 * c_A * h_A * h_A + 100. * torch.trace(origin_A*origin_A) + sparse_loss #+  0.01 * torch.sum(variance * variance)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        loss = optimizer.step()\n",
    "\n",
    "        myA.data = stau(myA.data, args.tau_A*lr)\n",
    "\n",
    "        if torch.sum(origin_A != origin_A):\n",
    "            print('nan error\\n')\n",
    "\n",
    "        # compute metrics\n",
    "        graph = origin_A.data.clone().numpy()\n",
    "        graph[np.abs(graph) < args.graph_threshold] = 0\n",
    "\n",
    "        fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "\n",
    "\n",
    "        mse_train.append(F.mse_loss(preds, target).item())\n",
    "        nll_train.append(loss_nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "        shd_trian.append(shd)\n",
    "\n",
    "    print(h_A.item())\n",
    "    nll_val = []\n",
    "    acc_val = []\n",
    "    kl_val = []\n",
    "    mse_val = []\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch),\n",
    "          'nll_train: {:.10f}'.format(np.mean(nll_train)),\n",
    "          'kl_train: {:.10f}'.format(np.mean(kl_train)),\n",
    "          'ELBO_loss: {:.10f}'.format(np.mean(kl_train)  + np.mean(nll_train)),\n",
    "          'mse_train: {:.10f}'.format(np.mean(mse_train)),\n",
    "          'shd_trian: {:.10f}'.format(np.mean(shd_trian)),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    if args.save_folder and np.mean(nll_val) < best_val_loss:\n",
    "        # torch.save(encoder.state_dict(), encoder_file)\n",
    "        # torch.save(decoder.state_dict(), decoder_file)\n",
    "        print('Best model so far, saving...')\n",
    "        print('Epoch: {:04d}'.format(epoch),\n",
    "              'nll_train: {:.10f}'.format(np.mean(nll_train)),\n",
    "              'kl_train: {:.10f}'.format(np.mean(kl_train)),\n",
    "              'ELBO_loss: {:.10f}'.format(np.mean(kl_train)  + np.mean(nll_train)),\n",
    "              'mse_train: {:.10f}'.format(np.mean(mse_train)),\n",
    "              'shd_trian: {:.10f}'.format(np.mean(shd_trian)),\n",
    "              'time: {:.4f}s'.format(time.time() - t), file=log)\n",
    "        log.flush()\n",
    "\n",
    "    if 'graph' not in vars():\n",
    "        print('error on assign')\n",
    "\n",
    "\n",
    "    return np.mean(np.mean(kl_train)  + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), graph, origin_A\n",
    "\n",
    "#===================================\n",
    "# main\n",
    "#===================================\n",
    "\n",
    "t_total = time.time()\n",
    "best_ELBO_loss = np.inf\n",
    "best_NLL_loss = np.inf\n",
    "best_MSE_loss = np.inf\n",
    "best_epoch = 0\n",
    "best_ELBO_graph = []\n",
    "best_NLL_graph = []\n",
    "best_MSE_graph = []\n",
    "# optimizer step on hyparameters\n",
    "c_A = args.c_A\n",
    "lambda_A = args.lambda_A\n",
    "h_A_new = torch.tensor(1.)\n",
    "h_tol = args.h_tol\n",
    "k_max_iter = int(args.k_max_iter)\n",
    "h_A_old = np.inf\n",
    "\n",
    "try:\n",
    "    for step_k in range(k_max_iter):\n",
    "        while c_A < 1e+20:\n",
    "            for epoch in range(args.epochs):\n",
    "                ELBO_loss, NLL_loss, MSE_loss, graph, origin_A = train(epoch, best_ELBO_loss, ground_truth_G, lambda_A, c_A, optimizer)\n",
    "                if ELBO_loss < best_ELBO_loss:\n",
    "                    best_ELBO_loss = ELBO_loss\n",
    "                    best_epoch = epoch\n",
    "                    best_ELBO_graph = graph\n",
    "\n",
    "                if NLL_loss < best_NLL_loss:\n",
    "                    best_NLL_loss = NLL_loss\n",
    "                    best_epoch = epoch\n",
    "                    best_NLL_graph = graph\n",
    "\n",
    "                if MSE_loss < best_MSE_loss:\n",
    "                    best_MSE_loss = MSE_loss\n",
    "                    best_epoch = epoch\n",
    "                    best_MSE_graph = graph\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "            print(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "            if ELBO_loss > 2 * best_ELBO_loss:\n",
    "                break\n",
    "\n",
    "            # update parameters\n",
    "            A_new = origin_A.data.clone()\n",
    "            h_A_new = _h_A(A_new, args.data_variable_size)\n",
    "            if h_A_new.item() > 0.25 * h_A_old:\n",
    "                c_A*=10\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # update parameters\n",
    "            # h_A, adj_A are computed in loss anyway, so no need to store\n",
    "        h_A_old = h_A_new.item()\n",
    "        lambda_A += c_A * h_A_new.item()\n",
    "\n",
    "        if h_A_new.item() <= h_tol:\n",
    "            break\n",
    "\n",
    "\n",
    "    if args.save_folder:\n",
    "        print(\"Best Epoch: {:04d}\".format(best_epoch), file=log)\n",
    "        log.flush()\n",
    "\n",
    "    # test()\n",
    "    print (best_ELBO_graph)\n",
    "    print(nx.to_numpy_array(ground_truth_G))\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_ELBO_graph))\n",
    "    print('Best ELBO Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    print(best_NLL_graph)\n",
    "    print(nx.to_numpy_array(ground_truth_G))\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_NLL_graph))\n",
    "    print('Best NLL Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "\n",
    "    print (best_MSE_graph)\n",
    "    print(nx.to_numpy_array(ground_truth_G))\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_MSE_graph))\n",
    "    print('Best MSE Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    graph = origin_A.data.clone().numpy()\n",
    "    graph[np.abs(graph) < 0.1] = 0\n",
    "    # print(graph)\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "    print('threshold 0.1, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    graph[np.abs(graph) < 0.2] = 0\n",
    "    # print(graph)\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "    print('threshold 0.2, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    graph[np.abs(graph) < 0.3] = 0\n",
    "    # print(graph)\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "    print('threshold 0.3, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # print the best anway\n",
    "    print(best_ELBO_graph)\n",
    "    print(nx.to_numpy_array(ground_truth_G))\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_ELBO_graph))\n",
    "    print('Best ELBO Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    print(best_NLL_graph)\n",
    "    print(nx.to_numpy_array(ground_truth_G))\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_NLL_graph))\n",
    "    print('Best NLL Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    print(best_MSE_graph)\n",
    "    print(nx.to_numpy_array(ground_truth_G))\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_MSE_graph))\n",
    "    print('Best MSE Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    graph = origin_A.data.clone().numpy()\n",
    "    graph[np.abs(graph) < 0.1] = 0\n",
    "    # print(graph)\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "    print('threshold 0.1, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    graph[np.abs(graph) < 0.2] = 0\n",
    "    # print(graph)\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "    print('threshold 0.2, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "    graph[np.abs(graph) < 0.3] = 0\n",
    "    # print(graph)\n",
    "    fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n",
    "    print('threshold 0.3, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n",
    "\n",
    "\n",
    "f = open('trueG', 'w')\n",
    "matG = np.matrix(nx.to_numpy_array(z))\n",
    "for line in matG:\n",
    "    np.savetxt(f, line, fmt='%.5f')\n",
    "f.closed\n",
    "\n",
    "f1 = open('predG', 'w')\n",
    "matG1 = np.matrix(origin_A.data.clone().numpy())\n",
    "for line in matG1:\n",
    "    np.savetxt(f1, line, fmt='%.5f')\n",
    "f1.closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inverse: LAPACK library not found in compilation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jqcla\\Documents\\GitHub\\Honours-Thesis\\code\\dag_learning.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jqcla/Documents/GitHub/Honours-Thesis/code/dag_learning.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49minverse(torch\u001b[39m.\u001b[39;49mtensor(adj_A))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: inverse: LAPACK library not found in compilation"
     ]
    }
   ],
   "source": [
    "torch.inverse(torch.tensor(adj_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
